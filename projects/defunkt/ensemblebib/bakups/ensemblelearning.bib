-*- BibTeX -*-
=====================================
This file has been sorted by
bibSort.edt on 15.08.06 at 10:10:42.

It includes 547 items, sorted by
"year" first, then by "year".
(No cross-referenced entries.)
=====================================





@BOOK{laplace1818,
  author       = {P. S. de Laplace},
  title        = {Deuxieme supplement a la theorie analytique des
                  probabilites},
  publisher    = {Paris, Gauthier-Villars},
  year         = 1818,
  note         = {Reprinted (1847) in Oeuvres Completes de Laplace,
                  vol. 7},
  annote       = {First known observation that combining probabilistic classifiers can help.},
}

@ARTICLE{markowitz52,
  title        = {Portfolio Selection},
  author       = {H. Markowitz},
  journal      = {Journal of Finance},
  volume       = 7,
  issue        = 1,
  month        = {March},
  year         = 1952,
  annote       = {Shows that a linear combination (ensemble) of individual regressors obey the bias-variance-covariance decomposition.
                  For this paper, Markowitz received the Nobel Prize for Economic in 1974.}
}

@BOOK{nilsson65,
  author =   {N.J.Nilsson},
  title =    {Learning Machines: Foundations of Trainable
                  Pattern-Classifying Systems},
  publisher =    {McGraw-Hill},
  year =     1965,
}

@ARTICLE{batesgranger69,
  author =   {J. M. Bates and C. W. J. Granger},
  title =    {The combination of forecasts},
  journal =  {Operations Research Quarterly},
  year =     1969,
  number =   20,
  pages =    {451-468},
}

@BOOK{tikhonov77book,
  author =   {A. N. Tikhonov and V. Y. Arsenin},
  title =    {Solutions of Ill-posed problems},
  publisher =    {W.H.Winston and Sons, Washington D.C.},
  year =     1977,
}

@ARTICLE{devroye79distribution,
  author =   {L. Devroye and T. Wagner},
  title =    {Distribution-free Performance Bounds for Potential
                  Function Rules},
  journal =  {IEEE Transactions on Information Theory},
  year =     1979,
  volume =   25,
  number =   5,
  pages =    {601--604},
}

@BOOK{mcclellandrumelhart86,
  author =   {J. McClelland and D. Rumelhart},
  title =    {Parallel Distributed Processing},
  publisher =    {MIT Press},
  year =     1986,
}

@INBOOK{mandler88:combining,
  author =   {E. Mandler and J. Schuermann},
  chapter =  {Combining the Classification Results of independent
                  classifiers based on the Dempster/Shafer theory of
                  evidence},
  title =    {Pattern Recognition and Artificial Intelligence},
  editor =   {E.S.Gelsema and L.N.Kanal},
  publisher =    {North Holland, Amsterdam},
  pages =    {381--393},
  year =     1988,
}

@ARTICLE{condorcetreview,
  author =   {Young, P},
  title =    {Condorcet's Theory of Voting},
  journal =  {American Political Science Review},
  year =     1988,
  volume =   82,
  number =   {1231-1244},
}

@ARTICLE{clemen89forecasts,
  author =   {R. Clemen},
  year =     1989,
  title =    {Combining forecast: A review and annotated
                  bibliography},
  journal =  {International Journal on Forecasting},
  volume =   5,
  pages =    {559--583},
}

@ARTICLE{granger89,
  pages =    {167--174},
  author =   {C.W.J. Granger},
  year =     1989,
  journal =  {Journal of Forecasting},
  volume =   8,
  title =    {Combining Forecasts -- Twenty Years Later},
}

@ARTICLE{hansensalamon90,
  volume =   12,
  number =   10,
  title =    {Neural Network Ensembles},
  author =   {Lars Kai Hansen and Peter Salamon},
  journal =  {IEEE Transactions on Pattern Analysis and Machine
                  Intelligence},
  year =     1990,
  pages =    {993-1001},
}

@ARTICLE{kleinberg90stochastic,
  author =   {E. M. Kleinberg},
  title =    {Stochastic Discrimination},
  journal =  {Annals of Mathematics and Artificial Intelligence},
  volume =   1,
  year =     1990,
}

@INBOOK{minsky90:multiple,
  author =   {Marvin Minsky},
  editor =   {Patrick H. Winston},
  title =    {Artificial Intelligence at MIT : Expanding
                  Frontiers},
  chapter =  {Logical vs. Analogical or Symbolic vs. Connectionist
                  or Neat vs Scruffy},
  publisher =    {MIT Press},
  year =     1990,
  volume =   1,
}

@ARTICLE{ruck90bayesperceptron,
  author =   {D. W. Ruck and S. K. Rogers and M. Kabrisky and
                  M. E. Oxley and B. W. Suter},
  title =    {The multilayer perceptron ass an approximation to a
                  Bayes optimal discrimant function},
  journal =  {IEEE Transactions on Neural Networks},
  type =     {Letter},
  year =     1990,
  volume =   1,
  number =   4,
  pages =    {296--298},
}

@ARTICLE{schapire90strength,
  author =   {Robert E. Schapire},
  title =    {The Strength of Weak Learnability},
  journal =  {Machine Learning},
  volume =   5,
  pages =    {197-227},
  year =     1990,
  url =      {http://citeseer.nj.nec.com/schapire90strength.html},
}

@ARTICLE{wan90bayesneural,
  author =   {E. A. Wan},
  title =    {Neural network classification: A Bayesian
                  interpretation},
  journal =  {IEEE Transactions on Neural Networks},
  type =     {Letter},
  year =     1990,
  volume =   1,
  number =   4,
  pages =    {303--304},
}

@INPROCEEDINGS{bottou91framework,
  author =   {L{\'e}on Bottou and Patrick Gallinari},
  title =    {A Framework for the Cooperation of Learning
                  Algorithms},
  booktitle =    {Advances in Neural Information Processing Systems},
  volume =   3,
  publisher =    {Morgan Kaufmann Publishers, Inc.},
  editor =   {Richard P. Lippmann and John E. Moody and David
                  S. Touretzky},
  pages =    {781--788},
  year =     1991,
}

@INPROCEEDINGS{dietterichbakiri91:errorcorrecting,
  author =   {T. G. Dietterich and G. Bakiri},
  title =    {Error-correcting output codes: a general method for
                  improving multiclass inductive learning programs},
  booktitle =    {Proceedings of the Ninth {AAAI} National Conference
                  on Artificial Intelligence},
  publisher =    {AAAI Press},
  address =  {Menlo Park, CA},
  editor =   {Dean, T. L. and McKeown, K.},
  pages =    {572--577},
  year =     1991,
}

@ARTICLE{friedman91mars,
  author =   {J.H. Friedman},
  title =    {Multivariate Adaptive Regression Splines},
  journal =  {Annals of Statistics},
  year =     1991,
  volume =   19,
  pages =    {1--141},
}

@ARTICLE{jacobsjordan91,
  author =   {R. A. Jacobs and M. I. Jordan and S. J. Nowlan and
                  G. E. Hinton},
  title =    {Adaptive Mixtures of Local Experts},
  type =     {Letter},
  journal =  {Neural Computation},
  volume =   3,
  number =   1,
  pages =    {79--87},
  year =     1991,
  abstract =     {We present a new supervised learning procedure for
                  systems composed of many separate networks, each of
                  which learns to handle a subset of the complete set
                  of training cases. The new procedure can be viewed
                  either as a modular version of a multilayer
                  supervised network, or as an associative version of
                  competitive learning. It therefore provides a new
                  link beetween these two apparently different
                  approaches. We demonstrate that the learning
                  procedure divides up a voewel discrimination task
                  into appropriate subtasks, each of which can be
                  solved by a very simple expert network.},
}

@ARTICLE{jacobsjordanbarto91,
  pages =    {219-250},
  title =    {Task decomposition through competition in a modular
                  connectionist architecture - the What and Where
                  vision tasks},
  volume =   15,
  year =     1991,
  author =   {Robert A. Jacobs and Michael I. Jordan and Andrew
                  G. Barto},
  journal =  {Cognitive Science},
}

@ARTICLE{mani91portfolio,
  author =   {G. Mani},
  title =    {Lowering variance of decisions by using artificial
                  network portfolios},
  type =     {Letter},
  journal =  {Neural Computation},
  volume =   3,
  number =   4,
  pages =    {484--486},
  year =     1991,
}

@INPROCEEDINGS{nips90:Pearlmutter-Rosenfeld,
  author =   {Barak A. Pearlmutter and Ronald Rosenfeld},
  title =    {Chaitin-{K}olmogorov Complexity and Generalization
                  in Neural Networks},
  pages =    {925--931},
  url =      {http://nips.djvuzone.org/djvu/nips03/0925.djvu},
  booktitle =    {Advances in Neural Information Processing Systems 3},
  publisher =    {Morgan Kaufmann},
  year =     1991,
}

@ARTICLE{geman92,
  author =   {S. Geman and E. Bienenstock and R. Doursat},
  title =    {Neural Networks and the Bias/Variance Dilemma},
  type =     {Letter},
  journal =  {Neural Computation},
  volume =   4,
  number =   1,
  pages =    {1--58},
  year =     1992,
  abstract =     {Feedforward neural networks trained by error
                  backpropagation are examples of nonparametric
                  regression estimators. We present a tutorial on
                  nonparametric inference and its relation to neural
                  networks, and we use the statistical viewpoint to
                  highlight strengths and weaknesses of neural
                  models. We illustrate the main points with some
                  recognition. experiments involving artificial data
                  as well as handwritten numerals. In way of
                  conclusion, we suggest that current-generation
                  feedforward neural networks are largely inadequate
                  for difficult problems in machine perception and
                  machine learning, regardless of
                  parallel-versus-serial hardware or other
                  implementation issues. Furthermore, we suggest that
                  the fundamental challenges in neural modeling are
                  about representation rather than learning per
                  se. This last point is supported by additional
                  experiments with handwritten numerals.},
}

@BOOK{koza92book,
  author =   {John R. Koza},
  title =    {Genetic Programming: On the Programming of Computers
                  by Means of Natural Selection},
  publisher =    {MIT Press},
  year =     1992,
  isbn =     {0-262-11170-5},
}

@BOOK{landesman92book,
  author =   {Edward Landesman and Magnus Hestenes},
  title =    {Linear Algebra for Mathematics, Science and
                  Engineering},
  publisher =    {Prentice Hall},
  year =     1992,
  isbn =     {0-13-529561-0},
}

@ARTICLE{smith92,
  number =   2,
  pages =    {127--149},
  title =    {Searching for Diverse Cooperative Populations with
                  Genetic Algorithms},
  volume =   1,
  year =     1992 ,
  journal =  {Evolutionary Computation},
  author =   {Robert E. Smith and Stephanie Forrest and Alan
                  S. Perelson},
}

@ARTICLE{wolpert92stacked,
  author =   {D. H. Wolpert},
  title =    {Stacked Generalization},
  journal =  {Neural Networks},
  volume =   5,
  year =     1992,
  pages =    {241--259},
  abstract =     {This paper introduces stacked generalization, a
                  scheme for minimizing the generalization error rate
                  of one or more generalizers. Stacked generalization
                  works by deducing the biases of the generalizer(s)
                  with respect to a provided learning set. This
                  deduction proceeds by generalizing in a second space
                  whose inputs are (for example) the guesses of the
                  original generalizers when taught with part of the
                  learning set and trying to guess the rest of it, and
                  whose output is (for example) the correct
                  guess. When used with multiple generalizers, stacked
                  generalization can be seen as a more sophisticated
                  version of cross-validation, exploiting a strategy
                  more sophisticated than cross-validation's crude
                  winner-takes-all for combining the individual
                  generalizers. When used with a single generalizer,
                  stacked generalization is a scheme for estimating
                  (and then correcting for) the error of a generalizer
                  which has been trained on a particular learning set
                  and then asked a particular question. After
                  introducing stacked generalization and justifying
                  its use, this paper presents two numerical
                  experiments. The first demonstrates how stacked
                  generalization improves upon a set of separate
                  generalizers for the NETtalk task or translating
                  text to phonemes. The second demonstrates how
                  stacked generalization improves the performance of a
                  single surface-fitter. With the other experimental
                  evidence in the literature, the usual arguments
                  supporting cross-validation, and the abstract
                  justifications presented in this paper, the
                  conclusion is that for almost any real-world
                  generalization problem one should use some version
                  of stacked generalization to minimize the
                  generalization error rate. This paper ends by
                  discussing some of the variation of stacked
                  generalization, and how it touches on other fields
                  like chaos theory. },
}

@ARTICLE{back93evolution,
  author =   {T. B{\"{a}}ck and H.-P. Schwefel},
  title =    {An overview of evolutionary algorithms for parameter
                  optimization},
  journal =  {Evolutionary Computation},
  year =     1993,
  volume =   1,
  number =   1,
  pages =    {1-23},
}

@BOOK{efron93bootstrap,
  author =   {B. Efron and R. Tibshirani},
  title =    {An Introduction to the Bootstrap},
  publisher =    {Chapman and Hall},
  year =     1993,
}

@PHDTHESIS{hashem93thesis,
  author =   {Sherif Hashem},
  title =    {{O}ptimal {L}inear {C}ombinations of {N}eural
                  {N}etworks},
  school =   {School of Industrial Engineering, University of
                  Purdue},
  year =     1993
}

@INPROCEEDINGS{kleinberg93pattern,
  author =   {E. M. Kleinberg and T. K. Ho},
  title =    {Pattern Recognition by Stochastic Modeling},
  booktitle =    {Proc. of the 3rd Workshop on Frontiers in
                  Handwriting Recognition},
  month =    {May},
  address =  {Buffalo, New York},
  pages =    {175--183},
  year =     1993,
}

@ARTICLE{montgomeryfriedman93,
  author =   {D.C. Montgomery and D.J. Friedman},
  title =    {Prediction Using Regression Models with
                  Multicollinear Predictor Variables},
  journal =  {IIE Transactions},
  year =     1993,
  volume =   25,
  number =   3,
  pages =    {73--85},
  abstract =     {Linear regression models are widely used for
                  forecasting and prediction of new observations from
                  the underlying modeled process. The article explores
                  the use of regression models in this context when
                  the regressor or predictor variables exhibit
                  multicollinearity, or near-linear dependence. It is
                  shown that multicollinearity can severely impact the
                  predictive performance of a regression model and
                  that biased estimation methods can be an effective
                  countermeasure when multicollinearity is
                  present. Several biased estimation methods are
                  described and evaluated, including a new method for
                  selecting the biasing parameter in ordinary ridge
                  regression. A simulation study is performed to
                  provide some guidelines for the choice of an
                  estimation method. },
}

@INCOLLECTION{perronecooper93,
  author =   {M. P. Perrone and L. N. Cooper},
  title =    {When networks disagree: Ensemble methods for hybrid
                  neural networks},
  editor =   {R. J. Mammone},
  booktitle =    {Artificial Neural Networks for Speech and Vision},
  address =  {London},
  pages =    {126--142},
  year =     1993,
}

@PHDTHESIS{perrone93improving,
  author =   {M.P. Perrone},
  title =    {Improving Regression Estimation: Averaging Methods
                  for Variance Reduction with Extensions to General
                  Convex Measure Optimization},
  year =     1993,
  school =   {Brown University, Institute for Brain and Neural
                  Systems},
}

@INPROCEEDINGS{saborin93digit,
  author =   {Michael Saborin and Amar Mitiche and Danny Thomas
                  and George Nagy},
  title =    {Classifier Combination for Hand-Printed Digit
                  Recognition},
  pages =    {163--166},
  booktitle =    {Proceedings of the Second International Conference
                  on Document Analysis and Recognition},
  year =     1993,
  organization = {IEEE},
  address =  {Japan},
}

@ARTICLE{tetko93structure,
  author =   {Tetko, I. V. and Luik, A. I. and Poda, G. I.},
  title =    {Applications of neural networks in
                  structure-activity relationships of a small number
                  of molecules},
  journal =  {Journal of Medicinal Chemistry},
  volume =   36,
  number =   7,
  pages =    {811-4},
  abstract =     {We investigated the applications of back propagation
                  artificial neural networks (ANN) for a small dataset
                  analysis in the field of structure-activity
                  relationships. The derivatives of carboquinone were
                  used as an example. It\'s been found that in this
                  case the use of the same neural network results in
                  unambiguous classification of new
                  molecules. Predictions can be improved with
                  statistical analysis of independent prognosis
                  sets. We suggest that the sign criterion be used as
                  a classification rule. We also compared neural
                  networks with FALS and ALS in leave-one-out
                  prediction. ANN applied to the same dataset has
                  shown the same predictive ability as ALS but poorer
                  than FALS.},
  keywords =     {*Carbazilquinone/aa [Analogs & Derivatives]
                  Comparative Study *Mitomycins/pd [Pharmacology]
                  *Neural Networks (Computer) Structure-Activity
                  Relationship},
  year =     1993,
}

@ARTICLE{battiti1994democracy,
  author =   {Roberto Battiti and Anna Maria Colla},
  title =    {Democracy in Neural Nets: Voting Schemes for
                  Classification},
  journal =  {Neural Networks},
  year =     1994,
  volume =   7,
  number =   4,
  pages =    {691--707},
  abstract =     {Discusses some possible ways to combine the outputs
                  of a set of neural network classifiers to reach a
                  combined decision with a higher performance in terms
                  of lower rejection rates and/or better accuracy
                  rates. The methods considered range from the
                  requirement of a complete agreement among the
                  individual classifications to election schemes based
                  on the distribution of votes collected by the
                  different classes. In addition, the rejection rules
                  based on the different output classes can be
                  complemented by rules that also consider the
                  information in the individual output vectors, with
                  the possibility of using threshold requirements and
                  that of averaging the different vectors. Although
                  the Bayesian framework and some probabilistic
                  assumptions provide useful indications about the
                  potential advantage of different combination
                  schemes, the combined performance ultimately depends
                  on the joint probability distribution of the
                  outputs, and it can be estimated by joining the
                  results of different nets on the same test set. The
                  combination methods are very flexible, they permit a
                  straightforward cooperation of neural and
                  traditional recognizers, and they are appropriate in
                  a development environment where experiments are
                  performed with different kinds of nets and features
                  for a selected application. From the authors'
                  experiments in the field of handwritten digit
                  recognition (up to a total of more than 50000
                  characters), they found that the use of a small
                  number of nets (two to three) with a sufficiently
                  large uncorrelation in their mistakes reaches a
                  combined performance that is significantly higher
                  than the best obtainable from the individual nets,
                  with a negligible effort after starting from a pool
                  of networks produced in the development phase of an
                  application. In particular, for a real-world OCR
                  application, the best accuracy increase is about
                  half the increase in the rejection rate, so that
                  accuracies of the order of 99.5% can be reached by
                  rejecting less than 5% of the patterns. This
                  performance is significant for real applications.},
}

@ARTICLE{drucker94boosting,
  author =   {Harris Drucker and Corinna Cortes and L. D. Jackel
                  and Yann LeCun and Vladimir Vapnik},
  title =    {Boosting and Other Ensemble Methods},
  type =     {Letter},
  journal =  {Neural Computation},
  volume =   6,
  number =   6,
  pages =    {1289--1301},
  year =     1994,
  abstract =     {Compares the performance of three types of neural
                  network-based ensemble techniques to that of a
                  single neural network. The ensemble algorithms are
                  two versions of boosting and committees of neural
                  networks trained independently. For each of the four
                  algorithms, we experimentally determine the test and
                  training error curves in an optical character
                  recognition (OCR) problem as both a function of
                  training set size and computational cost, using
                  three architectures. We show that a single machine
                  is best for small training set sizes, while for
                  large training set sizes, some version of boosting
                  is best. However, for a given computational cost,
                  boosting is always best. Furthermore, we show a
                  surprising result for the original boosting
                  algorithm, namely that as the training set size
                  increases, the training error decreases until it
                  asymptotes to the test error rate. This has
                  potential implications in the search for better
                  training algorithms. },
}

@ARTICLE{feder94entropy,
  author =   {Meir Feder and Neri Merhav},
  title =    {Relations between entropy and error probability},
  journal =  {IEEE Transactions on Information Theory},
  volume =   40,
  number =   1,
  year =     1994,
  pages =    259,
}

@BOOK{haykin94,
  author =   {S. Haykin},
  title =    {Neural Networks: A Comprehensive Foundation},
  publisher =    {Macmillan Co., New York},
  year =     1994,
  isbn =     {0-13-273350-1},
}

@ARTICLE{ho94mcs,
  author =   {Tin Kam Ho and Jonathan J.~Hull and Sargur
                  N.~Srihari},
  title =    {Decision Combination in Multiple Classifier Systems},
  journal =  {Pattern Analysis and Machine Intelligence},
  year =     1994,
  volume =   16,
  number =   1,
  pages =    {66--75},
  month =    {January},
}

@INPROCEEDINGS{huang94nn,
  author =   {Y S Huang and C Y Suen},
  title =    {A Method of Combining Multiple Classifiers - A
                  Neural Network Approach},
  booktitle =    {Proceedings of the 12th International Conference on
                  Pattern Recognition and Computer Vision},
  year =     1994,
  pages =    {473-475},
  address =  {Jerusalem, Israel},
}

@ARTICLE{jordan94hierarchical,
  author =   {Michael I. Jordan and Robert A. Jacobs},
  title =    {Hierarchical Mixtures of Experts and the {EM}
                  Algorithm},
  journal =  {Neural Computation},
  year =     1994,
  volume =   6,
  pages =    {181--214},
  class =    {nn, learning},
  abstract =     {We present a tree-structured architecture for
                  supervised learning. The statistical model
                  underlying the architecture is a hierarchical
                  mixture model in which both the mixture coefficients
                  and the mixture components are generalized linear
                  models (GLIM). Learning is treated as a maximum
                  likelihood problem; in particular, we present an
                  Expectation-Maximization (EM) algorithm for
                  adjusting the parameters of the architecture. We
                  develop an online learning algorithm in which the
                  parameters are updated incrementally. Comparative
                  simulation results are presented in the robot
                  dynamics domain.},
}

@ARTICLE{rogova94combining,
  author =   {Galina Rogova},
  title =    {Combining the Results of Neural Network Classifiers},
  journal =  {Neural Networks},
  year =     1994,
  volume =   7,
  number =   5,
  pages =    {777--781},
  class =    {nn, learning},
}

@ARTICLE{tumerghosh94framework,
  author =   {K. Tumer and J. Ghosh},
  title =    {A framework for estimating performance improvements
                  in hybrid pattern classifiers},
  journal =  {World Congress on Neural Networks},
  volume =   3,
  pages =    {220--5},
  publisher =    {Lawrence Erlbaum Associates},
  month =    {June},
  year =     1994,
  abstract =     {Classification methods often perform significantly
                  below Bayesian limits in complex, high-dimensional
                  classification tasks because of model bias,
                  inadequate training data and noise/variability in
                  the data. When several classifiers are used for a
                  given task, selecting one method over all others
                  discards potentially valuable
                  information. Strategies aimed at suitably combining
                  the results of multiple classifiers are expected to
                  perform better than any single method, and reduce
                  overall bias and noise. An underwater passive sonar
                  data set consisting of over 1000 samples processed
                  to produce different 25-dimensional and
                  24-dimensional feature vectors is used in this study
                  to examine an evidence combination framework. An
                  analysis of the conditions that the data sets must
                  satisfy, and the conditions under which improvements
                  can be obtained is provided, and results are
                  presented for hybrid networks using both local and
                  global classifiers. },
}

@INPROCEEDINGS{tumerghosh94limits,
  author =   {K. Tumer and J. Ghosh},
  title =    {Limits to Performance Gains in Combined Neural
                  Classifiers},
  editor =   {Dagli and Akay and Chen and Fernandez and Ghosh},
  booktitle =    {Intelligent engineering systems through artificial
                  neural networks: proceedings of the Artificial
                  Neural Networks in Engineering ({ANNIE} '95)
                  Conference},
  publisher =    {American Society of Mechanical Engineers},
  address =  {United Engineering Center, 345 E. 47th St., New
                  York, NY 10017, USA},
  year =     1994,
  volume =   5,
  series =   {ASME Press Series on International Advances in
                  Design Productivity},
  abstract =     {The performance of a single classifier is often
                  inadequate in difficult classification problems. In
                  such cases, several researchers have combined the
                  outputs of multiple classifiers to obtain better
                  performance. However, the amount of improvement
                  possible through such combination techniques is
                  generally not known. We present two approaches to
                  estimating performance limits in hybrid
                  networks. First, we present a framework that
                  estimates Bayes error rates when linear combiners
                  are used. Then we discuss a more general method that
                  provides decision confidences and error bounds based
                  on error types arising from the training data. The
                  methods are illustrated for a difficult four class
                  problem involving underwater acoustic data. For this
                  data, we compute the single classifier and combiner
                  classification performances, as well as the Bayes
                  error rate and an error bound.},
}

@TECHREPORT{ali95comparison,
  author =   {K. Ali},
  year =     1995,
  title =    {A comparison of methods for learning and combining
                  evidence from multiple models},
  institution =  {University of California, Irvine, Dept. of
                  Information and Computer Sciences},
  number =   {UCI TR \#95-47},
  abstract =     {Most previous work on multiple models has been done
                  on a few domains. We present a comparsion of three
                  ways of learning multiple models on 29 data sets
                  from the UCI repository. The methods are bagging,
                  $k$-fold partition learning and stochastic
                  search. By using 29 data sets of various kinds -
                  artificial data sets, artificial data sets with
                  noise, molecular-biology and real-world noisy data
                  sets - we are able to draw robust experimental
                  conclusions about the kinds of data sets for which
                  each learning method works best. We also compare
                  four evidence combination methods (Uniform Voting,
                  Bayesian Combination, Distribution Summation and
                  Likelihood Combination) and characterize the kinds
                  of data sets for which each method works best.}
}

@ARTICLE{bishop95training,
  author =   {Chris M. Bishop},
  title =    {Training with Noise is Equivalent to {T}ikhonov
                  Regularization},
  journal =  {Neural Computation},
  volume =   7,
  number =   1,
  pages =    {108--116},
  year =     1995,
  url =      {http://citeseer.nj.nec.com/bishop94training.html},
}

@BOOK{bishop95book,
  author =   {Christopher M. Bishop},
  title =    {Neural Networks for Pattern Recogntion},
  publisher =    {Oxford University Press},
  year =     1995,
  isbn =     {0-19-853864-2},
}

@INPROCEEDINGS{chan95arbiter,
  author =   {Philip K.~Chan and Salvatore J.~Stolfo},
  title =    {Learning Arbiter and Combiner Trees from Partitioned
                  Data for Scaling Machine Learning},
  booktitle =    {The first international conference on knowledge
                  discovery and data mining, KDD '95},
  year =     1995,
  pages =    {39-45},
}

@INPROCEEDINGS{costa95bayesian,
  author =   {M. Costa and E. Filippi and E. Pasero},
  title =    {Artificial neural network ensembles: a Bayesian
                  standpoint},
  booktitle =    {Proceedings of the 7th Italian Workshop on Neural
                  Nets},
  pages =    {39-57},
  publisher =    {World Scientific},
  year =     1995,
  editor =   {M. Marinaro and R. Tagliaferri},
  abstract =     {Pooled estimates naturally arise from the Bayesian
                  framework as the elected approach to both regression
                  and classification problems whenever optimality in
                  the average sense is concerned. In contrast,
                  selection of a single 'best' member appears to be a
                  somewhat crude approximation where some important
                  features of the underlying model are
                  discarded. However, the elegant and powerful full
                  fledged formalism carries a very high computational
                  complexity. Some practical implementations relevant
                  to artificial neural network ensembles are therefore
                  reviewed that make things more tractable while
                  keeping the same generality.},
}

@TECHREPORT{darwenyao95,
  year =     1995,
  author =   {Paul Darwen and Xin Yao},
  institution =  {University of New South Wales},
  number =   {CS 8/95},
  title =    {How Good is Fitness Sharing with a Scaling Function},
  month =    {April},
}

@ARTICLE{girosi95regularization,
  author =   {Federico Girosi and Michael Jones and Tomaso Poggio},
  title =    {Regularization Theory and Neural Networks
                  Architectures},
  journal =  {Neural Computation},
  volume =   7,
  number =   2,
  pages =    {219--269},
  year =     1995,
  url =          {http://citeseer.nj.nec.com/girosi95regularization.html},
}

@ARTICLE{hashem95optimal,
  author =   {Sherif Hashem and Bruce Schmeiser},
  title =    {Improving Model Accuracy Using Optimal Linear
                  Combinations of Trained Neural Networks},
  journal =  {IEEE Transactions on Neural Networks},
  type =     {Letter},
  year =     1995,
  volume =   6,
  number =   3,
  pages =    {792--794},
  month =    may,
  abstract =     {Neural network (NN) based modeling often requires
                  trying multiple networks with different
                  architectures and training parameters in order to
                  achieve an acceptable model accuracy. Typically,
                  only one of the trained networks is selected as
                  'best' and the rest are discarded. The authors
                  propose using optimal linear combinations (OLC's) of
                  the corresponding outputs on a set of NN's as an
                  alternative to using a single network. Modeling
                  accuracy is measured by mean squared error (MSE)
                  with respect to the distribution of random
                  inputs. Optimality is defined by minimizing the MSE,
                  with the resultant combination referred to as
                  MSE-OLC. The authors formulate the MSE-OLC problem
                  for trained NN's and derive two closed-form
                  expressions for the optimal combination-weights. An
                  example that illustrates significant improvement in
                  model accuracy as a result of using MSE-OLC's of the
                  trained networks is included.},
}

@ARTICLE{huang95numerals,
  author =   {Y.S.~Huang and C.Y.~Suen},
  title =    {A Method of Combining Multiple Experts for the
                  Recognition of Unconstrained Handwritten Numerals},
  journal =  {Pattern Analysis and Machine Intelligence},
  year =     1995,
  volume =   17,
  number =   1,
  pages =    {90--94},
  month =    {January},
}

@INPROCEEDINGS{kang95learning,
  author =   {Kukjin Kang and Jong-Hoon Oh},
  title =    {Learning by a Population of Perceptrons},
  booktitle =    {Computational Learning Theory},
  pages =    {297-300},
  year =     1995,
  url =      {http://citeseer.nj.nec.com/kang97learning.html},
}

@INPROCEEDINGS{kongdietterich95correcting,
  author =   {E. B. Kong and T. G. Dietterich},
  title =    {Error-Correcting Output Coding Corrects Bias and
                  Variance},
  booktitle =    {Proceedings of the 12th International Conference on
                  Machine Learning},
  publisher =    {Morgan Kaufmann},
  year =     1995,
  pages =    {313--321},
  comment =  {Tahoe Cite, CA},
  abstract =     {Previous research has shown that a technique called
                  error-correcting output coding (ECOC) can
                  dramatically improve the classification accuracy of
                  supervised learning algorithms that learn to
                  classify data points into one of k>>2 classes. This
                  paper presents an investigation of why the ECOC
                  technique works, particularly when employed with
                  decision-tree learning algorithms. It shows that the
                  ECOC method, like any form of voting or committee,
                  can reduce the variance of the learning
                  algorithm. Furthermore, unlike methods that simply
                  combine multiple runs of the same learning
                  algorithm, ECOC can correct errors caused by the
                  bias of the learning algorithm. Experiments show
                  that this bias correction ability relies on the
                  non-local behavior of C4.5.},
}

@ARTICLE{kroghvedelsby95,
  author =   {A. Krogh and J. Vedelsby},
  title =    {Neural Network Ensembles, Cross Validation, and
                  Active Learning},
  journal =  {NIPS},
  year =     1995,
  volume =   7,
  pages =    {231--238},
  abstract =     {The learning of continuous-valued functions using
                  neural network ensembles (committees) can give
                  improved accuracy, a reliable estimation of the
                  generalization error, and active learning. The
                  ambiguity is defined as the variation of the output
                  of ensemble members averaged over unlabeled data, so
                  it quantifies the disagreement among the
                  networks. We discuss how to use the ambiguity in
                  combination with cross-validation to give a reliable
                  estimate of the ensemble generalization error, and
                  how this type of ensemble cross-validation can
                  sometimes improve performance. We show how to
                  estimate the optimal weights of the ensemble members
                  using unlabeled data. By a generalization of
                  query-by-committee, we show how the ambiguity can be
                  used to select new training data to be labeled in an
                  active learning scheme.},
}

@ARTICLE{lehto95,
  volume =   7,
  author =   {Mikko Lehtokangas and Jukka Saarinen and Kimmo Kaski
                  and Pentti Huuhtanen},
  year =     1995,
  pages =    {983-999},
  number =   5,
  title =    {Initializing Weights of a Multilayer Perceptron
                  Network by Using the Orthogonal Least Squares
                  Algorithm},
  journal =  {Neural Computation},
}

@INPROCEEDINGS{maclinshavlik95weights,
  author =   {R. Maclin and J. W. Shavlik},
  title =    {Combining the Predictions of Multiple Classifiers:
                  Using Competitive Learning to Initialize Neural
                  Networks},
  booktitle =    {Proceedings of the 14th International Joint
                  Conference on Artificial Intelligence, Montreal,
                  Canada},
  year =     1995,
  pages =    {524-530},
  abstract =     {The primary goal of inductive learning is to
                  generalize well-that is, induce a function that
                  accurately produces the correct output for future
                  inputs. Hansen and Salamon (1990) showed that, under
                  certain assumptions, combining the predictions of
                  several separately trained neural networks will
                  improve generalization. One of their key assumptions
                  is that the individual networks should be
                  independent in the errors they produce. In the
                  standard way of performing backpropagation this
                  assumption may be violated, because the standard
                  procedure is to initialize network weights in the
                  region of weight space near the origin. This means
                  that backpropagation's gradient-descent search may
                  only reach a small subset of the possible local
                  minima. In this paper we present an approach to
                  initializing neural networks that uses competitive
                  learning to intelligently create networks that are
                  originally located far from the origin of weight
                  space, thereby potentially increasing the set of
                  reachable local minima. We report experiments on two
                  real-world datasets where combinations of networks
                  initialized with our method generalize better than
                  combinations of networks initialized the traditional
                  way.},
}

@ARTICLE{meir95:bias,
  author =   {Ronny Meir},
  title =    {Bias, Variance and the Combination of Least Squares
                  Estimators},
  pages =    {295--302},
  editor =   {G. Tesauro and D. Touretzky and T. Leen},
  volume =   7,
  booktitle =    {Advances in Neural Information Processing Systems},
  year =     1995,
  publisher =    {The {MIT} Press},
  abstract =     {We consider the effect of combining several least
                  squares estimators on the expected performance of a
                  regression problem. Computing the exact bias and
                  variance curves as a function of the sample size we
                  are able to quantitatively compare the effect of the
                  combination on the bias and variance separately, and
                  thus on the expected error which is the sum of the
                  two. Our exact calculations, demonstrate that the
                  combination of estimators is particularly useful in
                  the case where the data set is small and noisy and
                  the function to be learned is unrealizable. For
                  large data sets the single estimator produces
                  superior results. Finally, we show that by splitting
                  the data set into several independent parts and
                  training each estimator on a different subset, the
                  performance can in some cases be significantly
                  improved.},
}

@MISC{orr95regularisation,
  author =   {M. Orr},
  title =    {Regularisation in the Selection of Radial Basis
                  Function Centres},
  text =     {M. J. L. Orr, Regularisation in the Selection of
                  Radial Basis Function Centres, Neural Computation,
                  Vol. 7, pp. 606-623, 1995.},
  year =     1995,
  url =      {citeseer.nj.nec.com/orr95regularisation.html},
}

@INPROCEEDINGS{powalka95,
  author =   {R Powalka, N Sherkat, R Whitrow},
  title =    {Multiple recognizer combination topologies},
  booktitle =    {Proceedings of the Seventh Biannual Conference of
                  the International Graphonomics Society},
  pages =    {128--129},
  year =     1995,
  month =    {August},
  organization = {University of Western Ontario},
  note =     {ISBN 0-921121-14-8},
}

@ARTICLE{sharkey95weight,
  author =   {Sharkey,N.E. and Neary,J. and Sharkey,A.J.C.},
  title =    {{S}earching {W}eight {S}pace for {B}ackpropagation
                  {S}olution {T}ypes},
  journal =  {{C}urrent {T}rends in {C}onnectionism: {P}roceedings
                  of the 1995 {S}wedish {C}onference on
                  {C}onnectionism},
  editor =   {Niklasson,L.F. and Boden,M.B.},
  pages =    {103--120},
  year =     1995,
}

@ARTICLE{tetko95overfitting,
  author =   {Tetko, I. V. and Livingstone, D. J. and Luik, A. I.},
  title =    {Neural network studies. 1. Comparison of overfitting
                  and overtraining},
  journal =  {Journal of Chemical Information & Computer Sciences},
  volume =   35,
  number =   5,
  pages =    {826-833},
  abstract =     {The application of feed forward back propagation
                  artificial neural networks with one hidden layer
                  (ANN) to perform the equivalent of multiple linear
                  regression (MLR) has been examined using artificial
                  structured data sets and real literature data. The
                  predictive ability of the networks has been
                  estimated using a training/test set protocol. The
                  results have shown advantages of ANN over MLR
                  analysis. The ANNs do not require high order terms
                  or indicator variables to establish complex
                  structure-activity relationships. Overfitting does
                  not have any influence on network prediction ability
                  when overtraining is avoided by
                  cross-validation. Application of ANN ensembles has
                  allowed the avoidance of chance correlations and
                  satisfactory predictions of new data have been
                  obtained for a wide range of numbers of neurons in
                  the hidden layer. [References: 30]},
  keywords =     {Qsar},
  year =     1995,
}

@INPROCEEDINGS{tresp95combining,
  author =   {Volker Tresp and Michiaki Taniguchi},
  title =    {Combining Estimators Using Non-Constant Weighting
                  Functions},
  pages =    {419--426},
  editor =   {G. Tesauro and D. Touretzky and T. Leen},
  volume =   7,
  booktitle =    {Advances in Neural Information Processing Systems},
  year =     1995,
  publisher =    {The {MIT} Press},
  abstract =     {This paper discusses the linearly weighted
                  combination of estimators in which the weighting
                  functions are dependent on the input. We show that
                  the weighting functions can be derived either by
                  evaluating the input dependent variance of each
                  estimator or by estimating how likely it is that a
                  given estimator has seen data in the region of the
                  input space close to the input pattern. The latter
                  solution is closely related to the mixture of
                  experts approach and we show how learning rules for
                  the mixture of experts can be derived from the
                  theory about learning with missing features. The
                  presented approaches are modular since the weighting
                  functions can easily be modified (no retraining) if
                  more estimators are added. Furthermore, it is easy
                  to incorporate estimators which were not derived
                  from data such as expert systems or algorithms.},
}

@TECHREPORT{tumerghosh95theoretical,
  author =   {Kagan Tumer and Joydeep Ghosh},
  title =    {Theoretical Foundations of Linear and Order
                  Statistics Combiners for Neural Pattern Classifiers},
  institution =  {Computer and Vision Research Center, University of
                  Texas, Austin},
  year =     1995,
  number =   {TR-95-02-98},
  url =      {http://citeseer.nj.nec.com/tumer96theoretical.html},
}

@INPROCEEDINGS{tumerghosh95boundary,
  author =   {K. Tumer and J. Ghosh},
  title =    {Boundary variance reduction for improved
                  classification through hybrid networks},
  booktitle =    {Proceedings of the Spie Conf. on Applications and
                  Science of Artificial Neural Networks IV},
  volume =   2492,
  pages =    {573--585},
  month =    {April},
  year =     1995,
  address =  {Orlando, FL},
  abstract =     {Several researchers have shown experimentally that
                  substantial improvements can be obtained in
                  difficult pattern recognition problems by combining
                  or integrating the outputs of multiple
                  classifiers. This paper provides an analytical
                  framework that quantifies the improvements in
                  classification results due to linear combination. We
                  show that combining networks in the output space
                  reduces the variance of the actual decision region
                  boundaries around the optimum boundary. In the
                  absence of network bias, the added classification
                  error is directly proportional to the boundary
                  variance. Moreover, if the network errors are
                  independent, then the reduction in variance boundary
                  location is by a factor of N, the number of
                  classifiers that are combined. In the presence of
                  network bias, the reductions are less than or equal
                  to N, depending on the interaction between network
                  biases. We discuss how the individual networks can
                  be selected to achieve significant gains through
                  combination, and we support them with experimental
                  results on 25-dimensional sonar data. The analysis
                  presented facilitates the understanding of the
                  relationships among error rates, classifier boundary
                  distributions and combination in the output space.},
}

@ARTICLE{tumerghosh95orderstats,
  author =   {K. Tumer and J. Ghosh},
  title =    {Order Statistics Combiners for Neural Classifiers},
  journal =  {World Congress on Neural Networks},
  volume =   {Vol. I},
  pages =    {31--34},
  publisher =    {INNS Press},
  address =  {Washington, DC},
  month =    {July},
  year =     1995,
  abstract =     {Several researchers have shown that linearly
                  combining outputs of multiple neural classifiers
                  results in better performance for many
                  applications. In this paper we introduce a family of
                  order statistics combiners as an alternative to
                  linear combiners. We show analytically that the
                  selection of the median, the maximum and in general,
                  the i-th order statistic improves classification
                  performance. Specifically, we show that order
                  statistics combiners reduce the variance of the
                  actual decision boundaries around the optimum
                  boundary, and that this is directly related to
                  classification error.},
}

@INCOLLECTION{yaoliu96,
  editor =   {R Stocker et al},
  author =   {Xin Yao and Yong Liu},
  pages =    {229-242},
  booktitle =    {Complex Systems - From Local Interactions to Global
                  Phenomena},
  publisher =    {IOS Press, Amsterdam},
  title =    {How to Make Best Use of Evolutionary Learning},
  year =     1996
}

@ARTICLE{parmanto96,
  author =   {B.Parmanto and P.W.Munro and H.R.Doyle},
  title =    {Improving Committee Diagnosis with Resampling
                  Techniques},
  editor =   {D.S.Touretzky and M.C.Mozer and M.E.Hesselmo},
  volume =   8,
  journal =  {Advances in Neural Information Processing Systems},
  year =     1996,
  pages =    {882--888},
  publisher =    {The {MIT} Press},
}

@TECHREPORT{breiman96arcing,
  author =   {Leo Breiman},
  title =    {Bias, Variance, and Arcing Classifiers},
  institution =  {Statistics Department, Berkeley},
  year =     1996,
  number =   460,
}

@ARTICLE{breiman96bagging,
  author =   {Leo Breiman},
  title =    {Bagging Predictors},
  journal =  {Machine Learning},
  volume =   24,
  number =   2,
  pages =    {123-140},
  year =     1996,
}

@INPROCEEDINGS{brodleylane96,
  author =   {C. Brodley and T. Lane},
  title =    {Creating and exploiting coverage and diversity},
  booktitle =    {AAAI-96 Workshop Integrating Multiple Learned
                  Models},
  year =     1996,
}

@INPROCEEDINGS{chan96local,
  author =   {Philip K.~Chan and Salvatore J.~Stolfo},
  title =    {Sharing Learned Models among Remote Database
                  Partitions by Local Meta-learning},
  booktitle =    {Proc. Second Intl. Conf. on Knowledge Discovery \&
                  Data Mining},
  year =     1996,
  pages =    {2--7},
}

@INCOLLECTION{darwenyao96,
  publisher =    {Springer-Verlag},
  author =   {Paul Darwen and Xin Yao},
  title =    {Every niching method has its niche: fitness sharing
                  and implicit sharing compared},
  year =     1996 ,
  booktitle =    {Proc. of Parallel Problem Solving from Nature (PPSN)
                  IV - Lecture Notes in Computer Science 1141},
}

@INPROCEEDINGS{freundschapire96experiments,
  author =   {Yoav Freund and Robert E. Schapire},
  title =    {Experiments with a new boosting algorithm},
  booktitle =    {Proceedings of the 13th International Conference on
                  Machine Learning},
  publisher =    {Morgan Kaufmann},
  year =     1996,
  pages =    {148--156},
  abstract =     {In an earlier paper, we introduced a new 'boosting'
                  algorithm called AdaBoost which, theoretically, can
                  be used to significantly reduce the error of any
                  learning algorithm that consistently generates
                  classifiers whose performance is a little better
                  than random guessing. We also introduced the related
                  notion of a 'pseudo-loss' which is a method for
                  forcing a learning algorithm of multi-label concepts
                  to concentrate on the labels that are hardest to
                  discriminate. In this paper, we describe experiments
                  we carried out to assess how well AdaBoost with and
                  without pseudo-loss, performs on real learning
                  problems. We performed two sets of experiments. The
                  first set compared boosting to Breiman's 'bagging'
                  method when used to aggregate various classifiers
                  (including decision trees and single attribute-value
                  tests). We compared the performance of the two
                  methods on a collection of machine-learning
                  benchmarks. In the second set of experiments, we
                  studied in more detail the performance of boosting
                  using a nearest-neighbor classifier on an OCR
                  problem.},
}

@TECHREPORT{friedman96,
  author =   {J.H. Friedman},
  title =    {Bias, Variance, 0-1 Loss and the Curse of
                  Dimensionality},
  institution =  {Stanford University},
  year =     1996,
}

@ARTICLE{intratoredelman96,
  author =   {N. Intrator and S. Edelman},
  title =    {Making a Low-Dimensional Representation Suitable for
                  Diverse Tasks},
  journal =  {Connection Science : Special Issue on Reuse of
                  Neural Networks Through Transfer},
  year =     1996,
  volume =   8,
  number =   2,
  pages =    {205-224},
}

@INPROCEEDINGS{jelonek96replicated,
  author =   {J. Jelonek},
  title =    {Generalization Capability of Homogeneous Voting
                  Classifier Based on Partially Replicated Data},
  series =   {Integrating Multiple Learned Models for Improving
                  and Scaling Machine Learning Algorithms Workshop},
  booktitle =    {AAAI'96},
  year =     1996,
  note =     {Portland, OR},
}

@INPROCEEDINGS{kohavi96bias,
  author =   {Ron Kohavi and David H. Wolpert},
  title =    {Bias Plus Variance Decomposition for Zero-One Loss
                  Functions},
  booktitle =    {Machine Learning: Proceedings of the Thirteenth
                  International Conference},
  publisher =    {Morgan Kaufmann},
  editor =   {Lorenza Saitta},
  pages =    {275--283},
  year =     1996,
  abstract =     {We present a bias-variance decomposition of expected
                  misclassification rate, the most commonly used loss
                  function in supervised classification learning. The
                  bias-variance decomposition for quadratic loss
                  functions is well known and serves as an important
                  tool for analyzing learning algorithms, yet no
                  decomposition was offered for the more commonly used
                  zero-one (misclassification) loss functions until
                  the work of Kong and Dietterich (1995) and Breiman
                  (1996). Their decomposition suffers from some major
                  shortcomings though (e.g., potentially negative
                  variance), which our decomposition avoids. We show
                  that, in practice, the naive frequency-based
                  estimation of the decomposition terms is by itself
                  biased and show how to correct for this bias. We
                  illustrate the decomposition on various algorithms
                  and datasets from the UCI repository.},
}

@ARTICLE{leblanc96combining,
  author =   {Michael LeBlanc and Robert Tibshirani},
  title =    {Combining Estimates in Regression and
                  Classification},
  journal =  {Journal of the American Statistical Association},
  volume =   91,
  number =   436,
  pages =    1641,
  year =     1996,
  url =      {http://citeseer.nj.nec.com/leblanc93combining.html},
}

@ARTICLE{opitz96:generating,
  author =   {David W. Opitz and Jude W. Shavlik},
  title =    {Generating Accurate and Diverse Members of a
                  Neural-Network Ensemble},
  pages =    {535--541},
  journal =  {NIPS},
  editor =   {David S. Touretzky and Michael C. Mozer and Michael
                  E. Hasselmo},
  volume =   8,
  year =     1996,
  publisher =    {The {MIT} Press},
  abstract =     {Neural-network ensembles have been shown to be very
                  accurate classification techniques. Previous work
                  has shown that an effective ensemble should consist
                  of networks that are not only highly correct, but
                  ones that make their errors on different parts of
                  the input space as well. Most existing techniques,
                  however, only indirectly address the problem of
                  creating such a set of networks. In this paper we
                  present a technique called ADDEMUP that uses genetic
                  algorithms to directly search for an accurate and
                  diverse set of trained networks. ADDEMUP works by
                  first creating an initial population, then uses
                  genetic operators to continually create new
                  networks, keeping the set of networks that are as
                  accurate as possible while disagreeing with each
                  other as much as possible. Experiments on three DNA
                  problems show that ADDEMUP is able to generate a set
                  of trained networks that is more accurate than
                  several existing approaches. Experiments also show
                  that ADDEMUP is able to effectively incorporate
                  prior knowledge, if available, to improve the
                  quality of its ensemble.},
}

@INPROCEEDINGS{ormoneit96improved,
  author =   {Dirk Ormoneit and Volker Tresp},
  title =    {Improved Gaussian Mixture Density Estimates Using
                  Bayesian Penalty Terms and Network Averaging},
  booktitle =    {Advances in Neural Information Processing Systems},
  volume =   8,
  publisher =    {The {MIT} Press},
  editor =   {David S. Touretzky and Michael C. Mozer and Michael
                  E. Hasselmo},
  pages =    {542--548},
  year =     1996,
}

@ARTICLE{partridge96network,
  author =   {D. Partridge},
  title =    {Network Generalization Differences Quantified},
  journal =  {Neural Networks},
  volume =   9,
  number =   2,
  pages =    {263--271},
  year =     1996,
  url =      {http://citeseer.nj.nec.com/partridge94network.html},
}

@ARTICLE{partridge96:engineering,
  author =   {D. Partridge and W. B. Yates},
  title =    {Engineering Multiversion Neural-Net Systems},
  journal =  {Neural Computation},
  volume =   8,
  number =   4,
  pages =    {869--893},
  year =     1996,
}

@ARTICLE{intratorraviv96:noise,
  volume =   8,
  pages =    {355-372},
  author =   {Yuval Raviv and Nathan Intrator},
  year =     1996,
  title =    {Bootstrapping with Noise: An Effective
                  Regularisation Technique},
  journal =  {Connection Science},
}

@ARTICLE{rosen96:decorrelated,
  volume =   8,
  number =   {3 and 4},
  title =    {Ensemble Learning using Decorrelated Neural
                  Networks},
  author =   {B.E. Rosen},
  journal =  {Connection Science - Special Issue on Combining
                  Artificial Neural Networks: Ensemble Approaches},
  year =     1996,
  pages =    {373--384},
  abstract =     {We describe a decorrelation network training method
                  for improving the quality of regression learning in
                  'ensemble' neural networks (NNs) that are composed
                  of linear combinations of individual NNs. In this
                  method, individual networks are trained by
                  backpropagation not only to reproduce a desired
                  output, but also to have their errors linearly
                  decorrelated with the other networks. Outputs from
                  the individual networks are then linearly combined
                  to produce the output of the ensemble network. We
                  demonstrate the performances of decorrelated network
                  training on learning the 'three-parity' logic
                  function, a noisy sine function and a
                  one-dimensional non-linear function, and compare the
                  results with the ensemble networks composed of
                  independently trained individual networks (without
                  decorrelation training). Empirical results show that
                  when individual networks are forced to be
                  decorrelated with one another the resulting ensemble
                  NNs have lower mean squared errors than the ensemble
                  networks having independently trained individual
                  networks. This method is particularly applicable
                  when there is insufficient data to train each
                  individual network on disjoint subsets of training
                  patterns.},
}

@ARTICLE{sharkeychandroth96,
  title =    {Diverse Neural Net Solutions to a Fault Diagnosis
                  Problem},
  year =     1996,
  journal =  {Neural Computing and Applications},
  volume =   4 ,
  author =   {Amanda Sharkey and Noel Sharkey and Gopinath
                  Chandroth},
  pages =    {218--227},
}

@ARTICLE{sollichkrogh96:overfitting,
  author =   {P. Sollich and A. Krogh},
  title =    {Learning with ensembles: How overfitting can be
                  useful},
  pages =    {190--196},
  booktitle =    {Advances in Neural Information Processing Systems},
  editor =   {David S. Touretzky and Michael C. Mozer and Michael
                  E. Hasselmo},
  volume =   8,
  year =     1996,
  publisher =    {The {MIT} Press},
  abstract =     {We study the characteristics of learning with
                  ensembles. Solving exactly the simple model of an
                  ensemble of linear students, we find surprisingly
                  rich behaviour. For learning in large ensembles, it
                  is advantageous to use under-regularized students,
                  which actually over-fit the training data. Globally
                  optimal performance can be obtained by choosing the
                  training set sizes of the students
                  appropriately. For smaller ensembles, optimization
                  of the ensemble weights can yield significant
                  improvements in ensemble generalization performance,
                  in particular if the individual students are subject
                  to noise in the training process. Choosing students
                  with a wide range of regularization parameters makes
                  this improvement robust against changes in the
                  unknown level of noise in the training data.},
}

@ARTICLE{tetko96variable,
  author =   {Tetko, I. V. and Villa, A. E. and Livingstone,
                  D. J.},
  title =    {Neural network studies. 2. Variable selection},
  journal =  {Journal of Chemical Information & Computer Sciences},
  volume =   36,
  number =   4,
  pages =    {794-803},
  abstract =     {Quantitative structure-activity relationship (QSAR)
                  studies usually require an estimation of the
                  relevance of a very large set of initial
                  variables. Determination of the most important
                  variables allows theoretically a better
                  generalization by all pattern recognition
                  methods. This study introduces and investigates five
                  pruning algorithms designed to estimate the
                  importance of input variables in feed-forward
                  artificial neural network trained by back
                  propagation algorithm (ANN) applications and to
                  prune nonrelevant ones in a statistically reliable
                  way. The analyzed algorithms performed similar
                  variable estimations for simulated data sets, but
                  differences were detected for real QSAR
                  examples. Improvement of ANN prediction ability was
                  shown after the pruning of redundant input
                  variables. The statistical coefficients computed by
                  ANNs for QSAR examples were better than those of
                  multiple linear regression. Restrictions of the
                  proposed algorithms and the potential use of ANNs
                  are discussed.},
  keywords =     {Databases, Factual Linear Models *Neural Networks
                  (Computer) Nonlinear Dynamics Pharmaceutical
                  Preparations/ch [Chemistry] Structure-Activity
                  Relationship Support, Non-U.S. Gov\'t},
  year =     1996,
}

@ARTICLE{tumerghosh96,
  author =   {Kagan Tumer and Joydeep Ghosh},
  title =    {Error Correlation and Error Reduction in Ensemble
                  Classifiers},
  journal =  {Connection Science},
  volume =   8,
  number =   {3-4},
  pages =    {385--403},
  year =     1996,
  abstract =     {Using an ensemble of classifiers, instead of a
                  single classifier, can lead to improved
                  generalization. The gains obtained by combining,
                  however, are often affected more by the selection of
                  what is presented to the combiner than by the actual
                  combining method that is chosen. In this paper, we
                  focus on data selection and classifier training
                  methods, in order to 'prepare' classifiers for
                  combining. We review a combining framework for
                  classification problems that quantifies the need for
                  reducing the correlation among individual
                  classifiers. Then, we discuss several methods that
                  make the classifiers in an ensemble more
                  complementary. Experimental results are provided to
                  illustrate the benefits and pitfalls of reducing the
                  correlation among classifiers, especially when the
                  training data are in limited supply.},
}

@ARTICLE{tumerghosh96analysis,
  author =   {Kagan Tumer and Joydeep Ghosh},
  title =    {Analysis of decision boundaries in linearly combined
                  neural classifiers},
  journal =  {Pattern Recognition},
  year =     1996,
  volume =   29,
  number =   2,
  pages =    {341--348},
  month =    {February},
}

@INPROCEEDINGS{tumerghosh96estimating,
  author =   {Kagan Tumer and Joydeep Ghosh},
  title =    {Estimating the Bayes Error Rate Through Classifier
                  Combining},
  booktitle =    {Proceedings of the 13th International Conference on
                  Pattern Recognition},
  month =    {August},
  year =     1996,
  abstract =     {The Bayes error provides the lowest achievable error
                  rate for a given pattern classification
                  problem. There are several classical approaches for
                  estimating or finding bounds for the Bayes
                  error. One type of approach focuses on obtaining
                  analytical bounds, which are both difficult to
                  calculate and dependent on distribution parameters
                  that may not be known. Another strategy is to
                  estimate the class densities through non-parametric
                  methods, and use these estimates to obtain bounds on
                  the Bayes error. This article presents a novel
                  approach to estimating the Bayes error based on
                  classifier combining techniques. For an artificial
                  data set where the Bayes error is known, the
                  combiner-based estimate outperforms the classical
                  methods.},
}

@INPROCEEDINGS{uedanakano96,
  booktitle =    {Proceedings of International Conference on Neural
                  Networks},
  title =    {Generalization Error of Ensemble Estimators},
  author =   {N. Ueda and R. Nakano},
  year =     1996 ,
  pages =    {90--95},
}

@ARTICLE{partridge95use,
  author =   {W. Yates and D. Partridge},
  title =    {Use of methodological diversity to improve neural
                  network generalization},
  journal =  {Neural Computing and Applications},
  year =     1996,
  volume =   4,
  pages =    {114--128},
  number =   2,
  url =      {http://citeseer.nj.nec.com/partridge95use.html},
}

@ARTICLE{chan98accuracy,
  author =   {Philip K.~Chan and Salvatore J.~Stolfo},
  title =    {On the Accuracy of Meta-learning for Scalable Data
                  Mining,},
  journal =  {Journal of Intelligent Information Systems},
  year =     1997,
  volume =   8,
  pages =    {5--28},
}

@ARTICLE{darwenyao97,
  author =   {Paul J. Darwen and Xin Yao},
  title =    {Speciation as Automatic Categorical Modularization},
  journal =  {{IEEE} {T}rans. on {E}volutionary {C}omputation},
  volume =   1,
  number =   2,
  pages =    {100--108},
  year =     1997,
}

@INPROCEEDINGS{domingos97,
  title =    {Why Does Bagging Work? {A} Bayesian Account and its
                  Implications},
  author =   {P.~Domingos},
  pages =    155,
  booktitle =    {Proceedings of the Third International Conference on
                  Knowledge Discovery and Data Mining ({KDD}-97)},
  year =     1997,
  editor =   {David Heckerman and Heikki Mannila and Daryl
                  Pregibon and Ramasamy Uthurusamy},
  publisher =    {AAAI Press},
  url =      {http://www.boosting.org/papers/Dom97.ps.gz},
  ps =       {http://www.boosting.org/papers/Dom97.ps},
  psgz =     {http://www.boosting.org/papers/Dom97.ps.gz},
  pdf =      {http://www.boosting.org/papers/Dom97.pdf},
}

@INCOLLECTION{edelmanintrator97,
  author =   {S. Edelman and N. Intrator},
  title =    {Learning as Extraction of Low-Dimensional
                  Representations},
  booktitle =    {Mechanisms of Perceptual Learning},
  publisher =    {Academic Press},
  year =     1997,
  editor =   {D. Medlin, R. Goldstone, P. Schyns},
}

@ARTICLE{hashem97optimal,
  author =   {Sherif Hashem},
  title =    {{O}ptimal {L}inear {C}ombinations of {N}eural
                  {N}etworks},
  journal =  {Neural Networks},
  volume =   10,
  number =   4,
  month =    {August},
  pages =    {599-614},
  year =     1997,
}

@ARTICLE{jacobs97analyses,
  pages =    {369--383},
  author =   {Robert Jacobs},
  year =     1997,
  journal =  {Neural Computation},
  volume =   9,
  title =    {Bias-Variance Analyses of Mixture-of-Experts
                  Architectures},
}

@ARTICLE{jima97weak,
  author =   {C. Ji and S. Ma},
  title =    {Combinations of Weak Classifiers},
  journal =  {IEEE Trans. on Neural Networks, Special Issue on
                  Neural Networks and Pattern Recognition},
  volume =   8,
  year =     1997,
  month =    {January},
  pages =    {32--42},
}

@INPROCEEDINGS{kang97statistical,
  author =   {Kukjin Kang and Jong-Hoon Oh},
  title =    {Statistical Mechanics of the Mixture of Experts},
  booktitle =    {Advances in Neural Information Processing Systems},
  volume =   9,
  publisher =    {The {MIT} Press},
  editor =   {Michael C. Mozer and Michael I. Jordan and Thomas
                  Petsche},
  pages =    183,
  year =     1997,
  url =      {http://citeseer.nj.nec.com/kang97statistical.html},
}

@ARTICLE{kroghsollich97:statistical_mechanics,
  title =    {Statistical mechanics of ensemble learning},
  author =   {A. Krogh and P. Sollich},
  journal =  {Physical Review E},
  year =     1997,
  volume =   55,
  number =   {1PtB},
  pages =    {811--825},
  abstract =     {Within the context of learning a rule from examples,
                  we study the general characteristics of learning,
                  with ensembles. The generalization performance
                  achieved by a simple model ensemble of linear
                  students is calculated exactly in the thermodynamic
                  limit of a large number of input components and
                  shows a surprisingly rich behavior. Our main
                  findings are the following. For learning in large
                  ensembles, it is advantageous to use
                  underregularized students, which actually overfit
                  the training data. Globally optimal generalization
                  performance can be obtained by choosing the training
                  set sizes of the students optimally. For smaller
                  ensembles, optimization of the ensemble weights can
                  yield significant improvements in ensemble
                  generalization performance, in particular if the
                  individual students are subject to noise in the
                  training process. Choosing students with a wide
                  range of regularization parameters makes this
                  improvement robust against changes in the unknown
                  level of corruption of the training data.},
}

@ARTICLE{liuyao97:negatively,
  title =    {Negatively Correlated Neural Networks can Produce
                  Best Ensembles},
  author =   {Y. Liu and X. Yao},
  number =   {3/4},
  year =     1997,
  journal =  {Australian Journal of Intelligent Information
                  Processing Systems},
  pages =    {176--185},
  volume =   4,
}

@INPROCEEDINGS{maclin97empirical,
  author =   {Richard Maclin and David Opitz},
  title =    {An Empirical Evaluation of Bagging and Boosting},
  booktitle =    {{AAAI}/{IAAI}},
  pages =    {546-551},
  year =     1997,
  url =      {http://citeseer.nj.nec.com/maclin97empirical.html},
}

@MISC{mak97combining,
  author =   {B. Mak},
  title =    {Combining ANNs to improve phoneme recognition},
  text =     {B. Mak. Combining ANNs to improve phoneme
                  recognition. ICASSP, 4:3253--3256, 1997.},
  year =     1997,
  url =      {http://citeseer.nj.nec.com/mak97combining.html},
}

@INPROCEEDINGS{margineantu97pruning,
  author =   {Dragos D. Margineantu and Thomas G. Dietterich},
  title =    {Pruning adaptive boosting},
  booktitle =    {Proc. 14th International Conference on Machine
                  Learning},
  publisher =    {Morgan Kaufmann},
  pages =    {211--218},
  year =     1997,
  url =          {http://citeseer.nj.nec.com/margineantu97pruning.html},
}

@INPROCEEDINGS{merz97combining,
  author =   {Christopher J. Merz and Michael J. Pazzani},
  title =    {Combining Neural Network Regression Estimates with
                  Regularized Linear Weights},
  booktitle =    {Advances in Neural Information Processing Systems},
  volume =   9,
  publisher =    {The {MIT} Press},
  editor =   {Michael C. Mozer and Michael I. Jordan and Thomas
                  Petsche},
  pages =    564,
  year =     1997,
}

@INBOOK{murphy97exploring,
  author =   {Patrick M. Murphy and Michael J. Pazzani},
  title =    {Exploring the decision forest: an empirical
                  investigation of Occam's razor in decision tree
                  induction},
  booktitle =    {Computational Learning Theory and Natural Learning
                  Systems},
  volume =   {IV: Making Learning Systems Practical},
  publisher =    {MIT Press},
  pages =    {171--187},
  year =     1997,
  url =      {http://citeseer.nj.nec.com/murphy94exploring.html},
}

@ARTICLE{naftaly97:optimal,
  author =   {Ury Naftaly and Nathan Intrator and David Horn},
  year =     1997,
  volume =   8,
  title =    {Optimal Ensemble Averaging of Neural Networks},
  month =    {May},
  number =   3 ,
  pages =    {283--296},
  journal =  {Network},
}

@INBOOK{ohkang97,
  author =   {Jong-Hoon Oh and Kookjin Kang},
  editor =   {K.Y.M.Wong and D.Yan},
  title =    {Theoretical Aspects of Neural Computation: A
                  Multidisciplinary Perspective},
  chapter =  {Experts or Ensemble? A statistical Mechanics of
                  Multiple Neural Network Approaches},
  publisher =    {Springer, Heidelberg},
  pages =    {81--92},
  year =     1997,
}

@ARTICLE{sharkey97combining,
  author =   {A. Sharkey and N. Sharkey},
  title =    {Combining diverse neural networks},
  journal =  {The Knowledge Engineering Review},
  year =     1997,
  volume =   12,
  pages =    {231-247},
  number =   3,
}

@INPROCEEDINGS{sharkey97:diversity,
  booktitle =    {Neural Networks and their Applications (NEURAP'97)},
  pages =    {205--212},
  year =     1997 ,
  author =   {Amanda Sharkey and Noel Sharkey},
  title =    {Diversity, Selection, and Ensembles of Artificial
                  Neural Nets},
}

@ARTICLE{sharkey97:arm,
  author =   {Noel Sharkey},
  title =    {Artificial Neural Networks for Coordination and
                  Control: The Portability of Experimental
                  Representations},
  journal =  {Robotics and Autonomous Systems},
  year =     1997,
  volume =   22,
  pages =    {345-360},
}

@ARTICLE{taniguchi97averaging,
  author =   {Michiaki Taniguchi and Volker Tresp},
  title =    {Averaging Regularized Estimators},
  journal =  {Neural Computation},
  volume =   9,
  number =   5,
  pages =    {1163-1178},
  year =     1997,
}

@ARTICLE{tetko97epa,
  author =   {Tetko, I. V. and Villa, A. E.},
  title =    {An efficient partition of training data set improves
                  speed and accuracy of cascade-correlation algorithm},
  journal =  {Neural Processing Letters},
  volume =   6,
  number =   {1-2},
  pages =    {51-59},
  abstract =     {This study extends an application of efficient
                  partition algorithm (EPA) for artificial neural
                  network ensemble trained according to Cascade
                  Correlation Algorithm. We show that EPA allows to
                  decrease the number of cases in learning and
                  validated data sets. The predictive ability of the
                  ensemble calculated using the whole data set is not
                  affected and in some cases it is even improved. It
                  is shown that a distribution of cases selected by
                  this method is proportional to the second derivative
                  of the analyzed function},
  keywords =     {algorithm, cascade correlation, early stopping,
                  efficient partition of training data set},
  year =     1997,
}

@ARTICLE{tetko97overfitting,
  author =   {Tetko, I. V. and Villa, A. E.},
  title =    {An enhancement of generalization ability in cascade
                  correlation algorithm by avoidance of
                  overfitting/overtraining problem},
  journal =  {Neural Processing Letters},
  volume =   6,
  number =   {1-2},
  pages =    {43-50},
  abstract =     {The current study investigates a method for
                  avoidance of an overfitting/overtraining problem in
                  Artificial Neural Network (ANN) based on a
                  combination of two algorithms: Early Stopping and
                  Ensemble averaging (ESE). We show that ESE provides
                  an improvement of the prediction ability of ANN
                  trained according to Cascade Correlation
                  Algorithm. A simple algorithm to estimate the
                  generalization ability of the method according to
                  the Leave-One-Out technique is proposed and
                  discussed. In the accompanying paper the problem of
                  optimal selection of training cases is considered
                  for accelerated learning of the ESE method},
  keywords =     {cascade correlation algorithm, early stopping,
                  overfitting, overtraining},
  year =     1997,
}

@ARTICLE{tetko97partition,
  author =   {Tetko, I. V. and Villa, A. E.},
  title =    {Efficient Partition of Learning Data Sets for Neural
                  Network Training},
  journal =  {Neural Networks},
  volume =   10,
  number =   8,
  pages =    {1361-1374.},
  abstract =     {This study investigates the emerging possibilities
                  of combining unsupervised and supervised learning in
                  neural network ensembles. Such strategy is used to
                  get an efficient partition of a noisy input data set
                  in order to focus the training of neural networks on
                  the most complex and informative domains of the data
                  set and accelerate the learning phase. The proposed
                  algorithm provides a good prediction accuracy using
                  fewer cases from non-informative domains according
                  to a correlative measure of dependency between cases
                  of the training set. This measure takes into account
                  internal relationships amid analyzed data and can be
                  used to cluster neighbor cases in a multidimensional
                  space and to filter out the outliers. The possible
                  relation of the proposed algorithm to brain
                  processing occurring in the thalamo-cortical pathway
                  is discussed.},
  year =     1997,
}

@INPROCEEDINGS{ting97stacked,
  author =   {Kai Ming Ting and Ian H. Witten},
  title =    {Stacked Generalization: When Does It Work?},
  booktitle =    {{IJCAI} (2)},
  pages =    {866--873},
  year =     1997,
  url =      {http://citeseer.nj.nec.com/ting97stacked.html},
}

@INPROCEEDINGS{wan97:sunspots,
  author =   {Eric A. Wan},
  booktitle =    {International Conference On Neural Networks
                  (ICNN97)},
  year =     1997 ,
  title =    {Combining Fossil and Sunspot Data: Committee
                  Predictions},
  url =      {http://citeseer.ist.psu.edu/146595.html},
}

@ARTICLE{windeatt97spectral,
  author =   {Windeatt, T. and Tebbs, R.},
  title =    {Spectral technique for hidden layer neural network
                  training},
  journal =  {Pattern Recognition Letters},
  volume =   18,
  issue =    8,
  pages =    723,
  year =     1997,
  month =    {August},
}

@ARTICLE{wolpert97bias,
  author =   {David Wolpert},
  title =    {On Bias Plus Variance},
  journal =  {Neural Computation},
  volume =   9,
  number =   6,
  pages =    {1211-1243},
  year =     1997,
  url =          {http://citeseer.nj.nec.com/article/wolpert96bias.html},
}

@ARTICLE{woods97:local,
  author =   {K. Woods and W.P. Kegelmeyer and K. Bowyer},
  title =    {Combination of multiple classifiers using local
                  accuracy estimates},
  journal =  {IEEE Transactions on Pattern Analysis and Machine
                  Intelligence},
  year =     1997,
  volume =   19,
  pages =    {405-410},
}

@ARTICLE{yaoliu97:epnet,
  author =   {Xin Yao and Yong Liu},
  number =   3,
  pages =    {694--713},
  journal =  {IEEE Transactions on Neural Networks},
  year =     1997,
  title =    {A new evolutionary system for evolving artificial
                  neural networks},
  month =    {May},
  volume =   8,
}

@MISC{uci,
  author =   {C.L. Blake and C.J. Merz},
  year =     1998,
  title =    {{UCI} Repository of machine learning databases},
  url =          {http://www.ics.uci.edu/$\sim$mlearn/MLRepository.html},
  institution =  {University of California, Irvine, Dept. of
                  Information and Computer Sciences},
}

@TECHREPORT{breiman98randomizing,
  author =   {Leo Breiman},
  title =    {Randomizing Outputs to increase prediction accuracy},
  institution =  {Statistics Department, University of California},
  month =    {May},
  year =     1998,
  type =     {Technical Report},
  number =   518,
  url =      {http://www.boosting.org/papers/Bre98.pdf},
}

@ARTICLE{dietterich98approximate,
  author =   {Thomas G. Dietterich},
  title =    {Approximate Statistical Test For Comparing
                  Supervised Classification Learning Algorithms},
  journal =  {Neural Computation},
  volume =   10,
  number =   7,
  pages =    {1895-1923},
  year =     1998,
  url =      {citeseer.nj.nec.com/dietterich98approximate.html},
  abstract =     {This paper reviews five approximate statistical
                  tests for determining whether one learning algorithm
                  out-performs another on a particular learning
                  task. These tests are compared experimentally to
                  determine their probability of incorrectly detecting
                  a difference when no difference exists (type I
                  error). Two widely-used statistical tests are shown
                  to have high probability of Type I error in certain
                  situations and should never be used. These tests are
                  (a) a test for the difference of two proportions and
                  (b) a paired-differences t test based on taking
                  several random train/test splits. A third test, a
                  paired-differences t test based on 10-fold
                  cross-validation, exhibits somewhat elevated
                  probability of Type I error. A fourth test,
                  McNemar's test, is shown to have low Type I
                  error. The fifth test is a new test, 5x2cv, based on
                  5 iterations of 2-fold cross-validation. Experiments
                  show that this test also has acceptable Type I
                  error.},
}

@INPROCEEDINGS{domingos98occams,
  author =   {Pedro Domingos},
  title =    {Occam's Two Razors: The Sharp and the Blunt},
  booktitle =    {Proceedings of the Fourth International Conference
                  on Knowledge Discovery and Data Mining},
  publisher =    {AAAI Press},
  year =     1998,
}

@INPROCEEDINGS{feraud98ensemblemodular,
  author =   {Rapha{\"e}l Feraud and Olivier Bernier},
  title =    {Ensemble and Modular Approaches for Face Detection:
                  {A} Comparison},
  booktitle =    {Advances in Neural Information Processing Systems},
  volume =   10,
  year =     1998,
  publisher =    {The {MIT} Press},
  editor =   {Michael I. Jordan and Michael J. Kearns and Sara
                  A. Solla},
}

@ARTICLE{ho98subspaces,
  author =   {T.K. Ho},
  title =    {The Random Subspace Method for Constructing Decision
                  Forests},
  journal =  {IEEE Trans. on Pattern Analysis and Machine
                  Intelligence},
  year =     1998,
  volume =   20,
  number =   8,
  pages =    {832--844},
  month =    {August},
}

@ARTICLE{husmeier98overfitting,
  author =   {Husmeier D., Althoefer K.},
  title =    {Modelling conditional probabilities with network
                  committees: how overfitting can be useful},
  journal =  {Neural Network World},
  year =     1998,
  volume =   8,
  numver =   4,
  pages =    {417--439},
}

@INPROCEEDINGS{jimenez98dynamically,
  author =   {D. Jimenez and N. Walsh},
  title =    {Dynamically weighted ensemble neural networks for
                  classification},
  booktitle =    {Proceedings of the International Joint Conference on
                  Neural Networks},
  year =     1998,
  url =          {http://citeseer.nj.nec.com/jimenez98dynamically.html},
}

@ARTICLE{kovalishyn98cascade,
  author =   {Kovalishyn, V. V. and Tetko, I. V. and Luik,
                  A. I. and Kholodovych, V. V. and Villa, A. E. P. and
                  Livingstone, D. J.},
  title =    {Neural network studies. 3. Variable selection in the
                  cascade-correlation learning architecture},
  journal =  {Journal of Chemical Information & Computer Sciences},
  volume =   38,
  number =   4,
  pages =    {651-659},
  abstract =     {Pruning methods for feed-forward artificial neural
                  networks trained by the cascade-correlation learning
                  algorithm are proposed. The cascade-correlation
                  algorithm starts with a small network and
                  dynamically adds new nodes until the analyzed
                  problem has been solved. This feature of the
                  algorithm removes the requirement to predefine the
                  architecture of the neural network prior to network
                  training. The developed pruning methods are used to
                  estimate the importance of large sets of initial
                  variables for quantitative structure-activity
                  relationship studies and simulated data sets. The
                  calculated results are compared with the performance
                  of fixed-size back-propagation neural networks and
                  multiple regression analysis and are carefully
                  validated using different training/test set
                  protocols, such as leave-one-out and full
                  cross-validation procedures. The results suggest
                  that the pruning methods can be successfully used to
                  optimize the set of variables for the
                  cascade-correlation learning algorithm neural
                  networks. The use of variables selected by the
                  elaborated methods provides an improvement of neural
                  network prediction ability compared to that
                  calculated using the unpruned sets of variables.},
  year =     1998,
}

@PHDTHESIS{liu98:thesis,
  author =   {Y. Liu},
  title =    {Negative Correlation Learning and Evolutionary
                  Neural Network Ensembles},
  school =   {University College, The University of New South
                  Wales, Australian Defence Force Academy, Canberra,
                  Australia},
  year =     1998,
}

@INPROCEEDINGS{liuyao98:towards,
  month =    {February},
  author =   {Yong Liu and Xin Yao},
  year =     1998 ,
  booktitle =    {Proceedings of International Symposium on Artificial
                  Life and Robotics (AROB)},
  pages =    {265-268},
  title =    {Towards Designing Neural Network Ensembles by
                  Evolution},
}

@PHDTHESIS{merz98thesis,
  author =   {C.J. Merz},
  title =    {Classification and Regression by Combining Models},
  school =   {University of California, Irvine},
  year =     1998,
  abstract =     {Two novel methods for combining predictors are
                  introduced in this thesis; one for the task of
                  regression, and the other for the task of
                  classification. The goal of combining the
                  predictions of a set of models is to form an
                  improved predictor. This dissertation demonstrates
                  how a combining scheme can rely on the stability of
                  the consensus opinion and, at the same time,
                  capitalize on the unique contributions of each
                  model. An empirical evaluation reveals that the new
                  methods consistently perform as well or better than
                  existing combining schemes for a variety of
                  prediction problems. The success of these algorithms
                  is explained empirically and analytically by
                  demonstrating how they adhere to a set of
                  theoretical and heuristic guidelines. A byproduct of
                  the empirical investigation is the evidence that
                  existing combining methods fail to satisfy one or
                  more of the guidelines defined. The new combining
                  approaches satisfy these criteria by relying upon
                  Singular Value Decomposition as a tool for filtering
                  out the redundancy and noise in the predictions of
                  the learn models, and for characterizing the areas
                  of the example space where each model is
                  superior. The SVD-based representation used in the
                  new combining methods aids in avoiding sensitivity
                  to correlated predictions without discarding any
                  learned models. Therefore, the unique contributions
                  of each model can still be discovered and
                  exploited. An added advantage of the combining
                  algorithms derived in this dissertation is that they
                  are not limited to models generated by a single
                  algorithm; they may be applied to model sets
                  generated by a diverse collection of machine
                  learning and statistical modeling methods.},
}

@INPROCEEDINGS{murata98bias,
  author =   {N. Murata},
  title =    {Bias of estimators and regularization terms},
  booktitle =    {Proceedings of Workshop on Information-Based
                  Induction Sciences},
  pages =    {87--94},
  year =     1998,
  month =    {July},
  url =      {http://citeseer.nj.nec.com/murata98bias.html},
}

@INBOOK{neal98relevance,
  author =   {R. M. Neal},
  editor =   {C. M. Bishop},
  title =    {Neural Networks and Machine Learning},
  chapter =  {Assessing relevance determination methods using
                  DELVE},
  publisher =    {Springer-Verlag},
  year =     1998,
  pages =    {97--129},
}

@ARTICLE{prechelt98automatic,
  author =   {Lutz Prechelt},
  title =    {Automatic early stopping using cross validation:
                  quantifying the criteria},
  journal =  {Neural Networks},
  volume =   11,
  number =   4,
  pages =    {761--767},
  year =     1998,
  url =      {http://citeseer.nj.nec.com/prechelt98automatic.html},
}

@ARTICLE{rahman98,
  author =   {A.F.R. Rahman and M.C. Fairhurst},
  title =    {An evaluation of multi-expert configurations for the
                  recognition of handwritten numerals},
  journal =  {Pattern Recognition},
  year =     1998,
  number =   9,
  pages =    {1255--1273},
}

@INPROCEEDINGS{schapire98improved,
  author =   {Robert E. Schapire and Yoram Singer},
  title =    {Improved Boosting Algorithms using Confidence-Rated
                  Predictions},
  booktitle =    {Computational Learning Theory},
  pages =    {80-91},
  year =     1998,
  url =      {http://citeseer.nj.nec.com/schapire99improved.html},
}

@TECHREPORT{scott98parcel,
  author =   {M. Scott and M. Niranjan and R. Prager},
  title =    {Parcel: Feature subset selection in variable cost
                  domains},
  number =   {CUED/F-INFENG/TR 323},
  institution =  {Cambridge University},
  year =     1998,
  url =      {http://citeseer.nj.nec.com/scott98parcel.html},
}

@INPROCEEDINGS{sharkey98:adapting,
  author =   {Amanda Sharkey and Noel Sharkey and Simon Cross},
  booktitle =    {ICANN `98},
  publisher =    {Springer-Verlag},
  title =    {Adapting an ensemble approach for the diagnosis of
                  breast cancer},
  year =     1998,
  pages =    {281-286},
}

@ARTICLE{sierra98global,
  author =   {A. Sierra and C. Cruz},
  title =    {Global and Local Neural Network Ensembles},
  journal =  {Pattern Recognition Letters},
  volume =   19,
  number =   8,
  pages =    {651--655},
  year =     1998,
  pdf =      {http://dx.doi.org/10.1016/S0167-8655(98)00042-7},
}

@ARTICLE{swann98:fastcommittee,
  volume =   34,
  number =   14,
  title =    {Fast Committee Learning: Preliminary Results},
  month =    {July},
  journal =  {Electronics Letters},
  pages =    {1408-1410},
  year =     1998,
  author =   {A. Swann and N. Allinson},
}

@ARTICLE{tetko98pruning,
  author =   {Tetko, I. V. and Villa, A. E. P. and Aksenova,
                  T. I. and Zielinski, W. L. and Brower, J. and
                  Collantes, E. R. and Welsh, W. J.},
  title =    {Application of a pruning algorithm to optimize
                  artificial neural networks for pharmaceutical
                  fingerprinting},
  journal =  {Journal of Chemical Information & Computer Sciences},
  volume =   38,
  number =   4,
  pages =    {660-668},
  abstract =     {The present study investigates an application of
                  artificial neural networks (ANNs) for use in
                  pharmaceutical fingerprinting. Several pruning
                  algorithms were applied to decrease the dimension of
                  the input parameter data set. A localized
                  fingerprint region was identified within the
                  original input parameter space from which a subset
                  of input parameters was extracted leading to
                  enhanced ANN performance. The present results
                  confirm that ANNs can provide a fast, accurate, and
                  consistent methodology applicable to pharmaceutical
                  fingerprinting. [References: 26]},
  year =     1998,
}

@ARTICLE{tumer98rbfmedical,
  author =   {Kagan Tumer and Nirmala Ramanujam and Joydeep Ghosh
                  and Rebecca Richards-Kortum},
  title =    {Ensembles of Radial Basis Function Networks for
                  Spectroscopic Detection of Cervical Pre-Cancer},
  journal =  {IEEE Transactions on Biomedical Engineering},
  year =     1998,
  volume =   45,
  number =   8,
  pages =    {953--961},
  abstract =     {Medical applications usually used Radial Basis
                  Function Networks just as Artificial Neural
                  Networks. However, RBFNs are Knowledge-Based
                  Networks that can be interpreted in several way:
                  Artificial Neural Networks, Regularization Networks,
                  Support Vector Machines, Wavelet Networks, Fuzzy
                  Controllers, Kernel Estimators, Instanced-Based
                  Learners. A survey of their interpretations and of
                  their corresponding learning algorithms is provided
                  as well as a brief survey on dynamic learning
                  algorithms. RBFNs' interpretations can suggest
                  applications that are particularly interesting in
                  medical domains. },
}

@INPROCEEDINGS{wezel98maximum,
  author =   {M.C. van Wezel and W.A. Kosters and J.N. Kok},
  title =    {Maximum Likelihood Weights for a Linear Ensemble of
                  Regression Neural Networks},
  booktitle =    {Proceedings International Conference in Neural
                  Information Processing (ICONIP\'98)},
  year =     1998,
  publisher =    {IOS Press},
  editor =   {S. Usui and T. Omori},
  pages =    {498--501},
  address =  {Kitakyushu, Japan},
}

@INCOLLECTION{yaoliu98:making_use,
  pages =    {417--425},
  booktitle =    {IEEE Transactions on Systems, Man and Cybernetics,
                  Part B: Cybernetics},
  year =     1998,
  volume =   28,
  number =   3,
  publisher =    {IEEE Press},
  title =    {Making use of Population Information in Evolutionary
                  Artificial Neural Networks},
  month =    {June},
  author =   {Xin Yao and Yong Liu},
}

@ARTICLE{avnimelech99boostedregression,
  author =   {R.~Avnimelech and N.~Intrator},
  title =    {Boosting Regression Estimators},
  journal =  {Neural Computation},
  year =     1999,
  volume =   11,
  pages =    {491--513},
}

@ARTICLE{avnimelechintrator99boosted,
  pages =    {475-490},
  author =   {Ran Avnimelech and Nathan Intrator},
  year =     1999,
  journal =  {Neural Computation},
  volume =   11,
  title =    {Boosted Mixtures of Experts: An Ensemble Learning
                  Scheme},
}

@ARTICLE{bauer99empirical,
  author =   {E. Bauer and R. Kohavi},
  title =    {An Empirical Comparison of Voting Classification
                  Algorithms: Bagging, Boosting, and Variants},
  journal =  {Machine Learning},
  year =     1999,
  volume =   36,
  number =   {1,2},
}

@TECHREPORT{breiman99random,
  author =   {Leo Breiman},
  year =     1999,
  title =    {Random Forests Random Features},
  institution =  {University of California, Berkley (Dept of
                  Statistics)},
  number =   567,
}

@TECHREPORT{carneytuning99,
  author =   {John Carney and Padraig Cunningham},
  title =    {Tuning diversity in bagged neural network ensembles},
  institution =  {Trinity College Dublin},
  year =     1999,
  number =   {TCD-CS-1999-44},
}

@ARTICLE{hoeting99bma,
  author =   {D. Hoeting, J. A.and Madigan and C.T. Raftery,
                  A.E.and Volinsky},
  title =    {Bayesian model averaging: A tutorial},
  journal =  {Statistical Science},
  year =     1999,
  volume =   44,
  number =   4,
  pages =    {382--417},
  url =          "http://www.stat.washington.edu/www/research/online/hoeting1999.pdf",
  url =      {http://citeseer.nj.nec.com/context/1052498/0},
}

@ARTICLE{deb99multiobjective,
  author =   {Kalyanmoy Deb},
  title =    {Multi-objective Genetic Algorithms: Problem
                  Difficulties and Construction of Test Problems},
  journal =  {Evolutionary Computation},
  volume =   7,
  number =   3,
  pages =    {205-230},
  year =     1999,
}

@INPROCEEDINGS{fan99adacost,
  author =   {Wei Fan and Salvatore J. Stolfo and Junxin Zhang and
                  Philip K. Chan},
  title =    {Ada{C}ost: misclassification cost-sensitive
                  boosting},
  booktitle =    {Proc. 16th International Conf. on Machine Learning},
  publisher =    {Morgan Kaufmann, San Francisco, CA},
  pages =    {97--105},
  year =     1999,
  url =      {http://citeseer.nj.nec.com/fan99adacost.html},
}

@ARTICLE{freund99short,
  author =   {Y. Freund and R. Schapire},
  title =    {A short introduction to boosting},
  journal =  {Journal of Japanese Society for Artificial
                  Intelligence},
  year =     1999,
  pages =    {771--780},
  volume =   14,
  number =   5,
  url =      {http://citeseer.nj.nec.com/freund99short.html},
}

@TECHREPORT{friedman99bagging,
  author =   {J. Friedman and P. Hall},
  title =    {On Bagging and Nonlinear Estimation - available
                  online at
                  http://citeseer.nj.nec.com/friedman99bagging.html},
  institution =  {Stanford University},
  year =     1999,
  url =      {http://citeseer.nj.nec.com/friedman99bagging.html},
}

@INPROCEEDINGS{guerrasalcedo99genetic,
  author =   {Cesar Guerra-Salcedo and Darrell Whitley},
  title =    {Genetic Approach to Feature Selection for Ensemble
                  Creation},
  booktitle =    {Proceedings of the Genetic and Evolutionary
                  Computation Conference},
  volume =   1,
  month =    {13-17},
  publisher =    {Morgan Kaufmann},
  address =  {Orlando, Florida, USA},
  editor =   {Wolfgang Banzhaf and Jason Daida and Agoston
                  E. Eiben and Max H. Garzon and Vasant Honavar and
                  Mark Jakiela and Robert E. Smith},
  isbn =     {1-55860-611-4},
  pages =    {236--243},
  year =     1999,
  url =      {citeseer.ist.psu.edu/531553.html}
}

@ARTICLE{hansen99combining,
  author =   {J.V. Hansen},
  title =    {Combining Predictors: Comparison of Five Meta
                  Machine Learning Methods},
  journal =  {Information Sciences},
  volume =   119,
  number =   {1-2},
  pages =    {91-105},
  year =     1999,
}

@MISC{haselsteiner99dynamic,
  author =   {E. Haselsteiner},
  title =    {Dynamic targets - adapting supervised learning to
                  time series classification},
  text =     {In Proceedings of the International Joint Conference
                  on Neural Networks IJCNN'99, Washington D.C., IEEE
                  Press.},
  year =     1999,
}

@INBOOK{jacobs99mixturesofx,
  author =   {R.A. Jacobs and M.A. Tanner},
  chapter =  {Mixtures of {X}},
  editor =   {A.J. Sharkey},
  title =    {Combining Articial Neural Nets},
  publisher =    {Springer-Verlag, London},
  year =     1999,
}

@ARTICLE{jordan99introduction,
  author =   {Michael I. Jordan and Zoubin Ghahramani and Tommi
                  Jaakkola and Lawrence K. Saul},
  title =    {An Introduction to Variational Methods for Graphical
                  Models},
  journal =  {Machine Learning},
  volume =   37,
  number =   2,
  pages =    {183-233},
  year =     1999,
}

@ARTICLE{liaomoody99,
  author =   {Yuansong Liao and John Moody},
  title =    {Constructing Heterogeneous Committees Using Input
                  Feature Grouping},
  journal =  {Advances in Neural Information Processing Systems},
  year =     1999,
  volume =   12,
}

@ARTICLE{liuyao99:ensemblelearningvia,
  author =   {Yong Liu and Xin Yao},
  title =    {Ensemble learning via negative correlation},
  journal =  {Neural Networks},
  volume =   12,
  number =   10,
  pages =    {1399--1404},
  year =     1999,
}

@ARTICLE{merz99using,
  author =   {Christopher J. Merz},
  title =    {Using Correspondence Analysis to Combine
                  Classifiers},
  journal =  {Machine Learning},
  volume =   36,
  number =   {1-2},
  pages =    {33-58},
  year =     1999,
}

@TECHREPORT{moerland99dynaboost,
  author =   {P.~Moerland and E.~Mayoraz},
  title =    {DynaBoost: Combining Boosted Hypotheses in a Dynamic
                  Way},
  institution =  {IDIAP},
  year =     1999,
  number =   {RR 99-09},
  address =  {Switzerland},
  month =    {May},
  url =      {http://www.boosting.org/papers/MoeMay99.pdf},
}

@MISC{opitz99genetic,
  author =   {D. Opitz and J. Shavlik},
  title =    {A genetic algorithm approach for creating neural
                  network ensembles},
  text =     {In A.J.C. Sharkey, editor, Combining Articial Neural
                  Nets, pages 79-99. Springer-Verlag, London, 1999.},
  year =     1999,
}

@INPROCEEDINGS{opitz99feature,
  author =   {David Opitz},
  title =    {Feature Selection for Ensembles},
  booktitle =    {Proceedings of 16th National Conference on
                  Artificial Intelligence (AAAI)},
  pages =    {379-384},
  year =     1999,
}

@ARTICLE{opitzmaclin99:popular,
  title =    {Popular Ensemble Methods: An Empirical Study},
  journal =  {Journal of Artificial Intelligence Research},
  volume =   11 ,
  year =     1999,
  pages =    {169-198},
  author =   {David Opitz and Richard Maclin},
}

@TECHREPORT{tumer99decimated,
  author =   {N. Oza and K. Tumer},
  title =    {Dimensionality Reduction through Classifier
                  Ensembles},
  institution =  {NASA Ames Labs},
  year =     1999,
  number =   {NASA-ARC-IC-1999-126},
}

@ARTICLE{schapiresinger99confidence,
  author =   {R.E.~Schapire and Y.~Singer},
  title =    {Improved boosting algorithms using confidence-rated
                  predictions},
  journal =  {Machine Learning},
  year =     1999,
  volume =   37,
  number =   3,
  month =    dec,
  pages =    {297-336},
  abstract =     { We describe several improvements to Freund and
                  Schapire's \adaboost\ boosting algorithm,
                  particularly in a setting in which hypotheses may
                  assign confidences to each of their predictions. We
                  give a simplified analysis of AdaBoost in this
                  setting, and we show how this analysis can be used
                  to find improved parameter settings as well as a
                  refined criterion for training weak hypotheses. We
                  give a specific method for assigning confidences to
                  the predictions of decision trees, a method closely
                  related to one used by Quinlan. This method also
                  suggests a technique for growing decision trees
                  which turns out to be identical to one proposed by
                  Kearns and Mansour. We focus next on how to apply
                  the new boosting algorithms to multiclass
                  classification problems, particularly to the
                  multi-label case in which each example may belong to
                  more than one class. We give two boosting methods
                  for this problem. One of these leads to a new method
                  for handling the single-label case which is simpler
                  but as effective as techniques suggested by Freund
                  and Schapire. Finally, we give some experimental
                  results comparing a few of the algorithms discussed
                  in this paper. },
  url =      {http://www.boosting.org/papers/SchSin99b.pdf},
}

@INPROCEEDINGS{schapire99theoretical,
  author =   {Robert E. Schapire},
  title =    {Theoretical Views of Boosting and Applications},
  booktitle =    {Algorithmic Learning Theory, 10th International
                  Conference, {ALT} '99, Tokyo, Japan, December 1999,
                  Proceedings},
  volume =   1720,
  publisher =    {Springer},
  pages =    {13--25},
  year =     1999,
  url =          {http://citeseer.nj.nec.com/article/schapire99theoretical.html},
}

@INBOOK{sharkey98:book,
  publisher =    {Springer-Verlag},
  pages =    {1--30},
  year =     1999,
  author =   {Amanda Sharkey},
  chapter =  {Combining Artificial Neural Nets: Ensemble and
                  Modular Multi-Net Systems},
  title =    {Multi-Net Systems},
}

@INPROCEEDINGS{wahba99biasvariance,
  author =   {G. Wahba and X. Lin and F. Gao and D. Xiang and
                  R. Klein and B. Klein},
  title =    {The bias-variance tradeoff and the randomized
                  {GACV}},
  booktitle =    {Advances in Neural Information Processing Systems},
  number =   11,
  pages =    {620--626},
  publisher =    {MIT Press},
  editor =   {M. Kearns and S. Solla and D. Cohn},
  year =     1999,
  url =      {http://citeseer.nj.nec.com/wahba99biasvariance.html},
}

@ARTICLE{ifepyao99,
  author =   {X. Yao and Y. Liu and G. Lin},
  number =   2,
  pages =    {82-102},
  year =     1999,
  journal =  {IEEE Transactions on Evolutionary Computation},
  volume =   3,
  title =    {Evolutionary programming made faster},
  month =    {July},
}

@INCOLLECTION{yao99,
  publisher =    {IEEE},
  pages =    {1423-1447},
  volume =   87 ,
  year =     1999,
  author =   {Xin Yao},
  number =   9,
  title =    {Evolving Artificial Neural Networks},
  month =    {September},
  booktitle =    {Proceedings of the IEEE},
}

@INPROCEEDINGS{yaoliu99:breastcancer,
  author =   {Xin Yao and Yong Liu},
  title =    {Neural networks for breast cancer diagnosis},
  year =     1999,
  booktitle =    {Proceedings of the 1999 Congress on Evolutionary
                  Computation},
  pages =    {1760-1767},
  volume =   3,
  month =    {July},
  publisher =    {IEEE Press},
}

@BOOK{greenberglatex,
  author =   {H.J. Greenberg},
  title =    {A Simplified Introduction to {\LaTeX}},
  publisher =    {World Wide Web},
  url =          {http://www.cudenver.edu/\~{}hgreenbe/aboutme/pubsrec.html},
  year =     {1999--2001},
}

@INPROCEEDINGS{bahler00,
  author =   {Dennis Bahler and Laura Navarro},
  title =    {Methods for Combining Heterogeneous Sets of
                  Classifiers},
  booktitle =    {17th Natl. Conf. on Artificial Intelligence (AAAI),
                  Workshop on New Research Problems for Machine
                  Learning},
  year =     2000,
}

@INPROCEEDINGS{benediktsson00consensus,
  author =   {Jon Atli Benediktsson and Johannes R. Sveinsson},
  title =    {Consensus Based Classification of Multisource Remote
                  Sensing Data},
  booktitle =    {Proc. Int. Workshop on Multiple Classifier Systems
                  (LNCS 1857)},
  publisher =    {Springer},
  month =    {June},
  pages =    {280-289},
  year =     2000,
  address =  {Calgiari, Italy},
  url =          "http://link.springer.de/link/service/series/0558/tocs/t1857.htm"
}

@TECHREPORT{breiman00infinite,
  author =   {L.~Breiman},
  title =    {Some Infinite Theory for Predictor Ensembles},
  institution =  {Statistics Department, UC Berkeley},
  year =     2000,
  number =   577,
  month =    {August},
  url =          {http://www.boosting.org/papers/some_theory2001.ps.gz},
  pdf =      {http://www.boosting.org/papers/some_theory2001.pdf},
  psgz =
                  {http://www.boosting.org/papers/some_theory2001.ps.gz},
  ps =       {http://www.boosting.org/papers/some_theory2001.ps},
}

@INPROCEEDINGS{bruzzone00combining,
  author =   {Lorenzo Bruzzone and Roberto Cossu and Diego
                  Fernandez Prieto},
  title =    {Combining Parametric and Nonparametric Classifiers
                  for an Unsupervised Updating of Land-Cover Maps},
  booktitle =    {Proc. Int. Workshop on Multiple Classifier Systems
                  (LNCS 1857)},
  publisher =    {Springer},
  month =    {June},
  pages =    {290-299},
  year =     2000,
  address =  {Calgiari, Italy},
  url =          "http://link.springer.de/link/service/series/0558/tocs/t1857.htm"
}

@TECHREPORT{buhlmann00explaining,
  author =   {Peter Buhlmann and Bin Yu},
  title =    {Explaining Bagging},
  institution =  {ETH Zurich, Seminar Fur Statistik},
  month =    {May},
  year =     2000,
  number =   92,
  url =          {ftp://ftp.stat.math.ethz.ch/Research-Reports/92.html},
}

@INPROCEEDINGS{cappelli00combining,
  author =   {Raffaele Cappelli and Dario Maio and Davide Maltoni},
  title =    {Combining Fingerprint Classifiers},
  booktitle =    {Proc. Int. Workshop on Multiple Classifier Systems
                  (LNCS 1857)},
  publisher =    {Springer},
  month =    {June},
  pages =    {351-361},
  year =     2000,
  address =  {Calgiari, Italy},
  url =          "http://link.springer.de/link/service/series/0558/tocs/t1857.htm"
}

@INPROCEEDINGS{cohen00hybrid,
  author =   {Shimon Cohen and Nathan Intrator},
  title =    {A Hybrid Projection Based and Radial Basis Function
                  Architecture},
  booktitle =    {Proc. Int. Workshop on Multiple Classifier Systems
                  (LNCS 1857)},
  publisher =    {Springer},
  month =    {June},
  pages =    {147-156},
  year =     2000,
  address =  {Calgiari, Italy},
  url =          "http://link.springer.de/link/service/series/0558/tocs/t1857.htm"
}

@INPROCEEDINGS{conversano00supervised,
  author =   {Claudio Conversano and Roberta Siciliano and
                  Francesco Mola},
  title =    {Supervised Classifier Combination through
                  Generalized Additive Multi-model},
  booktitle =    {Proc. Int. Workshop on Multiple Classifier Systems
                  (LNCS 1857)},
  publisher =    {Springer},
  month =    {June},
  pages =    {167-176},
  year =     2000,
  address =  {Calgiari, Italy},
  url =          "http://link.springer.de/link/service/series/0558/tocs/t1857.htm"
}

@INPROCEEDINGS{cordella00cascaded,
  author =   {Luigi P. Cordella and Pasquale Foggia and Carlo
                  Sansone and Francesco Tortorella and Mario Vento},
  title =    {A Cascaded Multiple Expert System for Verification},
  booktitle =    {Proc. Int. Workshop on Multiple Classifier Systems
                  (LNCS 1857)},
  publisher =    {Springer},
  month =    {June},
  pages =    {330-339},
  year =     2000,
  address =  {Calgiari, Italy},
  url =          "http://link.springer.de/link/service/series/0558/tocs/t1857.htm"
}

@INPROCEEDINGS{cunningham00diversity,
  author =   {Padraig Cunningham and John Carney},
  title =    {Diversity versus Quality in Classification Ensembles
                  Based on Feature Selection},
  booktitle =    {LNCS - European Conference on Machine Learning},
  volume =   1810,
  publisher =    {Springer, Berlin},
  pages =    {109--116},
  year =     2000,
}

@INPROCEEDINGS{dietterich00ensemble,
  author =   {Thomas G. Dietterich},
  title =    {Ensemble Methods in Machine Learning},
  booktitle =    {Proc. Int. Workshop on Multiple Classifier Systems
                  (LNCS 1857)},
  publisher =    {Springer},
  month =    {June},
  pages =    {1-15},
  year =     2000,
  address =  {Calgiari, Italy},
  url =          "http://link.springer.de/link/service/series/0558/tocs/t1857.htm"
}

@ARTICLE{dietterich00experimental,
  author =   {Thomas G. Dietterich},
  title =    {An Experimental Comparison of Three Methods for
                  Constructing Ensembles of Decision Trees: Bagging,
                  Boosting, and Randomization},
  journal =  {Machine Learning},
  volume =   40,
  number =   2,
  pages =    {139-157},
  year =     2000,
}

@INPROCEEDINGS{diez00applying,
  author =   {Juan J. Rodriguez Diez and Carlos Alonso Gonzalez},
  title =    {Applying Boosting to Similarity Literals for Time
                  Series Classification},
  booktitle =    {Proc. Int. Workshop on Multiple Classifier Systems
                  (LNCS 1857)},
  publisher =    {Springer},
  month =    {June},
  pages =    {210-219},
  year =     2000,
  address =  {Calgiari, Italy},
  url =          "http://link.springer.de/link/service/series/0558/tocs/t1857.htm"
}

@INPROCEEDINGS{domingos00unified,
  author =   {Pedro Domingos},
  title =    {A Unified Bias-Variance Decomposition and its
                  Applications},
  booktitle =    {Proc. 17th International Conf. on Machine Learning},
  publisher =    {Morgan Kaufmann, San Francisco, CA},
  pages =    {231--238},
  year =     2000,
  url =      {http://citeseer.nj.nec.com/domingos00unified.html},
}

@INPROCEEDINGS{domingos00unified-AAAI,
  author =   {Pedro Domingos},
  title =    {A Unified Bias-Variance Decomposition for Zero-One
                  and Squared Loss},
  booktitle =    {{AAAI}/{IAAI}},
  pages =    {564-569},
  year =     2000,
}

@INPROCEEDINGS{duin00experiments,
  author =   {Robert P. W. Duin and David M. J. Tax},
  title =    {Experiments with Classifier Combining Rules},
  booktitle =    {Proc. Int. Workshop on Multiple Classifier Systems
                  (LNCS 1857)},
  publisher =    {Springer},
  month =    {June},
  pages =    {16-29},
  year =     2000,
  address =  {Calgiari, Italy},
  url =          "http://link.springer.de/link/service/series/0558/tocs/t1857.htm"
}

@INPROCEEDINGS{fanelli00modular,
  author =   {Anna Maria Fanelli and Giovanna Castellano and
                  C. Alessandro Buscicchio},
  title =    {A Modular Neuro-Fuzzy Network for Musical
                  Instruments Classification},
  booktitle =    {Proc. Int. Workshop on Multiple Classifier Systems
                  (LNCS 1857)},
  publisher =    {Springer},
  month =    {June},
  pages =    {372-382},
  year =     2000,
  address =  {Calgiari, Italy},
  url =          "http://link.springer.de/link/service/series/0558/tocs/t1857.htm"
}

@INPROCEEDINGS{froba00statistical,
  author =   {Bernhard Froba and Constanze Rothe and Christian
                  Kublbeck},
  title =    {Statistical Sensor Calibration for Fusion of
                  Different Classifiers in a Biometric Person
                  Recognition Framework},
  booktitle =    {Proc. Int. Workshop on Multiple Classifier Systems
                  (LNCS 1857)},
  publisher =    {Springer},
  month =    {June},
  pages =    {362-371},
  year =     2000,
  address =  {Calgiari, Italy},
  url =          "http://link.springer.de/link/service/series/0558/tocs/t1857.htm"
}

@INPROCEEDINGS{furlanello00boosting,
  author =   {Cesare Furlanello and Stefano Merler},
  title =    {Boosting of Tree-Based Classifiers for Predictive
                  Risk Modeling in GIS},
  booktitle =    {Proc. Int. Workshop on Multiple Classifier Systems
                  (LNCS 1857)},
  publisher =    {Springer},
  month =    {June},
  pages =    {220-229},
  year =     2000,
  address =  {Calgiari, Italy},
  url =          "http://link.springer.de/link/service/series/0558/tocs/t1857.htm"
}

@INPROCEEDINGS{giacinto00dynamic,
  author =   {Giorgio Giacinto and Fabio Roli},
  title =    {Dynamic Classifier Selection},
  booktitle =    {Proc. Int. Workshop on Multiple Classifier Systems
                  (LNCS 1857)},
  publisher =    {Springer},
  month =    {June},
  pages =    {177-189},
  year =     2000,
  address =  {Calgiari, Italy},
  url =          "http://link.springer.de/link/service/series/0558/tocs/t1857.htm"
}

@INPROCEEDINGS{griffith00self-organizing,
  author =   {Niall Griffith and Derek Partridge},
  title =    {Self-Organizing Decomposition of Functions in the
                  Context of a Unified Framework for Multiple
                  Classifier Systems},
  booktitle =    {Proc. Int. Workshop on Multiple Classifier Systems
                  (LNCS 1857)},
  publisher =    {Springer},
  month =    {June},
  pages =    {250-259},
  year =     2000,
  address =  {Calgiari, Italy},
  url =          "http://link.springer.de/link/service/series/0558/tocs/t1857.htm"
}

@INPROCEEDINGS{grim00combining,
  author =   {Jiri Grim and Josef Kittler and Pavel Pudil and Petr
                  Somol},
  title =    {Combining Multiple Classifiers in Probabilistic
                  Neural Networks},
  booktitle =    {Proc. Int. Workshop on Multiple Classifier Systems
                  (LNCS 1857)},
  publisher =    {Springer},
  month =    {June},
  pages =    {157-166},
  year =     2000,
  address =  {Calgiari, Italy},
  url =          "http://link.springer.de/link/service/series/0558/tocs/t1857.htm"
}

@INPROCEEDINGS{navone2000,
  author =   {H.D.Navone and P.F.Verdes and P.M.Granitto and
                  H.A.Ceccatto},
  title =    {A New Algorithm for Selecting Diverse Members of a
                  Neural Network Ensemble},
  booktitle =    {6th International Congress on Information
                  Engineering},
  year =     2000,
  note =     {Buenos Aires, Argentina},
}

@PHDTHESIS{hansenthesis,
  author =   {J.V. Hansen},
  title =    {Combining Predictors: Meta Machine Learning Methods
                  and Bias/Variance and Ambiguity Decompositions},
  school =   {Aarhus Universitet, Datalogisk Institut},
  year =     2000,
}

@INPROCEEDINGS{happel00analysis,
  author =   {Mark D. Happel and Peter Bock},
  title =    {Analysis of a Fusion Method for Combining Marginal
                  Classifiers},
  booktitle =    {Proc. Int. Workshop on Multiple Classifier Systems
                  (LNCS 1857)},
  publisher =    {Springer},
  month =    {June},
  pages =    {137-146},
  year =     2000,
  address =  {Calgiari, Italy},
  url =          "http://link.springer.de/link/service/series/0558/tocs/t1857.htm"
}

@INPROCEEDINGS{ho00complexity,
  author =   {Tin Kam Ho},
  title =    {Complexity of Classification Problems and
                  Comparative Advantages of Combined Classifiers},
  booktitle =    {Proc. Int. Workshop on Multiple Classifier Systems
                  (LNCS 1857)},
  publisher =    {Springer},
  month =    {June},
  pages =    {97-106},
  year =     2000,
  address =  {Calgiari, Italy},
  url =          "http://link.springer.de/link/service/series/0558/tocs/t1857.htm"
}

@INPROCEEDINGS{ianakiev00architecture,
  author =   {Krasimir G. Ianakiev and Venu Govindaraju},
  title =    {Architecture for Classifier Combination Using
                  Entropy Measures},
  booktitle =    {Proc. Int. Workshop on Multiple Classifier Systems
                  (LNCS 1857)},
  publisher =    {Springer},
  month =    {June},
  pages =    {340-350},
  year =     2000,
  address =  {Calgiari, Italy},
  url =          "http://link.springer.de/link/service/series/0558/tocs/t1857.htm"
}

@INPROCEEDINGS{impedovo00new,
  author =   {Sebastiano Impedovo and A. Salzo},
  title =    {A New Evaluation Method for Expert Combination in
                  Multi-expert System Designing},
  booktitle =    {Proc. Int. Workshop on Multiple Classifier Systems
                  (LNCS 1857)},
  publisher =    {Springer},
  month =    {June},
  pages =    {230-239},
  year =     2000,
  address =  {Calgiari, Italy},
  url =          "http://link.springer.de/link/service/series/0558/tocs/t1857.htm"
}

@ARTICLE{itqon01,
  author =   {Itqon and Shun'ichi Kaneko and Satoru Igarashi},
  title =    {Combining Multiple k-Nearest Neighbor Classifiers
                  Using Feature Combinations},
  journal =  {Journal of IECI (Indonesian Society on Electrical
                  Electronics, Communication and Information)},
  year =     2000,
  volume =   2,
  number =   3,
  pages =    {23--31},
}

@INBOOK{jaakkola00variational,
  author =   {Jaakkola, T.},
  editor =   {D. Saad and M. Opper},
  title =    {Advanced Mean Field methods - Theory and Practice},
  chapter =  {Tutorial on Variational Approximation Methods},
  publisher =    {MIT Press},
  year =     2000,
}

@INPROCEEDINGS{jiang00some,
  author =   {Wenxin Jiang},
  title =    {Some Results on Weakly Accurate Base Learners for
                  Boosting Regression and Classification},
  booktitle =    {Proc. Int. Workshop on Multiple Classifier Systems
                  (LNCS 1857)},
  publisher =    {Springer},
  month =    {June},
  pages =    {87-96},
  year =     2000,
  address =  {Calgiari, Italy},
  url =          "http://link.springer.de/link/service/series/0558/tocs/t1857.htm"
}

@INPROCEEDINGS{jiang00classifier,
  author =   {Xiaoyi Jiang and Keren Yu and Horst Bunke},
  title =    {Classifier Combination for Grammar-Guided Sentence
                  Recognition},
  booktitle =    {Proc. Int. Workshop on Multiple Classifier Systems
                  (LNCS 1857)},
  publisher =    {Springer},
  month =    {June},
  pages =    {383-392},
  year =     2000,
  address =  {Calgiari, Italy},
  url =          "http://link.springer.de/link/service/series/0558/tocs/t1857.htm"
}

@PROCEEDINGS{mcs2000,
  title =    {Lecture Notes in Computer Science : First
                  International Workshop on Multiple Classifier
                  Systems},
  year =     2000,
  editor =   {Josef Kittler and Fabio Roli},
  volume =   1857,
  address =  {Cagliari, Italy},
  month =    {June},
}

@ARTICLE{kleinberg00algorithmic,
  author =   {Eugene M. Kleinberg},
  title =    {On the Algorithmic Implementation of Stochastic
                  Discrimination},
  journal =  {IEEE Transactions on Pattern Analysis and Machine
                  Intelligence},
  volume =   22,
  number =   5,
  pages =    {473-490},
  year =     2000,
}

@INPROCEEDINGS{kleinberg00mathematically,
  author =   {Eugene M. Kleinberg},
  title =    {A Mathematically Rigorous Foundation for Supervised
                  Learning},
  booktitle =    {Proc. Int. Workshop on Multiple Classifier Systems
                  (LNCS 1857)},
  publisher =    {Springer},
  month =    {June},
  pages =    {67-76},
  year =     2000,
  address =  {Calgiari, Italy},
  url =          "http://link.springer.de/link/service/series/0558/tocs/t1857.htm"
}

@INPROCEEDINGS{kumar00hierarchical,
  author =   {Shailesh Kumar and Joydeep Ghosh and Melba
                  M. Crawford},
  title =    {A Hierarchical Multiclassifier System for
                  Hyperspectral Data Analysis},
  booktitle =    {Proc. Int. Workshop on Multiple Classifier Systems
                  (LNCS 1857)},
  publisher =    {Springer},
  month =    {June},
  pages =    {270-279},
  year =     2000,
  address =  {Calgiari, Italy},
  url =          "http://link.springer.de/link/service/series/0558/tocs/t1857.htm"
}

@INPROCEEDINGS{kumazawa00shape,
  author =   {Itsuo Kumazawa},
  title =    {Shape Matching and Extraction by an Array of
                  Figure-and-Ground Classifiers},
  booktitle =    {Proc. Int. Workshop on Multiple Classifier Systems
                  (LNCS 1857)},
  publisher =    {Springer},
  month =    {June},
  pages =    {393-402},
  year =     2000,
  address =  {Calgiari, Italy},
  url =          "http://link.springer.de/link/service/series/0558/tocs/t1857.htm"
}

@INPROCEEDINGS{kuncheva00independence,
  author =   {L. Kuncheva and C. Whitaker and C. Shipp and
                  R. Duin},
  title =    {Is independence good for combining classifiers},
  booktitle =    {Proceedings of the 15th International Conference on
                  Pattern Recognition, Barcelona, Spain},
  year =     2000,
  pages =    {168-171},
}

@INPROCEEDINGS{lam00implementations,
  author =   {Louisa Lam},
  title =    {Classifier Combinations: Implementations and
                  Theoretical Issues},
  booktitle =    {Proc. Int. Workshop on Multiple Classifier Systems
                  (LNCS 1857)},
  publisher =    {Springer},
  month =    {June},
  pages =    {77-86},
  year =     2000,
  address =  {Calgiari, Italy},
  url =          "http://link.springer.de/link/service/series/0558/tocs/t1857.htm"
}

@MISC{lappalainen00ensemble,
  author =   {H. Lappalainen and J. Miskin},
  title =    {Ensemble Learning},
  text =     {H. Lappalainen and J. Miskin, Ensemble Learning, in
                  M. Girolami (Ed.), Advances in Independent Component
                  Analysis, Springer, Berlin, 2000 (in press).},
  year =     2000,
}

@INPROCEEDINGS{latinne00different,
  author =   {Patrice Latinne and Olivier Debeir and Christine
                  Decaestecker},
  title =    {Different Ways of Weakening Decision Trees and Their
                  Impact on Classification Accuracy of DT Combination},
  booktitle =    {Proc. Int. Workshop on Multiple Classifier Systems
                  (LNCS 1857)},
  publisher =    {Springer},
  month =    {June},
  pages =    {200-209},
  year =     2000,
  address =  {Calgiari, Italy},
  url =          "http://link.springer.de/link/service/series/0558/tocs/t1857.htm"
}

@INPROCEEDINGS{lecce00multi-expert,
  author =   {Vincenzo Di Lecce and Giovanni Dimauro and Andrea
                  Guerriero and Sebastiano Impedovo and Giuseppe Pirlo
                  and A. Salzo},
  title =    {A Multi-expert System for Dynamic Signature
                  Verification},
  booktitle =    {Proc. Int. Workshop on Multiple Classifier Systems
                  (LNCS 1857)},
  publisher =    {Springer},
  month =    {June},
  pages =    {320-329},
  year =     2000,
  address =  {Calgiari, Italy},
  url =          "http://link.springer.de/link/service/series/0558/tocs/t1857.htm"
}

@ARTICLE{liu00evolutionary,
  author =   {Y. Liu and X. Yao and T. Higuchi},
  title =    {Evolutionary Ensembles with Negative Correlation
                  Learning},
  journal =  {IEEE Transactions on Evolutionary Computation},
  volume =   4,
  number =   4,
  month =    {November},
  year =     2000,
  url =          {http://citeseer.nj.nec.com/article/liu00evolutionary.html},
}

@INBOOK{mason00functional,
  author =   {L. Mason and J. Baxter and P. L. Bartlett and
                  M. Frean},
  editor =   {A.J. Smola, P. L. Bartlett, B. Scholkopf, and
                  D. Schuurmans},
  title =    {Advances in Large Margin Classifiers : Functional
                  gradient techniques for combining hypotheses},
  publisher =    {MIT Press},
  year =     2000,
  address =  {Cambridge, MA},
  pages =    {221--246},
}

@INPROCEEDINGS{masulli00effectiveness,
  author =   {Francesco Masulli and Giorgio Valentini},
  title =    {Effectiveness of Error Correcting Output Codes in
                  Multiclass Learning Problems},
  booktitle =    {Proc. Int. Workshop on Multiple Classifier Systems
                  (LNCS 1857)},
  publisher =    {Springer},
  month =    {June},
  pages =    {107-116},
  year =     2000,
  address =  {Calgiari, Italy},
  url =          "http://link.springer.de/link/service/series/0558/tocs/t1857.htm"
}

@INPROCEEDINGS{mckay00sharing,
  author =   {Bob McKay},
  title =    {Fitness Sharing in Genetic Programming},
  pages =    {435--442},
  year =     2000,
  publisher =    {Morgan Kaufmann},
  booktitle =    {Proceedings of the Genetic and Evolutionary
                  Computation Conference (GECCO-2000)},
  editor =   {Darrell Whitley and David Goldberg and Erick
                  Cantu-Paz and Lee Spector and Ian Parmee and
                  Hans-Georg Beyer},
  address =  {Las Vegas, Nevada, USA},
  publisher_address ="San Francisco, CA 94104, USA",
  month =    "10-12 " # jul,
  keywords =     {genetic algorithms, genetic programming},
  isbn =     {1-55860-708-0},
  notes =    {A joint meeting of the ninth International
                  Conference on Genetic Algorithms (ICGA-2000) and the
                  fifth Annual Genetic Programming Conference
                  (GP-2000) Part of whitley:2000:GECCO},
}

@INPROCEEDINGS{meir00localized,
  author =   {Ron Meir and Ran El-{Y}aniv and Shai Ben-{D}avid},
  title =    {Localized Boosting},
  booktitle =    {Proc. 13th Annu. Conference on Comput. Learning
                  Theory},
  publisher =    {Morgan Kaufmann, San Francisco},
  pages =    {190--199},
  year =     2000,
  url =      {http://citeseer.nj.nec.com/511402.html},
}

@INPROCEEDINGS{pekalska00combining,
  author =   {Elzbieta Pekalska and Marina Skurichina and Robert
                  P. W. Duin},
  title =    {Combining Fisher Linear Discriminants for
                  Dissimilarity Representations},
  booktitle =    {Proc. Int. Workshop on Multiple Classifier Systems
                  (LNCS 1857)},
  publisher =    {Springer},
  month =    {June},
  pages =    {117-126},
  year =     2000,
  address =  {Calgiari, Italy},
  url =          "http://link.springer.de/link/service/series/0558/tocs/t1857.htm"
}

@INPROCEEDINGS{pennock00normative,
  author =   {David M. Pennock and Pedrito {Maynard-Reid {II}} and
                  C. Lee Giles and Eric Horvitz},
  title =    {A Normative Examination of Ensemble Learning
                  Algorithms},
  booktitle =    {Proc. 17th International Conf. on Machine Learning},
  publisher =    {Morgan Kaufmann, San Francisco, CA},
  pages =    {735--742},
  year =     2000,
}

@ARTICLE{schapiresinger00boostexter,
  author =   {R.E.~Schapire and Y.~Singer},
  title =    {BoosTexter: A boosting-based system for text
                  categorization},
  journal =  {Machine Learning},
  year =     2000,
  volume =   39,
  number =   {2/3},
  pages =    {135-168},
}

@INPROCEEDINGS{sharkey00:test,
  author =   {Amanda J. C. Sharkey and Noel E. Sharkey and Uwe
                  Gerecke and G. O. Chandroth},
  title =    {The Test and Select Approach to Ensemble
                  Combination},
  booktitle =    {Proc. Int. Workshop on Multiple Classifier Systems
                  (LNCS 1857)},
  publisher =    {Springer},
  month =    {June},
  pages =    {30-44},
  year =     2000,
  address =  {Calgiari, Italy},
}

@INPROCEEDINGS{skurichina00role,
  author =   {M. Skurichina and R.P.W. Duin},
  title =    {The Role of Combining Rules in Bagging and Boosting},
  booktitle =    {Advances in Pattern Recognition, Proc. Joint IAPR
                  International Workshops SSPR2000 and SPR2000 Lecture
                  Notes in Computer Science, vol. 1876},
  publisher =    {Springer},
  month =    {June},
  pages =    {236-245},
  editors =  {F.J. Ferri, J.M. Inesta, A. Amin, P. Pudil},
  year =     2000,
  address =  {Alicante, Spain},
}

@INPROCEEDINGS{skurichina00boosting,
  author =   {Marina Skurichina and Robert P. W. Duin},
  title =    {Boosting in Linear Discriminant Analysis},
  booktitle =    {Proc. Int. Workshop on Multiple Classifier Systems
                  (LNCS 1857)},
  publisher =    {Springer},
  month =    {June},
  pages =    {190-199},
  year =     2000,
  address =  {Calgiari, Italy},
  url =          "http://link.springer.de/link/service/series/0558/tocs/t1857.htm"
}

@INPROCEEDINGS{slavik00use,
  author =   {Petr Slavik and Venu Govindaraju},
  title =    {Use of Lexicon Density in Evaluating Word
                  Recognizers},
  booktitle =    {Proc. Int. Workshop on Multiple Classifier Systems
                  (LNCS 1857)},
  publisher =    {Springer},
  month =    {June},
  pages =    {310-319},
  year =     2000,
  address =  {Calgiari, Italy},
  url =          "http://link.springer.de/link/service/series/0558/tocs/t1857.htm"
}

@INPROCEEDINGS{srihari00survey,
  author =   {Sargur N. Srihari},
  title =    {A Survey of Sequential Combination of Word
                  Recognizers in Handwritten Phrase Recognition at
                  CEDAR},
  booktitle =    {Proc. Int. Workshop on Multiple Classifier Systems
                  (LNCS 1857)},
  publisher =    {Springer},
  month =    {June},
  pages =    {45-51},
  year =     2000,
  address =  {Calgiari, Italy},
  url =          "http://link.springer.de/link/service/series/0558/tocs/t1857.htm"
}

@INPROCEEDINGS{stainvas00,
  title =    {Blurred Face Recognition via a Hybrid Network
                  Architecture},
  author =   {I. Stainvas and N. Intrator},
  booktitle =    {ICPR},
  year =     2000,
  volume =   2,
  pages =    {809--812},
}

@INPROCEEDINGS{suen00multiple,
  author =   {Ching Y. Suen and Louisa Lam},
  title =    {Multiple Classifier Combination Methodologies for
                  Different Output Levels},
  booktitle =    {Proc. Int. Workshop on Multiple Classifier Systems
                  (LNCS 1857)},
  publisher =    {Springer},
  month =    {June},
  pages =    {52-66},
  year =     2000,
  address =  {Calgiari, Italy},
  url =          "http://link.springer.de/link/service/series/0558/tocs/t1857.htm"
}

@INPROCEEDINGS{takahashi00learning,
  author =   {Katsuhiko Takahashi and Atsushi Sato},
  title =    {A Learning Method of Feature Selection for Rough
                  Classification},
  booktitle =    {Proc. Int. Workshop on Multiple Classifier Systems
                  (LNCS 1857)},
  publisher =    {Springer},
  month =    {June},
  pages =    {127-136},
  year =     2000,
  address =  {Calgiari, Italy},
  url =          "http://link.springer.de/link/service/series/0558/tocs/t1857.htm"
}

@ARTICLE{tresp00bayescommittee,
  author =   {Volker Tresp},
  title =    {A Bayesian Committee Machine},
  journal =  {Neural Computation},
  year =     2000,
  pages =    {2719-2741},
  volume =   12,
  number =   11,
  abstract =     { The Bayesian committee machine (BCM) is a novel
                  approach to combining estimators which were trained
                  on different data sets. Although the BCM can be
                  applied to the combination of any kind of estimators
                  the main foci are Gaussian process regression and
                  related systems such as regularization networks and
                  smoothing splines for which the degrees of freedom
                  increase with the number of training data. Somewhat
                  surprisingly, we find that the performance of the
                  BCM improves if several test points are queried at
                  the same time and is optimal if the number of test
                  points is at least as large as the degrees of
                  freedom of the estimator. The BCM also provides a
                  new solution for online learning with potential
                  applications to data mining. We apply the BCM to
                  systems with fixed basis functions and discuss its
                  relationship to Gaussian process
                  regression. Finally, we also show how the ideas
                  behind the BCM can be applied in a non-Bayesian
                  setting to extend the input dependent combination of
                  estimators.},
  url =      {http://www.boosting.org/papers/upload_7235_bcm5.ps},
}

@INPROCEEDINGS{tresp00generalizedbayescommittee,
  author =   {Volker Tresp},
  title =    {The Generalized Bayesian Committee Machine},
  booktitle =    {Proceedings of the Sixth ACM SIGKDD International
                  Conference on Knowledge Discovery and Data Mining,
                  KDD-2000},
  year =     2000,
  pages =    {130-139},
  abstract =     { In this paper we introduce the Generalized Bayesian
                  Committee Machine (GBCM) for applications with large
                  data sets. In particular, the GBCM can be used in
                  the context of kernel based systems such as
                  smoothing splines, kriging, regularization networks
                  and Gaussian process regression which ---for
                  computational reasons--- are otherwise limited to
                  rather small data sets. The GBCM provides a novel
                  and principled way of combining estimators trained
                  for regression, classification, the prediction of
                  counts, the prediction of lifetimes and other
                  applications which can be derived from the
                  exponential family of distributions. We describe an
                  online version of the GBCM which only requires one
                  pass through the data set and only requires the
                  storage of a matrix of the dimension of the number
                  of query or test points. After training, the
                  prediction at additional test points only requires
                  resources dependent on the number of query points
                  but is independent of the number of training
                  data. We confirm the good scaling behavior using
                  real and experimental data sets. },
  url =          {http://www.boosting.org/papers/upload_7240_kddpaper2.ps},
}

@ARTICLE{ueda00optimal,
  author =   {N Ueda},
  title =    {Optimal linear combination of neural networks for
                  improving classification Performance },
  journal =  {IEEE Transactions on Pattern Analysis and Machine
                  Intelligence},
  year =     2000,
  volume =   22,
  number =   2,
  pages =    {207--215},
}

@INPROCEEDINGS{wan00multiple,
  author =   {Weijian Wan and Donald Fraser},
  title =    {A Multiple Self-Organizing Map Scheme for Remote
                  Sensing Classification},
  booktitle =    {Proc. Int. Workshop on Multiple Classifier Systems
                  (LNCS 1857)},
  publisher =    {Springer},
  month =    {June},
  pages =    {300-309},
  year =     2000,
  address =  {Calgiari, Italy},
  url =          "http://link.springer.de/link/service/series/0558/tocs/t1857.htm"
}

@INPROCEEDINGS{wang00diversity,
  author =   {Wenjia Wang and Phillis Jones and Derek Partridge},
  title =    {Diversity between Neural Networks and Decision Trees
                  for Building Multiple Classifier Systems},
  booktitle =    {Proc. Int. Workshop on Multiple Classifier Systems
                  (LNCS 1857)},
  publisher =    {Springer},
  month =    {June},
  pages =    {240-249},
  year =     2000,
  address =  {Calgiari, Italy},
  url =          "http://link.springer.de/link/service/series/0558/tocs/t1857.htm"
}

@ARTICLE{webb00multiboosting,
  author =   {Geoffrey I. Webb},
  title =    {Multi{B}oosting: {A} Technique for Combining
                  {B}oosting and {W}agging},
  journal =  {Machine Learning},
  volume =   40,
  number =   2,
  publisher =    {Kluwer Academic Publishers, Boston},
  pages =    {159--196},
  year =     2000,
  url =      {http://citeseer.nj.nec.com/webb98multiboosting.html},
}

@INPROCEEDINGS{wezel00nonconformist,
  author =   {M.C. van Wezel and M.D. Out and W.A. Kosters},
  title =    {Ensembles of nonconformist neural networks},
  booktitle =    {Proceedings of the Twelfth Belgium-Netherlands
                  Artificial Intelligence Conference},
  editors =  {H. Weigand and A. van den Bosch},
  pages =    {165-172},
  year =     2000,
}

@INPROCEEDINGS{windeatt00classifier,
  author =   {Terry Windeatt},
  title =    {Classifier Instability and Partitioning},
  booktitle =    {Proc. Int. Workshop on Multiple Classifier Systems
                  (LNCS 1857)},
  publisher =    {Springer},
  month =    {June},
  pages =    {260-269},
  year =     2000,
  address =  {Calgiari, Italy},
  url =          "http://link.springer.de/link/service/series/0558/tocs/t1857.htm"
}

@INPROCEEDINGS{ahn01speciated,
  author =   {Joon-Hyun Ahn and Sung-Bae Cho},
  title =    {Speciated Neural Networks Evolved with Fitness
                  Sharing Technique},
  booktitle =    {Proceedings of the Congress on Evolutionary
                  Computation},
  address =  {Seoul, Korea},
  month =    {May 27-30},
  pages =    {390--396},
  year =     2001,
}

@INPROCEEDINGS{alkoot01improving,
  author =   {Fuad M. Alkoot and Josef Kittler},
  title =    {Improving Product by Moderating k-NN Classifiers},
  booktitle =    {Proc. Int. Workshop on Multiple Classifier Systems
                  (LNCS 2096)},
  publisher =    {Springer},
  month =    {June},
  pages =    {429-439},
  year =     2001,
  address =  {Cambridge, UK},
  url =          "http://link.springer.de/link/service/series/0558/tocs/t2096.htm"
}

@ARTICLE{brameier01evolving,
  author =   {Markus Brameier and Wolfgang Banzhaf},
  title =    {Evolving Teams of Predictors with Linear Genetic
                  Programming},
  journal =  {Genetic Programming and Evolvable Machines},
  volume =   2,
  number =   4,
  pages =    {381--407},
  year =     2001,
}

@INPROCEEDINGS{briem01boosting,
  author =   {Gunnar Jakob Briem and Jon Atli Benediktsson and
                  Johannes R. Sveinsson},
  title =    {Boosting, Bagging, and Consensus Based
                  Classification of Multisource Remote Sensing Data},
  booktitle =    {Proc. Int. Workshop on Multiple Classifier Systems
                  (LNCS 2096)},
  publisher =    {Springer},
  month =    {June},
  pages =    {279-288},
  year =     2001,
  address =  {Cambridge, UK},
  url =          "http://link.springer.de/link/service/series/0558/tocs/t2096.htm"
}

@INPROCEEDINGS{brownyao01:ukci,
  author =   {Gavin Brown and Xin Yao},
  title =    {On The {E}ffectiveness of {N}egative {C}orrelation
                  {L}earning},
  booktitle =    {Proceedings of First UK Workshop on Computational
                  Intelligence},
  note =     {Edinburgh, Scotland},
  pages =    {57-62},
  year =     2001,
}

@INPROCEEDINGS{bruzzone01robust,
  author =   {Lorenzo Bruzzone and Roberto Cossu},
  title =    {A Robust Multiple Classifier System for a Partially
                  Unsupervised Updating of Land-Cover Maps},
  booktitle =    {Proc. Int. Workshop on Multiple Classifier Systems
                  (LNCS 2096)},
  publisher =    {Springer},
  month =    {June},
  pages =    {259-268},
  year =     2001,
  address =  {Cambridge, UK},
  url =          "http://link.springer.de/link/service/series/0558/tocs/t2096.htm"
}

@INPROCEEDINGS{chawla01small,
  author =   {Nitesh Chawla and Thomas Moore and Kevin Bowyer and
                  Lawrence Hall and Clayton Springer and Philip
                  Kegelmeyer},
  title =    {Bagging is a Small-Data-Set Phenomenon},
  booktitle =    {International Conference on Computer Vision and
                  Pattern Recognition (CVPR)},
  year =     2001,
}

@INPROCEEDINGS{chen01averaging,
  author =   {Dechang Chen and Jian Liu},
  title =    {Averaging Weak Classifiers},
  booktitle =    {Proc. Int. Workshop on Multiple Classifier Systems
                  (LNCS 2096)},
  publisher =    {Springer},
  month =    {June},
  pages =    {119-125},
  year =     2001,
  address =  {Cambridge, UK},
  url =          "http://link.springer.de/link/service/series/0558/tocs/t2096.htm"
}

@INPROCEEDINGS{cohen01automatic,
  author =   {Shimon Cohen and Nathan Intrator},
  title =    {Automatic Model Selection in a Hybrid
                  Perceptron/Radial Network},
  booktitle =    {Proc. Int. Workshop on Multiple Classifier Systems
                  (LNCS 2096)},
  publisher =    {Springer},
  month =    {June},
  pages =    {440-454},
  year =     2001,
  address =  {Cambridge, UK},
  url =          "http://link.springer.de/link/service/series/0558/tocs/t2096.htm"
}

@INPROCEEDINGS{dahmen01combined,
  author =   {Jurg Dahmen and Daniel Keysers and Hermann Ney},
  title =    {Combined Classification of Handwritten Digits Using
                  the \'Virtual Test Sample Method\'},
  booktitle =    {Proc. Int. Workshop on Multiple Classifier Systems
                  (LNCS 2096)},
  publisher =    {Springer},
  month =    {June},
  pages =    {109-118},
  year =     2001,
  address =  {Cambridge, UK},
  url =          "http://link.springer.de/link/service/series/0558/tocs/t2096.htm"
}

@INPROCEEDINGS{dietrich01classification,
  author =   {Christian Dietrich and Friedhelm Schwenker and
                  Gunther Palm},
  title =    {Classification of Time Series Utilizing Temporal and
                  Decision Fusion},
  booktitle =    {Proc. Int. Workshop on Multiple Classifier Systems
                  (LNCS 2096)},
  publisher =    {Springer},
  month =    {June},
  pages =    {378-387},
  year =     2001,
  address =  {Cambridge, UK},
  url =          "http://link.springer.de/link/service/series/0558/tocs/t2096.htm"
}

@INPROCEEDINGS{diez01learning,
  author =   {Juan J. Rodriguez Diez and Carlos Alonso Gonzalez},
  title =    {Learning Classification RBF Networks by Boosting},
  booktitle =    {Proc. Int. Workshop on Multiple Classifier Systems
                  (LNCS 2096)},
  publisher =    {Springer},
  month =    {June},
  pages =    {43-52},
  year =     2001,
  address =  {Cambridge, UK},
  url =          "http://link.springer.de/link/service/series/0558/tocs/t2096.htm"
}

@INPROCEEDINGS{dolenko01solar,
  author =   {S. A. Dolenko and Yu. V. Orlov and I. G. Persiantsev
                  and Ju. S. Shugai and A. V. Dmitriev and
                  A. V. Suvorova and I. S. Veselovsky},
  title =    {Solar Wind Data Analysis Using Self-Organizing
                  Hierarchical Neural Network Classifiers},
  booktitle =    {Proc. Int. Workshop on Multiple Classifier Systems
                  (LNCS 2096)},
  publisher =    {Springer},
  month =    {June},
  pages =    {289-298},
  year =     2001,
  address =  {Cambridge, UK},
  url =          "http://link.springer.de/link/service/series/0558/tocs/t2096.htm"
}

@BOOK{duda01book,
  author =   {Richard Duda and Peter Hart and David Stork},
  title =    {Pattern Classification},
  publisher =    {John Wiley and Sons},
  year =     2001,
  note =     {0-471-05669-3},
}

@INPROCEEDINGS{foggia01automatic,
  author =   {Pasquale Foggia and Carlo Sansone and Francesco
                  Tortorella and Mario Vento},
  title =    {Automatic Classification of Clustered
                  Microcalcifications by a Multiple Classifier System},
  booktitle =    {Proc. Int. Workshop on Multiple Classifier Systems
                  (LNCS 2096)},
  publisher =    {Springer},
  month =    {June},
  pages =    {208-217},
  year =     2001,
  address =  {Cambridge, UK},
  url =          "http://link.springer.de/link/service/series/0558/tocs/t2096.htm"
}

@INPROCEEDINGS{fred01finding,
  author =   {Ana L. N. Fred},
  title =    {Finding Consistent Clusters in Data Partitions},
  booktitle =    {Proc. Int. Workshop on Multiple Classifier Systems
                  (LNCS 2096)},
  publisher =    {Springer},
  month =    {June},
  pages =    {309-318},
  year =     2001,
  address =  {Cambridge, UK},
  url =          "http://link.springer.de/link/service/series/0558/tocs/t2096.htm"
}

@INPROCEEDINGS{freund01averaging,
  title =    {Why Averaging Classifiers can Protect Against
                  Overfitting},
  author =   {Yoav Freund and Yishay Mansour and Robert Schapire},
  booktitle =    {Eighth International Workshop on Artificial
                  Intelligence and Statistics},
  year =     2001,
}

@INPROCEEDINGS{froba01on,
  author =   {Bernhard Froba and Walter Zink},
  title =    {On the Combination of Different Template Matching
                  Strategies for Fast Face Detection},
  booktitle =    {Proc. Int. Workshop on Multiple Classifier Systems
                  (LNCS 2096)},
  publisher =    {Springer},
  month =    {June},
  pages =    {418-428},
  year =     2001,
  address =  {Cambridge, UK},
  url =          "http://link.springer.de/link/service/series/0558/tocs/t2096.htm"
}

@INPROCEEDINGS{frossyniotis01multi-svm,
  author =   {Dimitrios S. Frossyniotis and Andreas Stafylopatis},
  title =    {A Multi-SVM Classification System},
  booktitle =    {Proc. Int. Workshop on Multiple Classifier Systems
                  (LNCS 2096)},
  publisher =    {Springer},
  month =    {June},
  pages =    {198-207},
  year =     2001,
  address =  {Cambridge, UK},
  url =          "http://link.springer.de/link/service/series/0558/tocs/t2096.htm"
}

@INPROCEEDINGS{fumera01error,
  author =   {Giorgio Fumera and Fabio Roli},
  title =    {Error Rejection in Linearly Combined Multiple
                  Classifiers},
  booktitle =    {Proc. Int. Workshop on Multiple Classifier Systems
                  (LNCS 2096)},
  publisher =    {Springer},
  month =    {June},
  pages =    {329-338},
  year =     2001,
  address =  {Cambridge, UK},
  url =          "http://link.springer.de/link/service/series/0558/tocs/t2096.htm"
}

@ARTICLE{gencay01pricing,
  author =   {R. Gencay and Min Qi},
  title =    {Pricing and hedging derivative securities with
                  neural networks: Bayesian regularization, early
                  stopping, and bagging},
  journal =  {IEEE Transactions on Neural Networks},
  year =     2001,
  volume =   12,
  issue =    4,
  month =    {July},
  pages =    {726--734},
}

@INPROCEEDINGS{ghaderi01least,
  author =   {Reza Ghaderi and Terry Windeatt},
  title =    {Least Squares and Estimation Measures via Error
                  Correcting Output Code},
  booktitle =    {Proc. Int. Workshop on Multiple Classifier Systems
                  (LNCS 2096)},
  publisher =    {Springer},
  month =    {June},
  pages =    {148-157},
  year =     2001,
  address =  {Cambridge, UK},
  url =          "http://link.springer.de/link/service/series/0558/tocs/t2096.htm"
}

@INPROCEEDINGS{gini01mixing,
  author =   {Giuseppina C. Gini and Marco Lorenzini and Emilio
                  Benfenati and Raffaella Brambilla and Luca Malve},
  title =    {Mixing a Symbolic and a Subsymbolic Expert to
                  Improve Carcinogenicity Prediction of Aromatic
                  Compounds},
  booktitle =    {Proc. Int. Workshop on Multiple Classifier Systems
                  (LNCS 2096)},
  publisher =    {Springer},
  month =    {June},
  pages =    {126-135},
  year =     2001,
  address =  {Cambridge, UK},
  url =          "http://link.springer.de/link/service/series/0558/tocs/t2096.htm"
}

@INPROCEEDINGS{grandvalet01bagging,
  author =   {Y.~Grandvalet},
  title =    {Bagging can stabilize without reducing variance},
  booktitle =    {ICANN'01},
  series =   {Lecture Notes in Computer Science},
  publisher =    {Springer},
  year =     2001,
}

@INPROCEEDINGS{grim01information,
  author =   {Jiri Grim and Josef Kittler and Pavel Pudil and Petr
                  Somol},
  title =    {Information Analysis of Multiple Classifier Fusion},
  booktitle =    {Proc. Int. Workshop on Multiple Classifier Systems
                  (LNCS 2096)},
  publisher =    {Springer},
  month =    {June},
  pages =    {168-177},
  year =     2001,
  address =  {Cambridge, UK},
  url =          "http://link.springer.de/link/service/series/0558/tocs/t2096.htm"
}

@INPROCEEDINGS{hand01multiple,
  author =   {David J. Hand and Niall M. Adams and Mark G. Kelly},
  title =    {Multiple Classifier Systems Based on Interpretable
                  Linear Classifiers},
  booktitle =    {Proc. Int. Workshop on Multiple Classifier Systems
                  (LNCS 2096)},
  publisher =    {Springer},
  month =    {June},
  pages =    {136-147},
  year =     2001,
  address =  {Cambridge, UK},
  url =          "http://link.springer.de/link/service/series/0558/tocs/t2096.htm"
}

@INPROCEEDINGS{hartono01learning-data,
  author =   {Pitoyo Hartono and Shuji Hashimoto},
  title =    {Learning-Data Selection Mechanism through Neural
                  Networks Ensemble},
  booktitle =    {Proc. Int. Workshop on Multiple Classifier Systems
                  (LNCS 2096)},
  publisher =    {Springer},
  month =    {June},
  pages =    {188-197},
  year =     2001,
  address =  {Cambridge, UK},
  url =          "http://link.springer.de/link/service/series/0558/tocs/t2096.htm"
}

@INPROCEEDINGS{higgins01application,
  author =   {Jonathan E. Higgins and Tony J. Dodd and Robert
                  I. Damper},
  title =    {Application of Multiple Classifier Techniques to
                  Subband Speaker Identification with an HMM/ANN
                  System},
  booktitle =    {Proc. Int. Workshop on Multiple Classifier Systems
                  (LNCS 2096)},
  publisher =    {Springer},
  month =    {June},
  pages =    {369-377},
  year =     2001,
  address =  {Cambridge, UK},
  url =          "http://link.springer.de/link/service/series/0558/tocs/t2096.htm"
}

@INPROCEEDINGS{ho01data,
  author =   {Tin Kam Ho},
  title =    {Data Complexity Analysis for Classifier Combination},
  booktitle =    {Proc. Int. Workshop on Multiple Classifier Systems
                  (LNCS 2096)},
  publisher =    {Springer},
  month =    {June},
  pages =    {53-67},
  year =     2001,
  address =  {Cambridge, UK},
  url =          "http://link.springer.de/link/service/series/0558/tocs/t2096.htm"
}

@INPROCEEDINGS{jorgensen01feature,
  author =   {Thomas Martini Jorgensen and Christian Linneberg},
  title =    {Feature Weighted Ensemble Classifiers - A Modified
                  Decision Scheme},
  booktitle =    {Proc. Int. Workshop on Multiple Classifier Systems
                  (LNCS 2096)},
  publisher =    {Springer},
  month =    {June},
  pages =    {218-227},
  year =     2001,
  address =  {Cambridge, UK},
  url =          "http://link.springer.de/link/service/series/0558/tocs/t2096.htm"
}

@INPROCEEDINGS{joshi01evaluating,
  author =   {Mahesh V. Joshi and Vipin Kumar and Ramesh
                  C. Agarwal},
  title =    {Evaluating Boosting Algorithms to Classify Rare
                  Classes: Comparison and Improvements},
  booktitle =    {{ICDM}},
  pages =    {257-264},
  year =     2001,
  url =      {http://citeseer.nj.nec.com/joshi01evaluating.html},
}

@INPROCEEDINGS{kittler01relationship,
  author =   {Josef Kittler and Fuad M. Alkoot},
  title =    {Relationship of Sum and Vote Fusion Strategies},
  booktitle =    {Proc. Int. Workshop on Multiple Classifier Systems
                  (LNCS 2096)},
  publisher =    {Springer},
  month =    {June},
  pages =    {339-348},
  year =     2001,
  address =  {Cambridge, UK},
  url =          "http://link.springer.de/link/service/series/0558/tocs/t2096.htm"
}

@INCOLLECTION{kuncheva01:tenmeasures,
  author =   {L.I. Kuncheva and C.J. Whitaker},
  title =    {Ten Measures of Diversity in Classifier Ensembles:
                  Limits for Two Classifiers},
  booktitle =    {IEE Workshop on Intelligent Sensor Processing},
  publisher =    {IEE},
  year =     2001,
  month =    {February},
}

@INPROCEEDINGS{kuncheva01complexity,
  author =   {Ludmila Kuncheva and Fabio Roli and Gian Luca
                  Marcialis and Catherine A. Shipp},
  title =    {Complexity of Data Subsets Generated by the Random
                  Subspace Method: An Experimental Investigation},
  booktitle =    {Proc. Int. Workshop on Multiple Classifier Systems
                  (LNCS 2096)},
  publisher =    {Springer},
  month =    {June},
  pages =    {349-358},
  year =     2001,
  address =  {Cambridge, UK},
  url =          "http://link.springer.de/link/service/series/0558/tocs/t2096.htm"
}

@INPROCEEDINGS{kuncheva01feature,
  author =   {Ludmila Kuncheva and Christopher J. Whitaker},
  title =    {Feature Subsets for Classifier Combination: An
                  Enumerative Experiment},
  booktitle =    {Proc. Int. Workshop on Multiple Classifier Systems
                  (LNCS 2096)},
  publisher =    {Springer},
  month =    {June},
  pages =    {228-237},
  year =     2001,
  address =  {Cambridge, UK},
  url =          "http://link.springer.de/link/service/series/0558/tocs/t2096.htm"
}

@TECHREPORT{kutin01stability,
  author =   {Samuel Kutin and Partha Niyogi},
  title =    {The Interaction of Stability and Weakness in
                  AdaBoost},
  institution =  {The University of Chicago},
  year =     2001,
  number =   {TR-2001-30},
  url =          "http://www.cs.uchicago.edu/research/publications/techreports/TR-2001-30"
}

@TECHREPORT{langdon:2001:edf,
  author =   {W. B. Langdon},
  title =    {Evolutionary Data Fusion},
  institution =  {University College, London},
  year =     2001,
  number =   {RN/01/19},
  address =  {UK},
  month =    {3 April},
  keywords =     {genetic algorithms, genetic programming, ROC},
  url =          {http://www.cs.ucl.ac.uk/staff/W.Langdon/datafusion.html},
  url =      {http://www.cs.ucl.ac.uk/staff/W.Langdon/roc},
  size =     {12 pages},
  notes =    {Distributed at 25 April 2001 Faraday meeting
                  http://www.npl.co.uk/intersect/ },
}

@INPROCEEDINGS{langdon:2001:wsc6,
  author =   {W. B. Langdon and S. J. Barrett and B. F. Buxton},
  title =    {Genetic Programming for Combining Neural Networks
                  for Drug Discovery},
  booktitle =    {Soft Computing and Industry Recent Applications},
  year =     2001,
  editor =   {Rajkumar Roy and Mario K\"oppen and Seppo Ovaska and
                  Takeshi Furuhashi and Frank Hoffmann},
  pages =    {597--608},
  month =    {10--24 September},
  publisher =    {Springer-Verlag},
  note =     {Published 2002},
  keywords =     {genetic algorithms, genetic programming, data
                  fusion, data mining, knowledge discovery, Receiver
                  Operating Characteristics, ensemble of classifiers,
                  size fair crossover},
  isbn =     {1-85233-539-4},
  url =          "http://www.springer.de/cgi/svcat/search_book.pl?isbn=1-85233-539-4"
}

@INPROCEEDINGS{langdon:2001:gROC,
  title =    {Genetic Programming for Combining Classifiers},
  author =   {W. B. Langdon and B. F. Buxton},
  pages =    {66--73},
  year =     2001,
  publisher =    {Morgan Kaufmann},
  booktitle =    {Proceedings of the Genetic and Evolutionary
                  Computation Conference (GECCO-2001)},
  editor =   {Lee Spector and Erik D. Goodman and Annie Wu and
                  W.B. Langdon and Hans-Michael Voigt and Mitsuo Gen
                  and Sandip Sen and Marco Dorigo and Shahram Pezeshk
                  and Max H. Garzon and Edmund Burke},
  address =  {San Francisco, California, USA},
  publisher_address ={San Francisco, CA 94104, USA},
  month =    {7-11 July},
  keywords =     {genetic algorithms, genetic programming, data
                  fusion, data mining, knowledge discovery, Receiver
                  Operating Characteristics, ensemble of classifiers,
                  size fair crossover},
  isbn =     {1-55860-774-9},
  url =          {ftp://cs.ucl.ac.uk/genetic/papers/WBL_gecco2001_roc.ps.gz},
  url =          {ftp://cs.ucl.ac.uk/genetic/papers/WBL_gecco2001_roc.pdf},
  size =     {8 pages},
  abstract =     {Genetic programming (GP) can automatically fuse
                  given classifiers to produce a combined classifier
                  whose Receiver Operating Characteristics (ROC) are
                  better than scott:1998:BMVC ``Maximum Realisable
                  Receiver Operating Characteristics''
                  (MRROC). I.e. better than their convex hull. This is
                  demonstrated on artificial, medical and satellite
                  image processing bench marks.},
  notes =    {GECCO-2001 A joint meeting of the tenth
                  International Conference on Genetic Algorithms
                  (ICGA-2001) and the sixth Annual Genetic Programming
                  Conference (GP-2001) Part of spector:2001:GECCO},
}

@INPROCEEDINGS{langdon01genetic,
  author =   {William B. Langdon and Bernard F. Buxton},
  title =    {Genetic Programming for Improved Receiver Operating
                  Characteristics},
  booktitle =    {Proc. Int. Workshop on Multiple Classifier Systems
                  (LNCS 2096)},
  publisher =    {Springer},
  month =    {June},
  pages =    {68-77},
  year =     2001,
  address =  {Cambridge, UK},
  url =          "http://link.springer.de/link/service/series/0558/tocs/t2096.htm"
  keywords =     {genetic algorithms, genetic programming, data
                  fusion, data mining, knowledge discovery, Receiver
                  Operating Characteristics, ensemble of classifiers,
                  size fair crossover},
  isbn =     {3-540-42284-6 },
  size =     {10 pages},
  abstract =     {Genetic programming (GP) can automatically fuse
                  given classifiers to produce a combined classifier
                  whose Receiver Operating Characteristics (ROC) are
                  better than scott:1998:BMVC ``Maximum Realisable
                  Receiver Operating Characteristics''
                  (MRROC). I.e. better than their convex hull. This is
                  demonstrated on artificial, medical and satellite
                  image processing bench marks.},
}

@INPROCEEDINGS{langdon:2001:eROC,
  author =   {William B. Langdon and Bernard F. Buxton},
  title =    {Evolving Receiver Operating Characteristics for Data
                  Fusion},
  booktitle =    {Genetic Programming, Proceedings of EuroGP'2001},
  year =     2001,
  volume =   2038,
  pages =    {87--96},
  address =  {Lake Como, Italy},
  publisher_address ={Berlin},
  month =    {18-20 April},
  organisation = {EvoNET},
  publisher =    {Springer-Verlag},
  keywords =     {genetic algorithms, genetic programming, Data
                  Fusion, Data Mining, Knowledge Discovery, Receiver
                  Operating Characteristics, ROC, Combining
                  Classifiers},
  isbn =     {3-540-41899-7},
  url =      {http://evonet.dcs.napier.ac.uk/eurogp2001/},
  url =      {ftp://cs.ucl.ac.uk/genetic/papers/wbl_egp2001.ps.gz},
  size =     {10 pages},
  abstract =     {It has been suggested that the ``Maximum Realisable
                  Receiver Operating Characteristics'' for a
                  combination of classifiers is the convex hull of
                  their individual ROCs [Scott et al., 1998]. As
                  expected in at least some cases better ROCs can be
                  produced. We show genetic programming (GP) can
                  automatically produce a combination of classifiers
                  whose ROC is better than the convex hull of the
                  supplied classifier's ROCs.},
  notes =    {EuroGP'2001, part of miller:2001:gp},
}

@INPROCEEDINGS{latinne01limiting,
  author =   {Patrice Latinne and Olivier Debeir and Christine
                  Decaestecker},
  title =    {Limiting the Number of Trees in Random Forests},
  booktitle =    {Proc. Int. Workshop on Multiple Classifier Systems
                  (LNCS 2096)},
  publisher =    {Springer},
  month =    {June},
  pages =    {178-187},
  year =     2001,
  address =  {Cambridge, UK},
  url =          "http://link.springer.de/link/service/series/0558/tocs/t2096.htm"
}

@INPROCEEDINGS{liu01mutual,
  author =   {Yong Liu and Xin Yao and Qiangfu Zhao and Tetsuya
                  Higuchi},
  title =    {Evolving a Cooperative Population of Neural Networks
                  by Minimizing Mutual Information},
  booktitle =    {Proceedings of the 2001 Congress on Evolutionary
                  Computation},
  pages =    {384-389},
  year =     2001,
  month =    {May},
  publisher =    {IEEE Press},
}

@INPROCEEDINGS{luttrell01self-organising,
  author =   {S. P. Luttrell},
  title =    {A Self-Organising Approach to Multiple Classifier
                  Fusion},
  booktitle =    {Proc. Int. Workshop on Multiple Classifier Systems
                  (LNCS 2096)},
  publisher =    {Springer},
  month =    {June},
  pages =    {319-328},
  year =     2001,
  address =  {Cambridge, UK},
  url =          "http://link.springer.de/link/service/series/0558/tocs/t2096.htm"
}

@INPROCEEDINGS{marti01use,
  author =   {Urs-Viktor Marti and Horst Bunke},
  title =    {Use of Positional Information in Sequence Alignment
                  for Multiple Classifier Combination},
  booktitle =    {Proc. Int. Workshop on Multiple Classifier Systems
                  (LNCS 2096)},
  publisher =    {Springer},
  month =    {June},
  pages =    {388-398},
  year =     2001,
  address =  {Cambridge, UK},
  url =          "http://link.springer.de/link/service/series/0558/tocs/t2096.htm"
}

@INPROCEEDINGS{masulli01dependence,
  author =   {Francesco Masulli and Giorgio Valentini},
  title =    {Dependence among Codeword Bits Errors in ECOC
                  Learning Machines: An Experimental Analysis},
  booktitle =    {Proc. Int. Workshop on Multiple Classifier Systems
                  (LNCS 2096)},
  publisher =    {Springer},
  month =    {June},
  pages =    {158-167},
  year =     2001,
  address =  {Cambridge, UK},
  url =          "http://link.springer.de/link/service/series/0558/tocs/t2096.htm"
}

@INPROCEEDINGS{mckayabbass01analyzing,
  author =   {R. McKay and H. Abbass},
  title =    {Analyzing Anticorrelation in Ensemble Learning},
  booktitle =    {Proceedings of 2001 Conference on Artificial Neural
                  Networks and Expert Systems},
  year =     2001,
  pages =    {22--27},
  address =  {Otago, New Zealand},
}

@INPROCEEDINGS{mckayabbass01rtqrt,
  author =   {Robert McKay and Hussein Abbass},
  title =    {Anticorrelation Measures in Genetic Programming},
  booktitle =    {Australasia-Japan Workshop on Intelligent and
                  Evolutionary Systems},
  pages =    {45--51},
  year =     2001,
}

@INPROCEEDINGS{merler01tuning,
  author =   {Stefano Merler and Cesare Furlanello and Barbara
                  Larcher and Andrea Sboner},
  title =    {Tuning Cost-Sensitive Boosting and Its Application
                  to Melanoma Diagnosis},
  booktitle =    {Proc. Int. Workshop on Multiple Classifier Systems
                  (LNCS 2096)},
  publisher =    {Springer},
  month =    {June},
  pages =    {32-42},
  year =     2001,
  address =  {Cambridge, UK},
  url =          "http://link.springer.de/link/service/series/0558/tocs/t2096.htm"
}

@phdthesis{oza2001oel,
  title={{Online Ensemble Learning}},
  author={Oza, N.C.},
  year={2001},
  school={UNIVERSITY of CALIFORNIA}
}

@article{oza2001eco,
  title={{Experimental comparisons of online and batch versions of bagging and boosting}},
  author={Oza, N.C. and Russell, S.},
  journal={Proceedings of the seventh ACM SIGKDD international conference on Knowledge discovery and data mining},
  pages={359-364},
  year={2001},
  publisher={ACM Press New York, NY, USA}
}

@INPROCEEDINGS{oza01input,
  author =   {Nikunj C. Oza and Kagan Tumer},
  title =    {Input Decimation Ensembles: Decorrelation through
                  Dimensionality Reduction},
  booktitle =    {Proc. Int. Workshop on Multiple Classifier Systems
                  (LNCS 2096)},
  publisher =    {Springer},
  month =    {June},
  pages =    {238-247},
  year =     2001,
  address =  {Cambridge, UK},
  url =          "http://link.springer.de/link/service/series/0558/tocs/t2096.htm"
}

@INPROCEEDINGS{pekalska01on,
  author =   {Elzbieta Pekalska and Robert P. W. Duin},
  title =    {On Combining Dissimilarity Representations},
  booktitle =    {Proc. Int. Workshop on Multiple Classifier Systems
                  (LNCS 2096)},
  publisher =    {Springer},
  month =    {June},
  pages =    {359-368},
  year =     2001,
  address =  {Cambridge, UK},
  url =          "http://link.springer.de/link/service/series/0558/tocs/t2096.htm"
}

@INPROCEEDINGS{prabhakar01decision-level,
  author =   {Salil Prabhakar and Anil K. Jain},
  title =    {Decision-Level Fusion in Fingerprint Verification},
  booktitle =    {Proc. Int. Workshop on Multiple Classifier Systems
                  (LNCS 2096)},
  publisher =    {Springer},
  month =    {June},
  pages =    {88-98},
  year =     2001,
  address =  {Cambridge, UK},
  url =          "http://link.springer.de/link/service/series/0558/tocs/t2096.htm"
}

@INPROCEEDINGS{roli01methods,
  author =   {Fabio Roli and Giorgio Giacinto and Gianni Vernazza},
  title =    {Methods for Designing Multiple Classifier Systems},
  booktitle =    {Proc. Int. Workshop on Multiple Classifier Systems
                  (LNCS 2096)},
  publisher =    {Springer},
  month =    {June},
  pages =    {78-87},
  year =     2001,
  address =  {Cambridge, UK},
  url =          "http://link.springer.de/link/service/series/0558/tocs/t2096.htm"
}

@PROCEEDINGS{mcs2001,
  title =    {Lecture Notes in Computer Science : Second
                  International Workshop on Multiple Classifier
                  Systems},
  year =     2001,
  editor =   {Fabio Roli and Josef Kittler},
  volume =   2096,
  address =  {Cambridge, UK},
  month =    {July},
}

@INPROCEEDINGS{ruta01application,
  author =   {Dymitr Ruta and Bogdan Gabrys},
  title =    {Application of the Evolutionary Algorithms for
                  Classifier Selection in Multiple Classifier Systems
                  with Majority Voting},
  booktitle =    {Proc. Int. Workshop on Multiple Classifier Systems
                  (LNCS 2096)},
  publisher =    {Springer},
  month =    {June},
  pages =    {399-408},
  year =     2001,
  address =  {Cambridge, UK},
  url =          "http://link.springer.de/link/service/series/0558/tocs/t2096.htm"
}

@INPROCEEDINGS{schwenker01tree-structured,
  author =   {Friedhelm Schwenker and Gunther Palm},
  title =    {Tree-Structured Support Vector Machines for
                  Multi-class Pattern Recognition},
  booktitle =    {Proc. Int. Workshop on Multiple Classifier Systems
                  (LNCS 2096)},
  publisher =    {Springer},
  month =    {June},
  pages =    {409-417},
  year =     2001,
  address =  {Cambridge, UK},
  url =          "http://link.springer.de/link/service/series/0558/tocs/t2096.htm"
}

@INPROCEEDINGS{sirlantzis01genetic,
  author =   {Konstantinos Sirlantzis and Michael C. Fairhurst and
                  Sanaul Hoque},
  title =    {Genetic Algorithms for Multi-classifier System
                  Configuration: A Case Study in Character
                  Recognition},
  booktitle =    {Proc. Int. Workshop on Multiple Classifier Systems
                  (LNCS 2096)},
  publisher =    {Springer},
  month =    {June},
  pages =    {99-108},
  year =     2001,
  address =  {Cambridge, UK},
  url =          "http://link.springer.de/link/service/series/0558/tocs/t2096.htm"
}

@INPROCEEDINGS{skurichina01bagging,
  author =   {Marina Skurichina and Robert P. W. Duin},
  title =    {Bagging and the Random Subspace Method for Redundant
                  Feature Spaces},
  booktitle =    {Proc. Int. Workshop on Multiple Classifier Systems
                  (LNCS 2096)},
  publisher =    {Springer},
  month =    {June},
  pages =    {1-10},
  year =     2001,
  address =  {Cambridge, UK},
  url =          "http://link.springer.de/link/service/series/0558/tocs/t2096.htm"
}

@INPROCEEDINGS{smits01combining,
  author =   {Paul C. Smits},
  title =    {Combining Supervised Remote Sensing Image
                  Classifiers Based on Individual Class Performances},
  booktitle =    {Proc. Int. Workshop on Multiple Classifier Systems
                  (LNCS 2096)},
  publisher =    {Springer},
  month =    {June},
  pages =    {269-278},
  year =     2001,
  address =  {Cambridge, UK},
  url =          "http://link.springer.de/link/service/series/0558/tocs/t2096.htm"
}

@ARTICLE{suganthan01hierarchical,
  author =   {P. N. Suganthan},
  title =    {Pattern classification using multiple hierarchical
                  overlapped self-organising maps},
  journal =  {Pattern Recognition},
  year =     2001,
  volume =   34,
  number =   11,
  pages =    {2173-2179},
  month =    {November},
}

@INPROCEEDINGS{tapia01generalized,
  author =   {Elizabeth Tapia and Jose Carlos Gonzalez and Julio
                  Villena},
  title =    {A Generalized Class of Boosting Algorithms Based on
                  Recursive Decoding Models},
  booktitle =    {Proc. Int. Workshop on Multiple Classifier Systems
                  (LNCS 2096)},
  publisher =    {Springer},
  month =    {June},
  pages =    {22-31},
  year =     2001,
  address =  {Cambridge, UK},
  url =          "http://link.springer.de/link/service/series/0558/tocs/t2096.htm"
}

@INPROCEEDINGS{tax01combining,
  author =   {David M. J. Tax and Robert P. W. Duin},
  title =    {Combining One-Class Classifiers},
  booktitle =    {Proc. Int. Workshop on Multiple Classifier Systems
                  (LNCS 2096)},
  publisher =    {Springer},
  month =    {June},
  pages =    {299-308},
  year =     2001,
  address =  {Cambridge, UK},
  url =          "http://link.springer.de/link/service/series/0558/tocs/t2096.htm"
}

@ARTICLE{tetko01volume,
  author =   {Tetko, I. V. and Kovalishyn, V. V. and Livingstone,
                  D. J.},
  title =    {Volume learning algorithm artificial neural networks
                  for 3D QSAR studies},
  journal =  {J Med Chem},
  volume =   44,
  number =   15,
  pages =    {2411-20.},
  abstract =     {The current study introduces a new method, the
                  volume learning algorithm (VLA), for the
                  investigation of three-dimensional quantitative
                  structure-activity relationships (QSAR) of chemical
                  compounds. This method incorporates the advantages
                  of comparative molecular field analysis (CoMFA) and
                  artificial neural network approaches. VLA is a
                  combination of supervised and unsupervised neural
                  networks applied to solve the same problem. The
                  supervised algorithm is a feed-forward neural
                  network trained with a back-propagation algorithm
                  while the unsupervised network is a self-organizing
                  map of Kohonen. The use of both of these algorithms
                  makes it possible to cluster the input CoMFA field
                  variables and to use only a small number of the most
                  relevant parameters to correlate spatial properties
                  of the molecules with their activity. The
                  statistical coefficients calculated by the proposed
                  algorithm for cannabimimetic aminoalkyl indoles were
                  comparable to, or improved, in comparison to the
                  original study using the partial least squares
                  algorithm. The results of the algorithm can be
                  visualized and easily interpreted. Thus, VLA is a
                  new convenient tool for three-dimensional QSAR
                  studies.},
  year =     2001,
}

@ARTICLE{tetko01estimation,
  author =   {Tetko, I. V. and Tanchuk, V. Y. and Kasheva,
                  T. N. and Villa, A. E.},
  title =    {Estimation of aqueous solubility of chemical
                  compounds using E-state indices},
  journal =  {J Chem Inf Comput Sci},
  volume =   41,
  number =   6,
  pages =    {1488-93.},
  abstract =     {The molecular weight and electrotopological E-state
                  indices were used to estimate by Artificial Neural
                  Networks aqueous solubility for a diverse set of
                  1291 organic compounds. The neural network with
                  33-4-1 neurons provided highly predictive results
                  with r(2) = 0.91 and RMS = 0.62. The used parameters
                  included several combinations of E-state indices
                  with similar properties. The calculated results were
                  similar to those published for these data by
                  Huuskonen (2000). However, in the current study only
                  E-state indices were used without need of additional
                  indices (the molecular connectivity, shape,
                  flexibility and indicator indices) also considered
                  in the previous study. In addition, the present
                  neural network contained three times less hidden
                  neurons. Smaller neural networks and use of one
                  homogeneous set of parameters provides a more robust
                  model for prediction of aqueous solubility of
                  chemical compounds. Limitations of the developed
                  method for prediction of large compounds are
                  discussed, The developed approach is available
                  online at http://www.vcclab.org/lab/alogps},
  year =     2001,
}

@ARTICLE{tetko01prediction,
  author =   {Tetko, I. V. and Tanchuk, V. Y. and Villa, A. E.},
  title =    {Prediction of n-octanol/water partition coefficients
                  from PHYSPROP database using artificial neural
                  networks and E-state indices},
  journal =  {J Chem Inf Comput Sci},
  volume =   41,
  number =   5,
  pages =    {1407-21.},
  abstract =     {The molecular weight and electrotopological E-state
                  indices were used to estimate by Artificial Neural
                  Networks aqueous solubility for a diverse set of
                  1291 organic compounds. The neural network with
                  33-4-1 neurons provided highly predictive results
                  with r(2) = 0.91 and RMS = 0.62. The used parameters
                  included several combinations of E-state indices
                  with similar properties. The calculated results were
                  similar to those published for these data by
                  Huuskonen (2000). However, in the current study only
                  E-state indices were used without need of additional
                  indices (the molecular connectivity, shape,
                  flexibility and indicator indices) also considered
                  in the previous study. In addition, the present
                  neural network contained three times less hidden
                  neurons. Smaller neural networks and use of one
                  homogeneous set of parameters provides a more robust
                  model for prediction of aqueous solubility of
                  chemical compounds. Limitations of the developed
                  method for prediction of large compounds are
                  discussed. The developed approach is available
                  online at http://www.vcclab.org/lab/alogps.},
  year =     2001,
}

@INBOOK{tresp01committeemachines,
  author =   {Volker Tresp},
  editor =   {Yu Hen Hu and Jenq-Nen Hwang},
  title =    {Handbook for Neural Network Signal Processing},
  chapter =  {Committee machines},
  publisher =    {CRC Press},
  year =     2001,
}

@ARTICLE{valev01multi,
  author =   {V. Valev and A. Asaithambi},
  title =    {Multidimensional pattern recognition problems and
                  combining classifiers},
  journal =  {Pattern Recognition Letters},
  volume =   22,
  year =     2001,
  number =   12,
  month =    {October},
  pages =    {1291-1297},
}

@INPROCEEDINGS{wang01diversity,
  author =   {Wenjia Wang and Derek Partridge and John
                  Etherington},
  title =    {Hybrid Ensembles and Coincident-Failure Diversity},
  booktitle =    {Proceedings of the International Joint Conference on
                  Neural Networks},
  publisher =    {IEEE Press},
  month =    {July},
  pages =    {2376--2381},
  volume =   4,
  year =     2001,
  address =  {Washington, USA},
}

@INPROCEEDINGS{wickramaratna01performance,
  author =   {Jeevani Wickramaratna and Sean B. Holden and Bernard
                  F. Buxton},
  title =    {Performance Degradation in Boosting},
  booktitle =    {Proc. Int. Workshop on Multiple Classifier Systems
                  (LNCS 2096)},
  publisher =    {Springer},
  month =    {June},
  pages =    {11-21},
  year =     2001,
  address =  {Cambridge, UK},
  url =          "http://link.springer.de/link/service/series/0558/tocs/t2096.htm"
}

@INPROCEEDINGS{windridge01classifier,
  author =   {David Windridge and Josef Kittler},
  title =    {Classifier Combination as a Tomographic Process},
  booktitle =    {Proc. Int. Workshop on Multiple Classifier Systems
                  (LNCS 2096)},
  publisher =    {Springer},
  month =    {June},
  pages =    {248-258},
  year =     2001,
  address =  {Cambridge, UK},
  url =          "http://link.springer.de/link/service/series/0558/tocs/t2096.htm"
}

@INPROCEEDINGS{yaobrown01:telecoms,
  author =   {X. Yao and M. Fischer and G. Brown},
  title =    {Neural Network Ensembles and their Application to
                  Traffic Flow Prediction in Telecommunications
                  Networks},
  booktitle =    {Proceedings of International Joint Conference on
                  Neural Networks},
  note =     {Washington DC},
  pages =    {693-698},
  publisher =    {IEEE Press},
  year =     2001,
}

@ARTICLE{zenobi01using,
  author =   {Gabriele Zenobi and P{\'a}draig Cunningham},
  title =    {Using Diversity in Preparing Ensembles of
                  Classifiers Based on Different Feature Subsets to
                  Minimize Generalization Error},
  journal =  {Lecture Notes in Computer Science},
  volume =   2167,
  pages =    {576--587},
  year =     2001,
}

@INPROCEEDINGS{langdon02gp,
  year =     2002,
  title =    {A Hybrid Genetic Programming Neural Network
                  Classifier for Use in Drug Discovery},
  institution =  {Department of Computer Science -- University College
                  London -- UK},
  booktitle =    {Soft Computing Systems - Design, Management and
                  Applications},
  author =   {William B. Langdon},
  abstract =     {We have shown genetic programming (GP) can
                  automatically fuse given classifiers of diverse
                  types to produce a hybrid classifier. Combinations
                  of neural networks, decision trees and Bayes
                  classifier shave been formed. On a range of
                  benchmarks the evolved multiple classifier system is
                  better than all of its components. Indeed its
                  Receiver Operating Characteristics (ROC) are better
                  than [Scott et al., 1998]s "Maximum Realisable
                  Receiver Operating Characteristics" MRROC (convex
                  hull) An important component in the drug discovery
                  is testing potential drugs for activity with P450
                  cell membrane molecules. Our technique has been used
                  in a blind trial where artificial neural networks
                  are trained by Clementine on P450 pharmaceutical
                  data. Using just the trained networks, GP
                  automatically evolves a composite classifier. Recent
                  experiments with boosting the networks will be
                  compared with genetic programming.}
}

@INPROCEEDINGS{altincay02post-processing,
  author =   {Hakan Altincay and Mubeccel Demirekler},
  title =    {Post-processing of Classifier Outputs in Multiple
                  Classifier Systems},
  booktitle =    {Proc. Int. Workshop on Multiple Classifier Systems
                  (LNCS 2364)},
  publisher =    {Springer},
  month =    {June},
  pages =    {159-168},
  year =     2002,
  address =  {Calgiari, Italy},
  url =          "http://link.springer.de/link/service/series/0558/tocs/t2364.htm"
}

@INPROCEEDINGS{benfenati02combining,
  author =   {Emilio Benfenati and Paolo Mazzatorta and Daniel
                  Neagu and Giuseppina C. Gini},
  title =    {Combining Classifiers of Pesticides Toxicity through
                  a Neuro-fuzzy Approach},
  booktitle =    {Proc. Int. Workshop on Multiple Classifier Systems
                  (LNCS 2364)},
  publisher =    {Springer},
  month =    {June},
  pages =    {293-303},
  year =     2002,
  address =  {Calgiari, Italy},
  url =          "http://link.springer.de/link/service/series/0558/tocs/t2364.htm"
}

@INPROCEEDINGS{brownyao02exploiting,
  author =   {Gavin Brown and Xin Yao and Jeremy Wyatt and Heiko
                  Wersing and Bernhard Sendhoff},
  title =    {Exploiting Ensemble Diversity for Automatic Feature
                  Extraction},
  booktitle =    {Proc. of the 9th International Conference on Neural
                  Information Processing (ICONIP'02)},
  pages =    {1786-1790},
  year =     2002,
  month =    {November},
}

@INPROCEEDINGS{caprile02highlighting,
  author =   {Bruno Caprile and Cesare Furlanello and Stefano
                  Merler},
  title =    {Highlighting Hard Patterns via AdaBoost Weights
                  Evolution},
  booktitle =    {Proc. Int. Workshop on Multiple Classifier Systems
                  (LNCS 2364)},
  publisher =    {Springer},
  month =    {June},
  pages =    {72-80},
  year =     2002,
  address =  {Calgiari, Italy},
  url =          "http://link.springer.de/link/service/series/0558/tocs/t2364.htm"
}

@INPROCEEDINGS{chawla02distributed,
  author =   {Nitesh V. Chawla and Lawrence O. Hall and Kevin
                  W. Bowyer and Thomas E. Moore and W. Philip
                  Kegelmeyer},
  title =    {Distributed Pasting of Small Votes},
  booktitle =    {Proc. Int. Workshop on Multiple Classifier Systems
                  (LNCS 2364)},
  publisher =    {Springer},
  month =    {June},
  pages =    {52-61},
  year =     2002,
  address =  {Calgiari, Italy},
  url =          "http://link.springer.de/link/service/series/0558/tocs/t2364.htm"
}

@ARTICLE{cohenIntrator01,
  author =   {S. Cohen and N. Intrator},
  year =     2002,
  title =    {A hybrid projection based and radial basis function
                  architecture: Initial values and global
                  optimization},
  journal =  {Pattern Anal. Appl. (Special issue on Fusion of
                  Multiple Classifiers)},
  volume =   5,
  number =   2,
  pages =    {113--120},
}

@ARTICLE{cohenIntratorFusion01,
  author =   {S. Cohen and N. Intrator},
  title =    {Automatic model selection in a hybrid
                  perceptron/radial network},
  journal =  {Information Fusion},
  volume =   3,
  number =   4,
  pages =    {259--266},
  year =     2002,
}

@INPROCEEDINGS{cohen02forward,
  author =   {Shimon Cohen and Nathan Intrator},
  title =    {Forward and Backward Selection in Regression Hybrid
                  Network},
  booktitle =    {Proc. Int. Workshop on Multiple Classifier Systems
                  (LNCS 2364)},
  publisher =    {Springer},
  month =    {June},
  pages =    {98-107},
  year =     2002,
  address =  {Calgiari, Italy},
  url =          "http://link.springer.de/link/service/series/0558/tocs/t2364.htm"
}

@INPROCEEDINGS{cordella02multi-expert,
  author =   {Luigi P. Cordella and Massimo De Santo and Gennaro
                  Percannella and Carlo Sansone and Mario Vento},
  title =    {A Multi-expert System for Movie Segmentation},
  booktitle =    {Proc. Int. Workshop on Multiple Classifier Systems
                  (LNCS 2364)},
  publisher =    {Springer},
  month =    {June},
  pages =    {304-313},
  year =     2002,
  address =  {Calgiari, Italy},
  url =          "http://link.springer.de/link/service/series/0558/tocs/t2364.htm"
}

@INPROCEEDINGS{dzeroski02stacking,
  author =   {Saso Dzeroski and Bernard Zenko},
  title =    {Stacking with Multi-response Model Trees},
  booktitle =    {Proc. Int. Workshop on Multiple Classifier Systems
                  (LNCS 2364)},
  publisher =    {Springer},
  month =    {June},
  pages =    {201-211},
  year =     2002,
  address =  {Calgiari, Italy},
  url =          "http://link.springer.de/link/service/series/0558/tocs/t2364.htm"
}

@INPROCEEDINGS{ghosh02multiclassifier,
  author =   {Joydeep Ghosh},
  title =    {Multiclassifier Systems: Back to the Future},
  booktitle =    {Proc. Int. Workshop on Multiple Classifier Systems
                  (LNCS 2364)},
  publisher =    {Springer},
  month =    {June},
  pages =    {1-15},
  year =     2002,
  address =  {Calgiari, Italy},
  url =          "http://link.springer.de/link/service/series/0558/tocs/t2364.htm"
}

@INPROCEEDINGS{gunter02generating,
  author =   {Simon Gunter and Horst Bunke},
  title =    {Generating Classifier Ensembles from Multiple
                  Prototypes and Its Application to Handwriting
                  Recognition},
  booktitle =    {Proc. Int. Workshop on Multiple Classifier Systems
                  (LNCS 2364)},
  publisher =    {Springer},
  month =    {June},
  pages =    {179-188},
  year =     2002,
  address =  {Calgiari, Italy},
  url =          "http://link.springer.de/link/service/series/0558/tocs/t2364.htm"
}

@ARTICLE{hayashi02medical,
  author =   {Y. Hayashi and R. Setiono},
  title =    {Combining neural network predictions for medical
                  diagnosis},
  journal =  {Computers in Biology and Medicine},
  year =     2002,
  volume =   32,
  number =   4,
  pages =    {237--246},
}

@INPROCEEDINGS{wuzhouchen02:ensembling,
  author =   {J.X.Wu and Z.H.Zhou and Z.Q.Chen},
  title =    "Ensemble of {GA} based selective neural network
                  ensembles" ,
  booktitle =    {8th International Conference on Neural Information
                  Processing (ICONIP)},
  volume =   3,
  pages =    {1477-1482},
  year =     2002,
}

@INPROCEEDINGS{janeliunas02reduction,
  author =   {Arunas Janeliunas and Sarunas Raudys},
  title =    {Reduction of the Boasting Bias of Linear Experts},
  booktitle =    {Proc. Int. Workshop on Multiple Classifier Systems
                  (LNCS 2364)},
  publisher =    {Springer},
  month =    {June},
  pages =    {242-251},
  year =     2002,
  address =  {Calgiari, Italy},
  url =          "http://link.springer.de/link/service/series/0558/tocs/t2364.htm"
}

@INPROCEEDINGS{kanamori02mixture,
  author =   {Takafumi Kanamori},
  title =    {A New Sequential Algorithm for Regression Problems
                  by using Mixture Distribution},
  booktitle =    {Int. Conf. Artif. Neur. Netw. ICANN},
  pages =    {535--540},
  year =     2002,
  publisher =    {Springer},
  url =      {http://citeseer.nj.nec.com/573689.html},
}

@INPROCEEDINGS{khareyao02,
  author =   {Vineet Khare and Xin Yao},
  title =    {Artificial Speciation of Neural Network Ensembles},
  booktitle =    {Proc. of the 2002 UK Workshop on Computational
                  Intelligence (UKCI'02)},
  pages =    {96--103},
  year =     2002,
  editor =   {J.A.Bullinaria},
  month =    {September},
  organization = {University of Birmingham, UK},
}

@INPROCEEDINGS{kittler02decision,
  author =   {Josef Kittler and Marco Ballette and Jacek Czyz and
                  Fabio Roli and Luc Vandendorpe},
  title =    {Decision Level Fusion of Intramodal Personal
                  Identity Verification Experts},
  booktitle =    {Proc. Int. Workshop on Multiple Classifier Systems
                  (LNCS 2364)},
  publisher =    {Springer},
  month =    {June},
  pages =    {314-324},
  year =     2002,
  address =  {Calgiari, Italy},
  url =          "http://link.springer.de/link/service/series/0558/tocs/t2364.htm"
}

@ARTICLE{kuncheva02sixstrategies,
  author =   {Kuncheva, L.I.},
  title =    {A Theoretical Study on Six Classifier Fusion
                  Strategies},
  journal =  {PAMI},
  volume =   24,
  year =     2002,
  number =   2,
  month =    {February},
  pages =    {281-286},
}

@ARTICLE{kuncheva02switching,
  author =   {L.I. Kuncheva},
  title =    {Switching between selection and fusion in combining
                  classifiers: An experiment},
  journal =  {IEEE Transactions On Systems Man And Cybernetics},
  volume =   32,
  number =   2,
  pages =    {146--156},
  year =     2002,
}

@ARTICLE{kuncheva02generating,
  author =   {L.I. Kuncheva and R.K. Kountchev},
  title =    {Generating Classifier Outputs of Fixed Accuracy and
                  Diversity},
  journal =  {Pattern Recognition Letters},
  year =     2002,
  number =   23,
  pages =    {593-600},
}

@INPROCEEDINGS{kuncheva02using,
  author =   {Ludmila Kuncheva and Christopher Whitaker},
  title =    {Using Diversity with Three Variants of Boosting:
                  Aggressive, Conservative, and Inverse},
  booktitle =    {Proc. Int. Workshop on Multiple Classifier Systems
                  (LNCS 2364)},
  publisher =    {Springer},
  month =    {June},
  pages =    {81-90},
  year =     2002,
  address =  {Calgiari, Italy},
  url =          "http://link.springer.de/link/service/series/0558/tocs/t2364.htm"
}

@INPROCEEDINGS{lai02on,
  author =   {Carmen Lai and David M. J. Tax and Robert P. W. Duin
                  and Elzbieta Pekalska and Pavel Pacl?k},
  title =    {On Combining One-Class Classifiers for Image
                  Database Retrieval},
  booktitle =    {Proc. Int. Workshop on Multiple Classifier Systems
                  (LNCS 2364)},
  publisher =    {Springer},
  month =    {June},
  pages =    {212-221},
  year =     2002,
  address =  {Calgiari, Italy},
  url =          "http://link.springer.de/link/service/series/0558/tocs/t2364.htm"
}

@MISC{langdon:2002:iberamia,
  author =   {W. B. Langdon},
  title =    {Application of genetic programming in drug lead
                  discovery},
  howpublished = {8th Iberoamerican Conference on Artificial
                  Intelligence},
  year =     2002,
  month =    {12 November},
  note =     {Invited conference speaker},
  address =  {Seville, Spain},
  keywords =     {genetic algorithms, genetic programming},
  url =          {http://www.lsi.us.es/iberamia2002/docs/plenarysession.pdf},
  notes =
                  {http://www.lsi.us.es/iberamia2002/principal_eng.html},
}

@MISC{langdon:2002:kdmdd,
  author =   {W. B. Langdon and B. F. Buxton and S. J. Barrett},
  title =    {Combining Machine Learning techniques to Predict
                  Compounds' Cytochrome P450 High Throughput Screening
                  Inhibition},
  howpublished = {Knowledge Discovery meets Drug Discovery},
  year =     2002,
  month =    {23 October},
  note =     {poster},
  keywords =     {genetic algorithms, genetic programming},
  url =      {ftp://cs.ucl.ac.uk/genetic/papers/wbl_kdmdd2002.pdf},
  notes =
                  {http://www.kdnet.org/workshop_overview1_bioinfoLeuven02.htm},
}

@INPROCEEDINGS{langdon:2002:EuroGP,
  title =    {Combining Decision Trees and Neural Networks for
                  Drug Discovery},
  author =   {William B. Langdon and S. J. Barrett and
                  B. F. Buxton},
  booktitle =    {Genetic Programming, Proceedings of the 5th European
                  Conference, EuroGP 2002},
  pages =    {60--70},
  address =  {Kinsale, Ireland},
  publisher_address ={Berlin},
  month =    {3-5 April},
  year =     2002,
  keywords =     {genetic algorithms, genetic programming, drug
                  design, Receiver Operating Characteristics (ROC),
                  ensemble of classifiers, data fusion, artificial
                  neural networks, clementine, decision trees C4.5,
                  high through put screening (HTS)},
  isbn =     {3-540-43378-3},
  size =     {10 pages},
  abstract =     {Genetic programming (GP) offers a generic method of
                  automatically fusing together classifiers using
                  their receiver operating characteristics (ROC) to
                  yield superior ensembles. We combine decision trees
                  (C4.5) and artificial neural networks (ANN) on a
                  difficult pharmaceutical data mining (KDD) drug
                  discovery application. Specifically predicting
                  inhibition of a P450 enzyme. Training data came from
                  high throughput screening (HTS) runs. The evolved
                  model may be used to predict behaviour of virtual
                  (i.e. yet to be manufactured) chemicals. Measures to
                  reduce over fitting are also described. },
  notes =    {EuroGP'2002, part of lutton:2002:GP},
}

@INPROCEEDINGS{liuyao02decision,
  author =   {Y. Liu and X. Yao and Q. Zhao and T. Higuchi},
  title =    {An experimental comparison of neural network
                  ensemble learning methods on decision boundaries},
  booktitle =    {Proceedings of the 2002 International Joint
                  Conference on Neural Networks (IJCNN'02)},
  pages =    {221-226},
  month =    {May},
  year =     2002,
  publisher =    {IEEE Press, Piscataway, NJ, USA},
}

@INPROCEEDINGS{luochen02a,
  author =   {Dingsheng Luo and Ke Chen},
  title =    {A comparative study of statistical ensemble methods
                  on mismatch conditions},
  booktitle =    {Proceedings of World Congress on Computational
                  Intelligence: International Joint Conference on
                  Neural Networks (WCCI 2002 and IJCNN 2002)},
  year =     2002,
  address =  {Honolulu USA},
  publisher =    {IEEE Press},
  pages =    {59--64},
  month =    {November},
}

@INPROCEEDINGS{luochen02b,
  author =   {Dingsheng Luo and Ke Chen},
  title =    {On the use of statistical ensemble methods for
                  telephone-line speaker identification},
  booktitle =    {Proceedings of International Joint Conference on
                  Communications, Circuits and Systems (ICCCAS'2002)},
  year =     2002,
  publisher =    {IEEE Press},
  address =  {Chengdu, China},
  pages =    {II904-II908},
  month =    {July},
}

@INPROCEEDINGS{masulli02boosting,
  author =   {Francesco Masulli and Matteo Pardo and Giorgio
                  Sberveglieri and Giorgio Valentini},
  title =    {Boosting and Classification of Electronic Nose Data},
  booktitle =    {Proc. Int. Workshop on Multiple Classifier Systems
                  (LNCS 2364)},
  publisher =    {Springer},
  month =    {June},
  pages =    {262-271},
  year =     2002,
  address =  {Calgiari, Italy},
  url =          "http://link.springer.de/link/service/series/0558/tocs/t2364.htm"
}

@INPROCEEDINGS{minguill?n02classifier,
  author =   {Juli? Minguill?n and Anne Rosemary Tate and Carles
                  Ar?s and John R. Griffiths},
  title =    {Classifier Combination for In Vivo Magnetic
                  Resonance Spectra of Brain Tumours},
  booktitle =    {Proc. Int. Workshop on Multiple Classifier Systems
                  (LNCS 2364)},
  publisher =    {Springer},
  month =    {June},
  pages =    {282-292},
  year =     2002,
  address =  {Calgiari, Italy},
  url =          "http://link.springer.de/link/service/series/0558/tocs/t2364.htm"
}

@INPROCEEDINGS{mola02discriminant,
  author =   {Francesco Mola and Roberta Siciliano},
  title =    {Discriminant Analysis and Factorial Multiple Splits
                  in Recursive Partitioning for Data Mining},
  booktitle =    {Proc. Int. Workshop on Multiple Classifier Systems
                  (LNCS 2364)},
  publisher =    {Springer},
  month =    {June},
  pages =    {118-126},
  year =     2002,
  address =  {Calgiari, Italy},
  url =          "http://link.springer.de/link/service/series/0558/tocs/t2364.htm"
}

@INPROCEEDINGS{morgan02adaptive,
  author =   {Joseph T. Morgan and Alex Henneguelle and Melba
                  M. Crawford and Joydeep Ghosh and Amy
                  Neuenschwander},
  title =    {Adaptive Feature Spaces for Land Cover
                  Classification with Limited Ground Truth Data},
  booktitle =    {Proc. Int. Workshop on Multiple Classifier Systems
                  (LNCS 2364)},
  publisher =    {Springer},
  month =    {June},
  pages =    {189-200},
  year =     2002,
  address =  {Calgiari, Italy},
  url =          "http://link.springer.de/link/service/series/0558/tocs/t2364.htm"
}

@ARTICLE{murua02upperbound,
  author =   {Murua, A.},
  title =    {Upper Bounds for Error Rates of Linear Combinations
                  of Classifiers},
  journal =  {PAMI},
  volume =   24,
  year =     2002,
  number =   5,
  month =    {May},
  pages =    {591-602},
}

@ARTICLE{nair:jmlr02,
  title =    {Some greedy learning algorithms for sparse
                  regression and classification with mercer kernels},
  author =   {P. B. Nair and A. Choudhury and A. J. Keane},
  journal =  {Journal of Machine Learning Research},
  volume =   3,
  pages =    {781-801},
  year =     2002,
}

@INPROCEEDINGS{pekalska02discussion,
  author =   {Elzbieta Pekalska and Robert P. W. Duin and Marina
                  Skurichina},
  title =    {A Discussion on the Classifier Projection Space for
                  Classifier Combining},
  booktitle =    {Proc. Int. Workshop on Multiple Classifier Systems
                  (LNCS 2364)},
  publisher =    {Springer},
  month =    {June},
  pages =    {137-148},
  year =     2002,
  address =  {Calgiari, Italy},
  url =          "http://link.springer.de/link/service/series/0558/tocs/t2364.htm"
}

@TECHREPORT{poggio02bagging,
  author =   {Tomaso Poggio and Ryan Rifkin and Sayan Mukherjee
                  and Alex Rakhlin},
  title =    {Bagging Regularizes},
  institution =  {MIT AI Lab},
  year =     2002,
  number =   {AI Memo 2002-003, CBCL Memo 214},
}

@INPROCEEDINGS{raudys02multiple,
  author =   {Sarunas Raudys},
  title =    {Multiple Classification Systems in the Context of
                  Feature Extraction and Selection},
  booktitle =    {Proc. Int. Workshop on Multiple Classifier Systems
                  (LNCS 2364)},
  publisher =    {Springer},
  month =    {June},
  pages =    {27-41},
  year =     2002,
  address =  {Calgiari, Italy},
  url =          "http://link.springer.de/link/service/series/0558/tocs/t2364.htm"
}

@INPROCEEDINGS{roli02analysis,
  author =   {Fabio Roli and Giorgio Fumera},
  title =    {Analysis of Linear and Order Statistics Combiners
                  for Fusion of Imbalanced Classifiers},
  booktitle =    {Proc. Int. Workshop on Multiple Classifier Systems
                  (LNCS 2364)},
  publisher =    {Springer},
  month =    {June},
  pages =    {252-261},
  year =     2002,
  address =  {Calgiari, Italy},
  url =          "http://link.springer.de/link/service/series/0558/tocs/t2364.htm"
}

@PROCEEDINGS{mcs2002,
  title =    {Lecture Notes in Computer Science : Third
                  International Workshop on Multiple Classifier
                  Systems},
  year =     2002,
  editor =   {Fabio Roli and Josef Kittler},
  volume =   2364,
  address =  {Cagliari, Italy},
  month =    {June},
}

@INPROCEEDINGS{roli02identity,
  author =   {Fabio Roli and Josef Kittler and Giorgio Fumera and
                  Daniele Muntoni},
  title =    {An Experimental Comparison of Classifier Fusion
                  Rules for Multimodal Personal Identity Verification
                  Systems},
  booktitle =    {Proc. Int. Workshop on Multiple Classifier Systems
                  (LNCS 2364)},
  publisher =    {Springer},
  month =    {June},
  pages =    {325-336},
  year =     2002,
  address =  {Calgiari, Italy},
  url =          "http://link.springer.de/link/service/series/0558/tocs/t2364.htm"
}

@INPROCEEDINGS{roli02experimental,
  author =   {Fabio Roli and Sarunas Raudys and Gian Luca
                  Marcialis},
  title =    {An Experimental Comparison of Fixed and Trained
                  Fusion Rules for Crisp Classifier Outputs},
  booktitle =    {Proc. Int. Workshop on Multiple Classifier Systems
                  (LNCS 2364)},
  publisher =    {Springer},
  month =    {June},
  pages =    {232-241},
  year =     2002,
  address =  {Calgiari, Italy},
  url =          "http://link.springer.de/link/service/series/0558/tocs/t2364.htm"
}

@INPROCEEDINGS{ruta02new,
  author =   {Dymitr Ruta and Bogdan Gabrys},
  title =    {New Measure of Classifier Dependency in Multiple
                  Classifier Systems},
  booktitle =    {Proc. Int. Workshop on Multiple Classifier Systems
                  (LNCS 2364)},
  publisher =    {Springer},
  month =    {June},
  pages =    {127-136},
  year =     2002,
  address =  {Calgiari, Italy},
  url =          "http://link.springer.de/link/service/series/0558/tocs/t2364.htm"
}

@INPROCEEDINGS{schettini02content-based,
  author =   {Raimondo Schettini and C. Brambilla and C. Cusano},
  title =    {Content-Based Classification of Digital Photos},
  booktitle =    {Proc. Int. Workshop on Multiple Classifier Systems
                  (LNCS 2364)},
  publisher =    {Springer},
  month =    {June},
  pages =    {272-281},
  year =     2002,
  address =  {Calgiari, Italy},
  url =          "http://link.springer.de/link/service/series/0558/tocs/t2364.htm"
}

@INPROCEEDINGS{sharkeytypes02,
  author =   {Amanda J. C. Sharkey},
  title =    {Types of Multinet System},
  booktitle =    {Proc. Int. Workshop on Multiple Classifier Systems
                  (LNCS 2364)},
  publisher =    {Springer},
  month =    {June},
  pages =    {108-117},
  year =     2002,
  address =  {Calgiari, Italy},
  url =          "http://link.springer.de/link/service/series/0558/tocs/t2364.htm"
}

@ARTICLE{shippkuncheva02,
  author =   {C.A. Shipp and L.I. Kuncheva},
  title =    {Relationships between combination methods and
                  measures of diversity in combining classifiers},
  journal =  {Information Fusion},
  year =     2002,
  number =   3,
  pages =    {135-148},
}

@INPROCEEDINGS{sirlantzis02trainable,
  author =   {Konstantinos Sirlantzis and Sanaul Hoque and Michael
                  C. Fairhurst},
  title =    {Trainable Multiple Classifier Schemes for
                  Handwritten Character Recognition},
  booktitle =    {Proc. Int. Workshop on Multiple Classifier Systems
                  (LNCS 2364)},
  publisher =    {Springer},
  month =    {June},
  pages =    {169-178},
  year =     2002,
  address =  {Calgiari, Italy},
  url =          "http://link.springer.de/link/service/series/0558/tocs/t2364.htm"
}

@article{skurichina2002bba,
  title={{Bagging, Boosting and the Random Subspace Method for Linear Classifiers}},
  author={Skurichina, M.W. and Duin, R.P.W.W.},
  journal={Pattern Analysis \& Applications},
  volume={5},
  number={2},
  pages={121-135},
  year={2002},
  publisher={Springer}
}

@INPROCEEDINGS{skurichina02:nmc,
  author =   {Marina Skurichina and Ludmila Kuncheva and Robert
                  P. W. Duin},
  title =    {Bagging and Boosting for the Nearest Mean
                  Classifier: Effects of Sample Size on Diversity and
                  Accuracy},
  booktitle =    {Proc. Int. Workshop on Multiple Classifier Systems
                  (LNCS 2364)},
  publisher =    {Springer},
  month =    {June},
  pages =    {62-71},
  year =     2002,
  address =  {Calgiari, Italy},
  url =          "http://link.springer.de/link/service/series/0558/tocs/t2364.htm"
}

@ARTICLE{tetko02associative,
  author =   {Tetko, I. V.},
  title =    {Associative neural network},
  journal =  {Neural Processing Letters},
  volume =   16,
  number =   2,
  pages =    {187-199},
  abstract =     {An associative neural network (ASNN) is a
                  combination of an ensemble of the feed-forward
                  neural networks and the K-nearest neighbor
                  technique. The introduced network uses correlation
                  between ensemble responses as a measure of distance
                  among the analyzed cases for the nearest neighbor
                  technique and provides an improved prediction by the
                  bias correction of the neural network ensemble both
                  for function approximation and
                  classification. Actually, the proposed method
                  corrects a bias of a global model for a considered
                  data case by analyzing the biases of its nearest
                  neighbors determined in the space of calculated
                  models. An associative neural network has a memory
                  that can coincide with the training set. If new data
                  become available the network can provide a
                  reasonable approximation of such data without a need
                  to retrain the neural network ensemble. Applications
                  of ASNN for prediction of lipophilicity of chemical
                  compounds and classification of UCI letter and
                  satellite data set are presented. The developed
                  algorithm is available on-line at
                  http://www.virtuallaboratory.org/lab/asnn.},
  year =     2002,
  url =      {http://cogprints.ecs.soton.ac.uk/archive/00001441/},
}

@ARTICLE{tetko02associative2,
  author =   {Tetko, I. V.},
  title =    {Neural network studies. 4. Introduction to
                  associative neural networks},
  journal =  {J Chem Inf Comput Sci},
  volume =   42,
  number =   3,
  pages =    {717-28.},
  keywords =     {*Neural Networks (Computer) Support,
                  Non-U.S. Gov\'t},
  abstract =     {Associative neural network (ASNN) represents a
                  combination of an ensemble of feed-forward neural
                  networks and the k-nearest neighbor technique. This
                  method uses the correlation between ensemble
                  response,, as a measure of distance amid the
                  analyzed cases for the nearest neighbor
                  technique. This provides an improved prediction by
                  the bias correction of the neural network
                  ensemble. An associative neural network has a memory
                  that can coincide with the training set. If new data
                  becomes available, the network further improves its
                  predictive ability and provides a reasonable
                  approximation of the unknown function without a need
                  to retrain the neural network ensemble, This feature
                  of the method dramatically improves its predictive
                  ability over traditional neural networks and
                  k-nearest neighbor techniques, as demonstrated using
                  several artificial data sets and a program to
                  predict lipophilicity of chemical compounds. Another
                  important feature of ASNN is the possibility to
                  interpret neural network results by analysis of
                  correlations between data cases in the space of
                  models, It is shown that analysis of such
                  correlations makes it possible to provide
                  \"property-targeted\" clustering of data. The
                  possible applications and importance of ASNN in drug
                  design and medicinal and combinatorial chemistry are
                  discussed. The method is available on-line at
                  http://www.vcclab.org/lab/asnn.},
  year =     2002,
}

@ARTICLE{tetko02asnnapp,
  author =   {Tetko, I. V. and Tanchuk, V. Y.},
  title =    {Application of associative neural networks for
                  prediction of lipophilicity in ALOGPS 2.1 program},
  journal =  {J Chem Inf Comput Sci},
  volume =   42,
  number =   5,
  pages =    {1136-45.},
  abstract =     {This article provides a systematic study of several
                  important parameters of the Associative Neural
                  Network (ASNN), such as the number of networks in
                  the ensemble, distance measures, neighbor functions,
                  selection of smoothing parameters, and strategies
                  for the user-training feature of the algorithm. The
                  performance of the different methods is assessed
                  with several training/test sets used to predict
                  lipophilicity of chemical compounds. The Spearman
                  rank-order correlation coefficient and Parzen-window
                  regression methods provide the best performance of
                  the algorithm. If additional user data is available,
                  an improved prediction of lipophilicity of chemicals
                  up to 2-5 times can be calculated when the
                  appropriate smoothing parameters for the neural
                  network are selected. The detected best combinations
                  of parameters and strategies are implemented in the
                  ALOGPS 2.1 program that is publicly available at
                  http://www.vcclab.org/lab/alogps.},
  year =     2002,
}

@ARTICLE{tumerghosh00robust,
  author =   {K Tumer and J Ghosh},
  title =    {Robust Combining of Disparate Classifiers through
                  Order Statistics},
  journal =  {To appear in Pattern Analysis and Applications,
                  Special issue on Fusion of Multiple Classifiers},
  year =     2002,
  number =   2,
  volume =   5,
  url =      {http://citeseer.nj.nec.com/592387.html},
}

@INPROCEEDINGS{valentini02svm,
  author =   {Giorgio Valentini and Thomas G. Dietterich},
  title =    {Bias-Variance Analysis and Ensembles of {SVM}},
  booktitle =    {Proc. Int. Workshop on Multiple Classifier Systems
                  (LNCS 2364)},
  publisher =    {Springer},
  month =    {June},
  pages =    {222-231},
  year =     2002,
  address =  {Calgiari, Italy},
  url =          "http://link.springer.de/link/service/series/0558/tocs/t2364.htm"
}

@INPROCEEDINGS{wersing02a,
  author =   {H. Wersing and E. K{\"o}rner},
  title =    {Unsupervised Learning of Combination Features for
                  Hierarchical Recognition Models},
  booktitle =    {Int. Conf. Artif. Neur. Netw. ICANN},
  year =     2002,
  note =     {accepted},
}

@ARTICLE{wersingkorner02,
  author =   {Heiko Wersing and Edgar Korner},
  title =    {Learning Optimized Features for Hierarchical Models
                  of Invariant Object Recognition},
  journal =  {Neural Computation (to appear)},
  year =     2002,
}

@INPROCEEDINGS{windeatt02boosted,
  author =   {Terry Windeatt and Gholamreza Ardeshir},
  title =    {Boosted Tree Ensembles for Solving Multiclass
                  Problems},
  booktitle =    {Proc. Int. Workshop on Multiple Classifier Systems
                  (LNCS 2364)},
  publisher =    {Springer},
  month =    {June},
  pages =    {42-51},
  year =     2002,
  address =  {Calgiari, Italy},
  url =          "http://link.springer.de/link/service/series/0558/tocs/t2364.htm"
}

@INPROCEEDINGS{windridge02on,
  author =   {David Windridge and Josef Kittler},
  title =    {On the General Application of the Tomographic
                  Classifier Fusion Methodology},
  booktitle =    {Proc. Int. Workshop on Multiple Classifier Systems
                  (LNCS 2364)},
  publisher =    {Springer},
  month =    {June},
  pages =    {149-158},
  year =     2002,
  address =  {Calgiari, Italy},
  url =          "http://link.springer.de/link/service/series/0558/tocs/t2364.htm"
}

@INPROCEEDINGS{yang02multistage,
  author =   {Shuang Yang and Antony Browne and Phil D. Picton},
  title =    {Multistage Neural Network Ensembles},
  booktitle =    {Proc. Int. Workshop on Multiple Classifier Systems
                  (LNCS 2364)},
  publisher =    {Springer},
  month =    {June},
  pages =    {91-97},
  year =     2002,
  address =  {Calgiari, Italy},
  url =          "http://link.springer.de/link/service/series/0558/tocs/t2364.htm"
}

@ARTICLE{zhouwutang02:ensembling,
  author =   {Z.-H. Zhou, J. Wu, and W. Tang},
  title =    {Ensembling neural networks: Many could be better
                  than all},
  journal =  {Artificial Intelligence},
  volume =   137,
  number =   {1-2},
  pages =    {239-263},
  year =     2002,
}

@INPROCEEDINGS{zhou02:ensembling,
  author =   {Z.H. Zhou and J.Wu and W.Tang and Z.Q. Chen},
  title =    "Selectively ensembling neural classifiers" ,
  booktitle =    {International Joint Conference on Neural Networks},
  volume =   2,
  page =     {1411-1415},
  year =     2002,
}

@INPROCEEDINGS{zhu02support,
  author =   {Ji Zhu and Trevor Hastie},
  title =    {Support Vector Machines, Kernel Logistic Regression
                  and Boosting},
  booktitle =    {Proc. Int. Workshop on Multiple Classifier Systems
                  (LNCS 2364)},
  publisher =    {Springer},
  month =    {June},
  pages =    {16-26},
  year =     2002,
  address =  {Calgiari, Italy},
  url =          "http://link.springer.de/link/service/series/0558/tocs/t2364.htm"
}

@ARTICLE{atikorale03hong,
  author =   {A. S. Atukorale, T. Downs and P. N. Suganthan},
  title =    {Boosting HONG Networks},
  journal =  {Neurocomputing},
  year =     2003,
  volume =   51,
  pages =    {75-86},
  month =    {April},
}

@INPROCEEDINGS{ahmad03combining,
  author =   {Khurshid Ahmad and Matthew Casey and Bogdan Vrusias
                  and Panagiotis Saragiotis},
  title =    {Combining Multiple Modes of Information Using
                  Unsupervised Neural Classifiers},
  booktitle =    {Proc. Int. Workshop on Multiple Classifier Systems
                  (LNCS 2709)},
  publisher =    {Springer},
  month =    {June},
  pages =    {236-245},
  year =     2003,
  address =  {Guildford, Surrey},
  url =          {http://link.springer.de/link/service/series/0558/tocs/t2709.htm},
}

@INPROCEEDINGS{aksela03comparison,
  author =   {Matti Aksela},
  title =    {Comparison of Classifier Selection Methods for
                  Improving Committee Performance},
  booktitle =    {Proc. Int. Workshop on Multiple Classifier Systems
                  (LNCS 2709)},
  publisher =    {Springer},
  month =    {June},
  pages =    {84-93},
  year =     2003,
  address =  {Guildford, Surrey},
  url =           "http://link.springer.de/link/service/series/0558/tocs/t2709.htm"
}

@INPROCEEDINGS{alam03solving,
  author =   {Hassan Alam and Ahmad Fuad Rezaur Rahman and Yulia
                  Tarnikova},
  title =    {Solving Problems Two at a Time: Classification of
                  Web Pages Using a Generic Pair-Wise Multiple
                  Classifier System},
  booktitle =    {Proc. Int. Workshop on Multiple Classifier Systems
                  (LNCS 2709)},
  publisher =    {Springer},
  month =    {June},
  pages =    {385-394},
  year =     2003,
  address =  {Guildford, Surrey},
  url =            "http://link.springer.de/link/service/series/0558/tocs/t2709.htm"
}

@INPROCEEDINGS{arenas-garcia03beneficial,
  author =   {J. Arenas-Garcia and An?bal R. Figueiras-Vidal and
                  Amanda J. C. Sharkey},
  title =    {The Beneficial Effects of Using Multi-net Systems
                  That Focus on Hard Patterns},
  booktitle =    {Proc. Int. Workshop on Multiple Classifier Systems
                  (LNCS 2709)},
  publisher =    {Springer},
  month =    {June},
  pages =    {45-54},
  year =     2003,
  address =  {Guildford, Surrey},
  url =          "http://link.springer.de/link/service/series/0558/tocs/t2709.htm"
}

@INPROCEEDINGS{asdornwised03automatic,
  author =   {Widhyakorn Asdornwised and Somchai Jitapunkul},
  title =    {Automatic Target Recognition Using Multiple
                  Description Coding Models for Multiple Classifier
                  Systems},
  booktitle =    {Proc. Int. Workshop on Multiple Classifier Systems
                  (LNCS 2709)},
  publisher =    {Springer},
  month =    {June},
  pages =    {336-345},
  year =     2003,
  address =  {Guildford, Surrey},
  url =          "http://link.springer.de/link/service/series/0558/tocs/t2709.htm"
}

@INPROCEEDINGS{ayad03finding,
  author =   {Hanan Ayad and Mohamed S. Kamel},
  title =    {Finding Natural Clusters Using Multi-clusterer
                  Combiner Based on Shared Nearest Neighbors},
  booktitle =    {Proc. Int. Workshop on Multiple Classifier Systems
                  (LNCS 2709)},
  publisher =    {Springer},
  month =    {June},
  pages =    {166-175},
  year =     2003,
  address =  {Guildford, Surrey},
  url =          "http://link.springer.de/link/service/series/0558/tocs/t2709.htm"
}

@INPROCEEDINGS{banfield03new,
  author =   {Robert E. Banfield and Lawrence O. Hall and Kevin
                  W. Bowyer and W. Philip Kegelmeyer},
  title =    {A New Ensemble Diversity Measure Applied to Thinning
                  Ensembles},
  booktitle =    {Proc. Int. Workshop on Multiple Classifier Systems
                  (LNCS 2709)},
  publisher =    {Springer},
  month =    {June},
  pages =    {306-316},
  year =     2003,
  address =  {Guildford, Surrey},
  url =          "http://link.springer.de/link/service/series/0558/tocs/t2709.htm"
}

@INPROCEEDINGS{baykut03towards,
  author =   {Alper Baykut and Ayt?l Er?il},
  title =    {Towards Automated Classifier Combination for Pattern
                  Recognition},
  booktitle =    {Proc. Int. Workshop on Multiple Classifier Systems
                  (LNCS 2709)},
  publisher =    {Springer},
  month =    {June},
  pages =    {94-105},
  year =     2003,
  address =  {Guildford, Surrey},
  url =          "http://link.springer.de/link/service/series/0558/tocs/t2709.htm"
}

@INPROCEEDINGS{brownwyatt03ambiguity,
  author =   {G. Brown and J.L. Wyatt},
  title =    {The Use of the Ambiguity Decomposition in Neural
                  Network Ensemble Learning Methods},
  booktitle =    {20th {I}nternational {C}onference on {M}achine
                  {L}earning (ICML'03)},
  year =     2003,
  editor =   {Tom Fawcett and Nina Mishra},
  month =    {August},
  address =  {Washington DC, USA},
}

@INPROCEEDINGS{brownwyatt03mcs,
  author =   {Gavin Brown and Jeremy L. Wyatt},
  title =    {Negative Correlation Learning and the Ambiguity
                  Family of Ensemble Methods},
  booktitle =    {Proc. Int. Workshop on Multiple Classifier Systems
                  (LNCS 2709)},
  publisher =    {Springer},
  month =    {June},
  pages =    {266-275},
  year =     2003,
  address =  {Guildford, Surrey},
  url =          "http://link.springer.de/link/service/series/0558/tocs/t2709.htm"
}

@INPROCEEDINGS{cheung03radial,
  author =   {Y.M. Cheung and R.B. Huang},
  title =    {An Advance on Divide-and-Conquer Based Radial Basis
                  Function Networks},
  booktitle =    {Proceedings of Fourth International Conference on
                  Intelligent Data Engineering and Automated Learning},
  address =  {Hong Kong},
  month =    {March},
  year =     2003,
}

@INPROCEEDINGS{christensen03ensemble,
  author =   {Stefan W. Christensen},
  title =    {Ensemble Construction via Designed Output
                  Distortion},
  booktitle =    {Proc. Int. Workshop on Multiple Classifier Systems
                  (LNCS 2709)},
  publisher =    {Springer},
  month =    {June},
  pages =    {286-295},
  year =     2003,
  address =  {Guildford, Surrey},
  url =          "http://link.springer.de/link/service/series/0558/tocs/t2709.htm"
}

@INPROCEEDINGS{cohen03study,
  author =   {Shimon Cohen and Nathan Intrator},
  title =    {A Study of Ensemble of Hybrid Networks with Strong
                  Regularization},
  booktitle =    {Proc. Int. Workshop on Multiple Classifier Systems
                  (LNCS 2709)},
  publisher =    {Springer},
  month =    {June},
  pages =    {227-235},
  year =     2003,
  address =  {Guildford, Surrey},
  url =          "http://link.springer.de/link/service/series/0558/tocs/t2709.htm"
}

@INPROCEEDINGS{cutzu03polychotomous,
  author =   {Florin Cutzu},
  title =    {Polychotomous Classification with Pairwise
                  Classifiers: A New Voting Principle},
  booktitle =    {Proc. Int. Workshop on Multiple Classifier Systems
                  (LNCS 2709)},
  publisher =    {Springer},
  month =    {June},
  pages =    {115-124},
  year =     2003,
  address =  {Guildford, Surrey},
  url =          "http://link.springer.de/link/service/series/0558/tocs/t2709.htm"
}

@INPROCEEDINGS{duan03multi-category,
  author =   {Kaibo Duan and S. Sathiya Keerthi and Wei Chu and
                  Shirish Krishnaj Shevade and Aun Neow Poo},
  title =    {Multi-category Classification by Soft-Max
                  Combination of Binary Classifiers},
  booktitle =    {Proc. Int. Workshop on Multiple Classifier Systems
                  (LNCS 2709)},
  publisher =    {Springer},
  month =    {June},
  pages =    {125-134},
  year =     2003,
  address =  {Guildford, Surrey},
  url =          "http://link.springer.de/link/service/series/0558/tocs/t2709.htm"
}

@INPROCEEDINGS{duchitert03undemocratic,
  author =   {Wlodzislaw Duch and Lukasz Itert},
  title =    {Committees of Undemocratic Competent Models},
  booktitle =    {International Conference on Artificial Neural
                  Networks (ICANN) and International Conference on
                  Neural Information Processing (ICONIP)},
  year =     2003,
  address =  {Istanbul, Turkey},
  month =    June,
  pages =    {33--36},
  url =          "http://www.phys.uni.torun.pl/publications/kmk/03-C-Ensambles-s.html"
}

@INPROCEEDINGS{estruch03beam,
  author =   {Vicent Estruch and Cesar Ferri and Jose
                  Hernandez-Orallo and M. Jose Ramirez-Quintana},
  title =    {Beam Search Extraction and Forgetting Strategies on
                  Shared Ensembles},
  booktitle =    {Proc. Int. Workshop on Multiple Classifier Systems
                  (LNCS 2709)},
  publisher =    {Springer},
  month =    {June},
  pages =    {206-216},
  year =     2003,
  address =  {Guildford, Surrey},
  url =          "http://link.springer.de/link/service/series/0558/tocs/t2709.htm"
}

@TECHREPORT{friedman03isle,
  author =   {J.H. Friedman and B. Popescu},
  title =    {Importance Sampling Learning Ensembles},
  institution =  {Stanford University},
  year =     2003,
  month =    {September},
  url =      {http://www-stat.stanford.edu/\~{}jhf/ftp/isle.pdf},
}

@INPROCEEDINGS{fumera03linear,
  author =   {G. Fumera and F. Roli},
  title =    {Linear Combiners for Classifier Fusion: Some
                  Theoretical and Experimental Results},
  booktitle =    {Proc. Int. Workshop on Multiple Classifier Systems
                  (LNCS 2709)},
  publisher =    {Springer},
  month =    {June},
  pages =    {74-83},
  year =     2003,
  address =  {Guildford, Surrey},
  url =          "http://link.springer.de/link/service/series/0558/tocs/t2709.htm"
}

@INPROCEEDINGS{giacinto03modular,
  author =   {Giorgio Giacinto and Fabio Roli and Luca Didaci},
  title =    {A Modular Multiple Classifier System for the
                  Detection of Intrusions in Computer Networks},
  booktitle =    {Proc. Int. Workshop on Multiple Classifier Systems
                  (LNCS 2709)},
  publisher =    {Springer},
  month =    {June},
  pages =    {346-355},
  year =     2003,
  address =  {Guildford, Surrey},
  url =          "http://link.springer.de/link/service/series/0558/tocs/t2709.htm"
}

@INPROCEEDINGS{gunter03new,
  author =   {Simon Gunter and Horst Bunke},
  title =    {New Boosting Algorithms for Classification Problems
                  with Large Number of Classes Applied to a
                  Handwritten Word Recognition Task},
  booktitle =    {Proc. Int. Workshop on Multiple Classifier Systems
                  (LNCS 2709)},
  publisher =    {Springer},
  month =    {June},
  pages =    {326-335},
  year =     2003,
  address =  {Guildford, Surrey},
  url =          "http://link.springer.de/link/service/series/0558/tocs/t2709.htm"
}

@INPROCEEDINGS{inoue03improving,
  author =   {Hirotaka Inoue and Hiroyuki Narihisa},
  title =    {Improving Performance of a Multiple Classifier
                  System Using Self-generating Neural Networks},
  booktitle =    {Proc. Int. Workshop on Multiple Classifier Systems
                  (LNCS 2709)},
  publisher =    {Springer},
  month =    {June},
  pages =    {256-265},
  year =     2003,
  address =  {Guildford, Surrey},
  url =          "http://link.springer.de/link/service/series/0558/tocs/t2709.htm"
}

@ARTICLE{islam03constructive,
  author =   {Md. M. Islam and X. Yao and K. Murase},
  number =   4,
  year =     2003 ,
  journal =  {IEEE Transactions on Neural Networks},
  title =    {A constructive algorithm for training cooperative
                  neural network ensembles},
  month =    {July},
  pages =    {820--834},
  volume =   14,
}

@ARTICLE{james03bv,
  author =   {Gareth James},
  year =     2003,
  title =    {Variance and Bias for General Loss Functions},
  journal =  {Machine Learning},
  volume =   51,
  pages =    {115--135},
}

@INPROCEEDINGS{jaser03building,
  author =   {Edward Jaser and Josef Kittler and William
                  J. Christmas},
  title =    {Building Classifier Ensembles for Automatic Sports
                  Classification},
  booktitle =    {Proc. Int. Workshop on Multiple Classifier Systems
                  (LNCS 2709)},
  publisher =    {Springer},
  month =    {June},
  pages =    {366-374},
  year =     2003,
  address =  {Guildford, Surrey},
  url =          "http://link.springer.de/link/service/series/0558/tocs/t2709.htm"
}

@INPROCEEDINGS{jin03regularizer,
  author =   {Rong Jin and Yan Liu and Luo Si and Jaime Carbonell
                  and Alexander Hauptmann},
  title =    {A New Boosting Algorithm using Input-Dependent
                  Regularizer},
  booktitle =    {20th International Conference on Machine Learning},
  year =     2003,
}

@INPROCEEDINGS{kamel03data,
  author =   {Mohamed S. Kamel and Nayer M. Wanas},
  title =    {Data Dependence in Combining Classifiers},
  booktitle =    {Proc. Int. Workshop on Multiple Classifier Systems
                  (LNCS 2709)},
  publisher =    {Springer},
  month =    {June},
  pages =    {1-14},
  year =     2003,
  address =  {Guildford, Surrey},
  url =          "http://link.springer.de/link/service/series/0558/tocs/t2709.htm"
}

@INPROCEEDINGS{kittler03serial,
  author =   {Josef Kittler and Alireza Ahmadyfard and David
                  Windridge},
  title =    {Serial Multiple Classifier Systems Exploiting a
                  Coarse to Fine Output Coding},
  booktitle =    {Proc. Int. Workshop on Multiple Classifier Systems
                  (LNCS 2709)},
  publisher =    {Springer},
  month =    {June},
  pages =    {106-114},
  year =     2003,
  address =  {Guildford, Surrey},
  url =          "http://link.springer.de/link/service/series/0558/tocs/t2709.htm"
}

@INPROCEEDINGS{ko03binary,
  author =   {Jaepil Ko and Hyeran Byun},
  title =    {Binary Classifier Fusion Based on the Basic
                  Decomposition Methods},
  booktitle =    {Proc. Int. Workshop on Multiple Classifier Systems
                  (LNCS 2709)},
  publisher =    {Springer},
  month =    {June},
  pages =    {146-155},
  year =     2003,
  address =  {Guildford, Surrey},
  url =          "http://link.springer.de/link/service/series/0558/tocs/t2709.htm"
}

@ARTICLE{kuncheva03limits,
  author =   {L. Kuncheva and C. Whitaker and C. Shipp and
                  R. Duin},
  title =    {Limits on the majority vote accuracy in classifier
                  fusion},
  journal =  {Pattern {A}nalysis and {A}pplications},
  year =     2003,
  month =    {April},
  volume =   6,
  number =   1,
  pages =    {22--31},
}

@INPROCEEDINGS{kuncheva03elusive,
  author =   {Kuncheva, L.I.},
  title =    {That {E}lusive {D}iversity in {C}lassifier
                  {E}nsembles},
  booktitle =    {First Iberian Conference on Pattern Recognition and
                  Image Analysis (IbPRIA), available as LNCS volume
                  2652},
  pages =    {1126--1138},
  year =     2003,
}

@ARTICLE{kunchevawhitaker03,
  author =   {L.I. Kuncheva and C. Whitaker},
  title =    {Measures of Diversity in Classifier Ensembles},
  journal =  {Machine Learning},
  year =     2003,
  number =   51,
  pages =    {181--207},
}

@INPROCEEDINGS{kuncheva03error,
  author =   {Ludmila Kuncheva},
  title =    {Error Bounds for Aggressive and Conservative
                  AdaBoost},
  booktitle =    {Proc. Int. Workshop on Multiple Classifier Systems
                  (LNCS 2709)},
  publisher =    {Springer},
  month =    {June},
  pages =    {25-34},
  year =     2003,
  address =  {Guildford, Surrey},
  url =          "http://link.springer.de/link/service/series/0558/tocs/t2709.htm"
}

@INPROCEEDINGS{langdon:2003:evowks,
  title =    {Comparison of AdaBoost and Genetic Programming for
                  combining Neural Networks for Drug Discovery},
  author =   {W. B. Langdon and S. J. Barrett and B. F. Buxton},
  booktitle =    {Applications of Evolutionary Computing,
                  EvoWorkshops2003: Evo{BIO}, Evo{COP}, Evo{IASP},
                  Evo{MUSART}, Evo{ROB}, Evo{STIM}},
  editor =   {G\"unther R.~Raidl and Stefano Cagnoni and Juan
                  Jes\'us Romero Cardalda and David W.~Corne and Jens
                  Gottlieb and Agn\`es Guillot and Emma Hart and Colin
                  G.~Johnson and Elena Marchiori and Jean-Arcady Meyer
                  and Martin Middendorf},
  volume =   2611,
  series =   {LNCS},
  pages =    {87--98},
  address =  {University of Essex, UK},
  publisher =    {Springer-Verlag},
  publisher_address ={Berlin},
  month =    {14-16 April},
  organisation = {EvoNet},
  year =     2003,
  keywords =     {genetic algorithms, genetic programming, adaboost,
                  drug design, Receiver Operating Characteristics
                  (ROC), ensemble of classifiers, data fusion,
                  artificial neural networks, clementine, high through
                  put screening (HTS)},
  size =     {12 pages},
  notes =    {EvoWorkshops2003},
}

@INPROCEEDINGS{lewitt03ensemble,
  author =   {Michael Lewitt and Robi Polikar},
  title =    {An Ensemble Approach for Data Fusion with Learn++},
  booktitle =    {Proc. Int. Workshop on Multiple Classifier Systems
                  (LNCS 2709)},
  publisher =    {Springer},
  month =    {June},
  pages =    {176-185},
  year =     2003,
  address =  {Guildford, Surrey},
  url =          "http://link.springer.de/link/service/series/0558/tocs/t2709.htm"
}

@INPROCEEDINGS{luttrell03markov,
  author =   {S. P. Luttrell},
  title =    {A Markov Chain Approach to Multiple Classifier
                  Fusion},
  booktitle =    {Proc. Int. Workshop on Multiple Classifier Systems
                  (LNCS 2709)},
  publisher =    {Springer},
  month =    {June},
  pages =    {217-226},
  year =     2003,
  address =  {Guildford, Surrey},
  url =          "http://link.springer.de/link/service/series/0558/tocs/t2709.htm"
}

@INPROCEEDINGS{magee03sequential,
  author =   {Derek Magee},
  title =    {A Sequential Scheduling Approach to Combining
                  Multiple Object Classifiers Using Cross-Entropy},
  booktitle =    {Proc. Int. Workshop on Multiple Classifier Systems
                  (LNCS 2709)},
  publisher =    {Springer},
  month =    {June},
  pages =    {135-145},
  year =     2003,
  address =  {Guildford, Surrey},
  url =          "http://link.springer.de/link/service/series/0558/tocs/t2709.htm"
}

@ARTICLE{malzahn03approximate,
  author =   {Dorthe Malzahn and Manfred Opper},
  title =    {An Approximate Analytical Approach to Resampling
                  Averages},
  journal =  {Journal of Machine Learning Research},
  year =     2003,
  volume =   4,
  pages =    {1151--1173},
  month =    {December},
  url =      {http://www.jmlr.org},
}

@INPROCEEDINGS{mcdonald03empirical,
  author =   {Ross A. McDonald and David J. Hand and Idris
                  A. Eckley},
  title =    {An Empirical Comparison of Three Boosting Algorithms
                  on Real Data Sets with Artificial Class Noise},
  booktitle =    {Proc. Int. Workshop on Multiple Classifier Systems
                  (LNCS 2709)},
  publisher =    {Springer},
  month =    {June},
  pages =    {35-44},
  year =     2003,
  address =  {Guildford, Surrey},
  url =          "http://link.springer.de/link/service/series/0558/tocs/t2709.htm"
}

@INPROCEEDINGS{melville03decorate,
  author =   {Prem Melville and Ray Mooney},
  title =    {Constructing Diverse Classifier Ensembles Using
                  Artificial Training Examples},
  booktitle =    {Proceedings of the Eighteenth International Joint
                  Conference on Artificial Intelligence},
  pages =    {505--510},
  year =     2003,
  address =  {Mexico},
  month =    {August},
}


@INPROCEEDINGS{oza03boosting,
  author =   {Nikunj C. Oza},
  title =    {Boosting with Averaged Weight Vectors},
  booktitle =    {Proc. Int. Workshop on Multiple Classifier Systems
                  (LNCS 2709)},
  publisher =    {Springer},
  month =    {June},
  pages =    {15-24},
  year =     2003,
  address =  {Guildford, Surrey},
}

@INPROCEEDINGS{oza03classification,
  author =   {Nikunj C. Oza and Kagan Tumer and Irem Y. Tumer and
                  Edward M. Huff},
  title =    {Classification of Aircraft Maneuvers for Fault
                  Detection},
  booktitle =    {Proc. Int. Workshop on Multiple Classifier Systems
                  (LNCS 2709)},
  publisher =    {Springer},
  month =    {June},
  pages =    {375-384},
  year =     2003,
  address =  {Guildford, Surrey},
  url =          "http://link.springer.de/link/service/series/0558/tocs/t2709.htm"
}

@INPROCEEDINGS{raudys03behavior,
  author =   {Sarunas Raudys and Fabio Roli},
  title =    {The Behavior Knowledge Space Fusion Method: Analysis
                  of Generalization Error and Strategies for
                  Performance Improvement},
  booktitle =    {Proc. Int. Workshop on Multiple Classifier Systems
                  (LNCS 2709)},
  publisher =    {Springer},
  month =    {June},
  pages =    {55-64},
  year =     2003,
  address =  {Guildford, Surrey},
  url =          "http://link.springer.de/link/service/series/0558/tocs/t2709.htm"
}

@INPROCEEDINGS{raudys03reducing,
  author =   {Sarunas Raudys and Ray L. Somorjai and Richard
                  Baumgartner},
  title =    {Reducing the Overconfidence of Base Classifiers when
                  Combining Their Decisions},
  booktitle =    {Proc. Int. Workshop on Multiple Classifier Systems
                  (LNCS 2709)},
  publisher =    {Springer},
  month =    {June},
  pages =    {65-73},
  year =     2003,
  address =  {Guildford, Surrey},
  url =          "http://link.springer.de/link/service/series/0558/tocs/t2709.htm"
}

@ARTICLE{ruta03sets,
  author =   {Dymitr Ruta and Bogdan Gabrys},
  title =    {Set Analysis of Coincident Errors and Its
                  Applications for Combining Classifiers},
  journal =  {Combinatorial Optimisation},
  volume =   13,
  publisher =    {Kluwer Academic},
  year =     2003,
}

@INPROCEEDINGS{santos03neural,
  author =   {Rafael Valle dos Santos and Marley Vellasco and
                  Fredy Artola and Sergio da Fontoura},
  title =    {Neural Net Ensembles for Lithology Recognition},
  booktitle =    {Proc. Int. Workshop on Multiple Classifier Systems
                  (LNCS 2709)},
  publisher =    {Springer},
  month =    {June},
  pages =    {246-255},
  year =     2003,
  address =  {Guildford, Surrey},
  url =          "http://link.springer.de/link/service/series/0558/tocs/t2709.htm"
}

@INPROCEEDINGS{shih03bundling,
  author =   {Lawrence Shih and Jason Rennie and Yu-Han Chang and
                  David Karger},
  title =    {Text Bundling : Statistics Based Data Reduction},
  booktitle =    {20th {I}nternational {C}onference on {M}achine
                  {L}earning (ICML'03)},
  year =     2003,
  editor =   {Tom Fawcett and Nina Mishra},
  month =    {August},
  address =  {Washington DC, USA},
}

@INPROCEEDINGS{sirlantzis03input,
  author =   {Konstantinos Sirlantzis and Sanaul Hoque and Michael
                  C. Fairhurst},
  title =    {Input Space Transformations for Multi-classifier
                  Systems Based on n-tuple Classifiers with
                  Application to Handwriting Recognition},
  booktitle =    {Proc. Int. Workshop on Multiple Classifier Systems
                  (LNCS 2709)},
  publisher =    {Springer},
  month =    {June},
  pages =    {356-365},
  year =     2003,
  address =  {Guildford, Surrey},
  url =          "http://link.springer.de/link/service/series/0558/tocs/t2709.htm"
}

@INPROCEEDINGS{stephenson03compiler,
  author =   {Mark Stephenson and Una-May O'Reilly and Martin
                  C. Martin and Saman P. Amarasinghe},
  title =    {Genetic Programming Applied to Compiler Heuristic
                  Optimization.},
  booktitle =    {EuroGP},
  year =     2003,
  pages =    {238-253},
  ee =
                  {http://link.springer.de/link/service/series/0558/bibs/2610/26100238.htm},
}

@INPROCEEDINGS{tapia03good,
  author =   {Elizabeth Tapia and Jose Carlos Gonzalez and
                  L. Javier Garcia-Villalba},
  title =    {Good Error Correcting Output Codes for Adaptive
                  Multiclass Learning},
  booktitle =    {Proc. Int. Workshop on Multiple Classifier Systems
                  (LNCS 2709)},
  publisher =    {Springer},
  month =    {June},
  pages =    {156-165},
  year =     2003,
  address =  {Guildford, Surrey},
  url =          "http://link.springer.de/link/service/series/0558/tocs/t2709.htm"
}

@INPROCEEDINGS{valentini03baggedsvm,
  author =   {Giorgio Valentini and Thomas G. Dietterich},
  title =    {Low Bias Bagged Support Vector Machines},
  booktitle =    {20th {I}nternational {C}onference on {M}achine
                  {L}earning (ICML'03)},
  year =     2003,
  editor =   {Tom Fawcett and Nina Mishra},
  month =    {August},
  address =  {Washington DC, USA},
}

@INPROCEEDINGS{velek03accumulated-recognition-rate,
  author =   {Ondrej Velek and Stefan Juger and Masaki Nakagawa},
  title =    {Accumulated-Recognition-Rate Normalization for
                  Combining Multiple On/Off-Line Japanese Character
                  Classifiers Tested on a Large Database},
  booktitle =    {Proc. Int. Workshop on Multiple Classifier Systems
                  (LNCS 2709)},
  publisher =    {Springer},
  month =    {June},
  pages =    {196-205},
  year =     2003,
  address =  {Guildford, Surrey},
  url =          "http://link.springer.de/link/service/series/0558/tocs/t2709.htm"
}

@INPROCEEDINGS{verbaeten03ensemble,
  author =   {Sofie Verbaeten and Anneleen Van Assche},
  title =    {Ensemble Methods for Noise Elimination in
                  Classification Problems},
  booktitle =    {Proc. Int. Workshop on Multiple Classifier Systems
                  (LNCS 2709)},
  publisher =    {Springer},
  month =    {June},
  pages =    {317-325},
  year =     2003,
  address =  {Guildford, Surrey},
  url =          "http://link.springer.de/link/service/series/0558/tocs/t2709.htm"
}

@TECHREPORT{whitaker03examining,
  author =   {Christopher Whitaker and Ludmila Kuncheva},
  title =    {Examining the relationship between majority vote
                  accuracy and diversity in bagging and boosting},
  institution =  {School of Informatics, University of Wales, Bangor},
  year =     2003,
  type =     {Technical Report},
  url =          "http://www.informatics.bangor.ac.uk/~kuncheva/papers/lkcw_tr.pdf"
}

@INPROCEEDINGS{wilczok03design,
  author =   {Elke Wilczok and Wolfgang Lellmann},
  title =    {Design and Evaluation of an Adaptive Combination
                  Framework for OCR Result Strings},
  booktitle =    {Proc. Int. Workshop on Multiple Classifier Systems
                  (LNCS 2709)},
  publisher =    {Springer},
  month =    {June},
  pages =    {395-404},
  year =     2003,
  address =  {Guildford, Surrey},
  url =          "http://link.springer.de/link/service/series/0558/tocs/t2709.htm"
}

@INPROCEEDINGS{windeatt03spectral,
  author =   {Terry Windeatt and Reza Ghaderi and Gholamreza
                  Ardeshir},
  title =    {Spectral Coefficients and Classifier Correlation},
  booktitle =    {Proc. Int. Workshop on Multiple Classifier Systems
                  (LNCS 2709)},
  publisher =    {Springer},
  month =    {June},
  pages =    {276-285},
  year =     2003,
  address =  {Guildford, Surrey},
  url =          "http://link.springer.de/link/service/series/0558/tocs/t2709.htm"
}

@PROCEEDINGS{mcs2003,
  editor =   {Terry Windeatt and Fabio Roli},
  title =    {Multiple Classifier Systems, 4th International
                  Workshop, MCS 2003, Guilford, UK, June 11-13, 2003,
                  Proceedings},
  booktitle =    {Multiple Classifier Systems},
  publisher =    {Springer},
  series =   {Lecture Notes in Computer Science},
  volume =   2709,
  year =     2003,
  isbn =     {3-540-40369-8},
}

@INPROCEEDINGS{windridge03practical,
  author =   {David Windridge and Josef Kittler},
  title =    {The Practical Performance Characteristics of
                  Tomographically Filtered Multiple Classifier Fusion},
  booktitle =    {Proc. Int. Workshop on Multiple Classifier Systems
                  (LNCS 2709)},
  publisher =    {Springer},
  month =    {June},
  pages =    {186-195},
  year =     2003,
  address =  {Guildford, Surrey},
  url =          "http://link.springer.de/link/service/series/0558/tocs/t2709.htm"
}

@ARTICLE{zhou02extracting,
  author =   {Z.-H. Zhou, Y. Jiang, and S.-F. Chen},
  title =    {Extracting Symbolic Rules from Trained Neural
                  Network Ensembles},
  journal =  {AI Communications, 2003, 16(1): 3-15},
  volume =   16,
  number =   1,
  pages =    {3-15},
  year =     2003,
  url =      {http://citeseer.nj.nec.com/zhou03extracting.html},
}

@INPROCEEDINGS{zouari03simulating,
  author =   {H. Zouari and Laurent Heutte and Yves Lecourtier and
                  Adel M. Alimi},
  title =    {Simulating Classifier Outputs for Evaluating
                  Parallel Combination Methods},
  booktitle =    {Proc. Int. Workshop on Multiple Classifier Systems
                  (LNCS 2709)},
  publisher =    {Springer},
  month =    {June},
  pages =    {296-305},
  year =     2003,
  address =  {Guildford, Surrey},
  url =          "http://link.springer.de/link/service/series/0558/tocs/t2709.htm"
}

@ARTICLE{anthony04lists,
  author =   {Martin Anthony},
  title =    {Generalisation Error Bounds for Threshold Decision
                  Lists},
  journal =  {Journal of {M}achine {L}earning {R}esearch},
  year =     2004,
  volume =   5,
  pages =    {189--217},
  abstract =     {In this paper we consider the generalization
                  accuracy of classification methods based on the
                  iterative use of linear classifiers. The resulting
                  classifiers, which we call threshold decision lists
                  act as follows. Some points of the data set to be
                  classified are given a particular classification
                  according to a linear threshold function (or
                  hyperplane). These are then removed from
                  consideration, and the procedure is iterated until
                  all points are classified. Geometrically, we can
                  imagine that at each stage, points of the same
                  classification are successively chopped off from the
                  data set by a hyperplane. We analyse theoretically
                  the generalization properties of data classification
                  techniques that are based on the use of threshold
                  decision lists and on the special subclass of
                  multilevel threshold functions. We present bounds on
                  the generalization error in a standard probabilistic
                  learning framework. The primary focus in this paper
                  is on obtaining generalization error bounds that
                  depend on the levels of separation---or
                  margins---achieved by the successive linear
                  classifiers. We also improve and extend previously
                  published theoretical bounds on the generalization
                  ability of perceptron decision trees.},
}

@PHDTHESIS{brown04thesis,
  author =   {G. Brown},
  title =    {Diversity in Neural Network Ensembles},
  school =   {School of Computer Science, University of
                  Birmingham},
  year =     2004,
}

@INPROCEEDINGS{davidson04stable,
  author =   {Ian Davidson},
  title =    {An Ensemble Technique for Stable Learners with
                  Performance Bounds},
  booktitle =    {Proceedings of the Nineteenth National Conference on
                  Artificial Intelligence},
  year =     2004,
  publisher =    {AAAI Press},
  pages =    {300-335},
}

@BOOK{kunchevabook,
  author =   {Ludmila Kuncheva},
  title =    {Combining Pattern Classifiers: Methods and
                  Algorithms},
  publisher =    {Wiley Press},
  year =     2004,
  note =     {ISBN 0-471-21078-1},
}

@article{lee2004lob,
  title={{Lossless Online Bayesian Bagging}},
  author={Lee, H.K.H. and Clyde, M.A.},
  journal={The Journal of Machine Learning Research},
  volume={5},
  pages={143-151},
  year={2004},
  publisher={MIT Press Cambridge, MA, USA}
}

@ARTICLE{tino04nonlinear,
  author =   {P. Tino and I. Nabney and B.S. Williams and J. Losel
                  and Y. Sun},
  title =    {Non-linear Prediction of Quantitative
                  Structure-Activity Relationships},
  journal =  {Journal of Chemical Information and Computer
                  Sciences},
  year =     2004,
  volume =   44,
  number =   5,
  pages =    {1647--1653},
}

@ARTICLE{baram04choice,
  author =       {Y. Baram and R. El-Yaniv and K. Luz},
  title =        {Online Choice of Active Learning Algorithms},
  journal =      {Journal of Machine Learning Research},
  year =         {2004},
  volume =       {5},
  pages =        {255--291},
  month =        {March},
}

@inproceedings{AyadBK04,
  author    = {Hanan Ayad and
               Otman A. Basir and
               Mohamed Kamel},
  title     = {A Probabilistic Model Using Information Theoretic Measures
               for Cluster Ensembles.},
  booktitle = {Multiple Classifier Systems},
  year      = {2004},
  pages     = {144-153},
  url       = {http://springerlink.metapress.com/openurl.asp?genre=article{&}issn=0302-9743{&}volume=3077{&}spage=144},
}

@inproceedings{BanfieldHBBKE04,
  author    = {Robert E. Banfield and
               Lawrence O. Hall and
               Kevin W. Bowyer and
               Divya Bhadoria and
               W. Philip Kegelmeyer and
               Steven Eschrich},
  title     = {A Comparison of Ensemble Creation Techniques.},
  booktitle = {Multiple Classifier Systems},
  year      = {2004},
  pages     = {223-232},
  url       = {http://springerlink.metapress.com/openurl.asp?genre=article{&}issn=0302-9743{&}volume=3077{&}spage=223},
}

@inproceedings{BonissoneGY04,
  author    = {Piero P. Bonissone and
               Kai Goebel and
               Weizhong Yan},
  title     = {Classifier Fusion Using Triangular Norms.},
  booktitle = {Multiple Classifier Systems},
  year      = {2004},
  pages     = {154-163},
  url       = {http://springerlink.metapress.com/openurl.asp?genre=article{&}issn=0302-9743{&}volume=3077{&}spage=154},
}

@inproceedings{CaprileMFJ04,
  author    = {Bruno Caprile and
               Stefano Merler and
               Cesare Furlanello and
               Giuseppe Jurman},
  title     = {Exact Bagging with k-Nearest Neighbour Classifiers.},
  booktitle = {Multiple Classifier Systems},
  year      = {2004},
  pages     = {72-81},
  url       = {http://springerlink.metapress.com/openurl.asp?genre=article{&}issn=0302-9743{&}volume=3077{&}spage=72},
}

@inproceedings{ChenKJ04,
  author    = {Lei Chen and
               Mohamed Kamel and
               Ju Jiang},
  title     = {A Modular System for the Classification of Time Series Data.},
  booktitle = {Multiple Classifier Systems},
  year      = {2004},
  pages     = {134-143},
  url       = {http://springerlink.metapress.com/openurl.asp?genre=article{&}issn=0302-9743{&}volume=3077{&}spage=134},
}

@inproceedings{CordellaLS04,
  author    = {Luigi P. Cordella and
               Alessandro Limongiello and
               Carlo Sansone},
  title     = {Network Intrusion Detection by a Multi-stage Classification
               System.},
  booktitle = {Multiple Classifier Systems},
  year      = {2004},
  pages     = {324-333},
  url       = {http://springerlink.metapress.com/openurl.asp?genre=article{&}issn=0302-9743{&}volume=3077{&}spage=324},
}

@inproceedings{DaraK04,
  author    = {Rozita A. Dara and
               Mohamed S. Kamel},
  title     = {Sharing Training Patterns among Multiple Classifiers.},
  booktitle = {Multiple Classifier Systems},
  year      = {2004},
  pages     = {243-252},
  url       = {http://springerlink.metapress.com/openurl.asp?genre=article{&}issn=0302-9743{&}volume=3077{&}spage=243},
}

@inproceedings{DidaciG04,
  author    = {Luca Didaci and
               Giorgio Giacinto},
  title     = {Dynamic Classifier Selection by Adaptive k-Nearest-Neighbourhood
               Rule.},
  booktitle = {Multiple Classifier Systems},
  year      = {2004},
  pages     = {174-183},
  url       = {http://springerlink.metapress.com/openurl.asp?genre=article{&}issn=0302-9743{&}volume=3077{&}spage=174},
}

@inproceedings{DiegoMM04,
  author    = {Isaac Mart'{i}n de Diego and
               Javier M. Moguerza and
               Alberto Mu{~n}oz},
  title     = {Combining Kernel Information for Support Vector Classification.},
  booktitle = {Multiple Classifier Systems},
  year      = {2004},
  pages     = {102-111},
  url       = {http://springerlink.metapress.com/openurl.asp?genre=article{&}issn=0302-9743{&}volume=3077{&}spage=102},
}

@inproceedings{EstruchFHR04,
  author    = {Vicent Estruch and
               C{'e}sar Ferri and
               Jos{'e} Hern{'a}ndez-Orallo and
               M. Jos{'e} Ram'{i}rez-Quintana},
  title     = {Bagging Decision Multi-trees.},
  booktitle = {Multiple Classifier Systems},
  year      = {2004},
  pages     = {41-51},
  url       = {http://springerlink.metapress.com/openurl.asp?genre=article{&}issn=0302-9743{&}volume=3077{&}spage=41},
}

@inproceedings{GunterB04,
  author    = {Simon G{"u}nter and
               Horst Bunke},
  title     = {Ensembles of Classifiers Derived from Multiple Prototypes
               and Their Application to Handwriting Recognition.},
  booktitle = {Multiple Classifier Systems},
  year      = {2004},
  pages     = {314-323},
  url       = {http://springerlink.metapress.com/openurl.asp?genre=article{&}issn=0302-9743{&}volume=3077{&}spage=314},
}

@inproceedings{Hernandez-EspinosaFT04,
  author    = {Carlos Hern{'a}ndez-Espinosa and
               Mercedes Fern{'a}ndez-Redondo and
               Joaqu'{i}n Torres-Sospedra},
  title     = {First Experiments on Ensembles of Radial Basis Functions.},
  booktitle = {Multiple Classifier Systems},
  year      = {2004},
  pages     = {253-262},
  url       = {http://springerlink.metapress.com/openurl.asp?genre=article{&}issn=0302-9743{&}volume=3077{&}spage=253},
}

@inproceedings{JuszczakD04,
  author    = {Piotr Juszczak and
               Robert P. W. Duin},
  title     = {Combining One-Class Classifiers to Classify Missing Data.},
  booktitle = {Multiple Classifier Systems},
  year      = {2004},
  pages     = {92-101},
  url       = {http://springerlink.metapress.com/openurl.asp?genre=article{&}issn=0302-9743{&}volume=3077{&}spage=92},
}

@inproceedings{Kang04,
  author    = {Hee-Joong Kang},
  title     = {Combining Classifiers Using Dependency-Based Product Approximation
               with Bayes Error Rate.},
  booktitle = {Multiple Classifier Systems},
  year      = {2004},
  pages     = {112-121},
  url       = {http://springerlink.metapress.com/openurl.asp?genre=article{&}issn=0302-9743{&}volume=3077{&}spage=112},
}

@inproceedings{KittlerS04,
  author    = {Josef Kittler and
               Mohammad Sadeghi},
  title     = {Physics-Based Decorrelation of Image Data for Decision Level
               Fusion in Face Verification.},
  booktitle = {Multiple Classifier Systems},
  year      = {2004},
  pages     = {354-363},
  url       = {http://springerlink.metapress.com/openurl.asp?genre=article{&}issn=0302-9743{&}volume=3077{&}spage=354},
}

@inproceedings{Kuncheva04,
  author    = {Ludmila I. Kuncheva},
  title     = {Classifier Ensembles for Changing Environments.},
  booktitle = {Multiple Classifier Systems},
  year      = {2004},
  pages     = {1-15},
  url       = {http://springerlink.metapress.com/openurl.asp?genre=article{&}issn=0302-9743{&}volume=3077{&}spage=1},
}

@inproceedings{MarcialisR04,
  author    = {Gian Luca Marcialis and
               Fabio Roli},
  title     = {High Security Fingerprint Verification by Perceptron-Based
               Fusion of Multiple Matchers.},
  booktitle = {Multiple Classifier Systems},
  year      = {2004},
  pages     = {364-373},
  url       = {http://springerlink.metapress.com/openurl.asp?genre=article{&}issn=0302-9743{&}volume=3077{&}spage=364},
}

@inproceedings{MarroccoT04,
  author    = {Claudio Marrocco and
               Francesco Tortorella},
  title     = {A Method for Designing Cost-Sensitive ECOC.},
  booktitle = {Multiple Classifier Systems},
  year      = {2004},
  pages     = {204-213},
  url       = {http://springerlink.metapress.com/openurl.asp?genre=article{&}issn=0302-9743{&}volume=3077{&}spage=204},
}

@inproceedings{MelvilleSMM04,
  author    = {Prem Melville and
               Nishit Shah and
               Lilyana Mihalkova and
               Raymond J. Mooney},
  title     = {Experiments on Ensembles with Missing and Noisy Data.},
  booktitle = {Multiple Classifier Systems},
  year      = {2004},
  pages     = {293-302},
  url       = {http://springerlink.metapress.com/openurl.asp?genre=article{&}issn=0302-9743{&}volume=3077{&}spage=293},
}

@inproceedings{MuhlbaierTP04,
  author    = {Michael Muhlbaier and
               Apostolos Topalis and
               Robi Polikar},
  title     = {Learn++.MT: A New Approach to Incremental Learning.},
  booktitle = {Multiple Classifier Systems},
  year      = {2004},
  pages     = {52-61},
  url       = {http://springerlink.metapress.com/openurl.asp?genre=article{&}issn=0302-9743{&}volume=3077{&}spage=52},
}

@inproceedings{Oza04,
  author    = {Nikunj C. Oza},
  title     = {AveBoost2: Boosting for Noisy Data.},
  booktitle = {Multiple Classifier Systems},
  year      = {2004},
  pages     = {31-40},
  url       = {http://springerlink.metapress.com/openurl.asp?genre=article{&}issn=0302-9743{&}volume=3077{&}spage=31},
}

@inproceedings{PekalskaSD04,
  author    = {Elzbieta Pekalska and
               Marina Skurichina and
               Robert P. W. Duin},
  title     = {Combining Dissimilarity-Based One-Class Classifiers.},
  booktitle = {Multiple Classifier Systems},
  year      = {2004},
  pages     = {122-133},
  url       = {http://springerlink.metapress.com/openurl.asp?genre=article{&}issn=0302-9743{&}volume=3077{&}spage=122},
}

@inproceedings{RahmanTKA04,
  author    = {Fuad Rahman and
               Yuliya Tarnikova and
               Aman Kumar and
               Hassan Alam},
  title     = {Second Guessing a Commercial 'Black Box' Classifier by an
               'In House' Classifier: Serial Classifier Combination in
               a Speech Recognition Application.},
  booktitle = {Multiple Classifier Systems},
  year      = {2004},
  pages     = {374-383},
  url       = {http://springerlink.metapress.com/openurl.asp?genre=article{&}issn=0302-9743{&}volume=3077{&}spage=374},
}

@inproceedings{RajanG04,
  author    = {Suju Rajan and
               Joydeep Ghosh},
  title     = {An Empirical Comparison of Hierarchical vs. Two-Level Approaches
               to Multiclass Problems.},
  booktitle = {Multiple Classifier Systems},
  year      = {2004},
  pages     = {283-292},
  url       = {http://springerlink.metapress.com/openurl.asp?genre=article{&}issn=0302-9743{&}volume=3077{&}spage=283},
}

@inproceedings{Rao04,
  author    = {Nageswara S. V. Rao},
  title     = {A Generic Sensor Fusion Problem: Classification and Function
               Estimation.},
  booktitle = {Multiple Classifier Systems},
  year      = {2004},
  pages     = {16-30},
  url       = {http://springerlink.metapress.com/openurl.asp?genre=article{&}issn=0302-9743{&}volume=3077{&}spage=16},
}

@inproceedings{RaudysI04,
  author    = {Sarunas Raudys and
               Masakazu Iwamura},
  title     = {Multiple Classifiers System for Reducing Influences of Atypical
               Observations.},
  booktitle = {Multiple Classifier Systems},
  year      = {2004},
  pages     = {233-242},
  url       = {http://springerlink.metapress.com/openurl.asp?genre=article{&}issn=0302-9743{&}volume=3077{&}spage=233},
}

@inproceedings{RooneyPAT04,
  author    = {Niall Rooney and
               David W. Patterson and
               Sarab S. Anand and
               Alexey Tsymbal},
  title     = {Dynamic Integration of Regression Models.},
  booktitle = {Multiple Classifier Systems},
  year      = {2004},
  pages     = {164-173},
  url       = {http://springerlink.metapress.com/openurl.asp?genre=article{&}issn=0302-9743{&}volume=3077{&}spage=164},
}

@inproceedings{SaerensF04,
  author    = {Marco Saerens and
               Francois Fouss},
  title     = {Yet Another Method for Combining Classifiers Outputs: A
               Maximum Entropy Approach.},
  booktitle = {Multiple Classifier Systems},
  year      = {2004},
  pages     = {82-91},
  url       = {http://springerlink.metapress.com/openurl.asp?genre=article{&}issn=0302-9743{&}volume=3077{&}spage=82},
}

@inproceedings{SchenkerBLK04,
  author    = {Adam Schenker and
               Horst Bunke and
               Mark Last and
               Abraham Kandel},
  title     = {Building Graph-Based Classifier Ensembles by Random Node
               Selection.},
  booktitle = {Multiple Classifier Systems},
  year      = {2004},
  pages     = {214-222},
  url       = {http://springerlink.metapress.com/openurl.asp?genre=article{&}issn=0302-9743{&}volume=3077{&}spage=214},
}

@inproceedings{SvetnikLTW04,
  author    = {Vladimir Svetnik and
               Andy Liaw and
               Christopher Tong and
               Ting Wang},
  title     = {Application of Breiman's Random Forest to Modeling Structure-Activity
               Relationships of Pharmaceutical Molecules.},
  booktitle = {Multiple Classifier Systems},
  year      = {2004},
  pages     = {334-343},
  url       = {http://springerlink.metapress.com/openurl.asp?genre=article{&}issn=0302-9743{&}volume=3077{&}spage=334},
}

@inproceedings{TapiaGHG04,
  author    = {Elizabeth Tapia and
               Jos{'e} Carlos Gonz{'a}lez and
               Alexander H{"u}termann and
               L. Javier Garc'{i}a-Villalba},
  title     = {Beyond Boosting: Recursive ECOC Learning Machines.},
  booktitle = {Multiple Classifier Systems},
  year      = {2004},
  pages     = {62-71},
  url       = {http://springerlink.metapress.com/openurl.asp?genre=article{&}issn=0302-9743{&}volume=3077{&}spage=62},
}

@inproceedings{Valentini04,
  author    = {Giorgio Valentini},
  title     = {Random Aggregated and Bagged Ensembles of SVMs: An Empirical
               Bias?Variance Analysis.},
  booktitle = {Multiple Classifier Systems},
  year      = {2004},
  pages     = {263-272},
  url       = {http://springerlink.metapress.com/openurl.asp?genre=article{&}issn=0302-9743{&}volume=3077{&}spage=263},
}

@inproceedings{WangT04,
  author    = {Xiaogang Wang and
               Xiaoou Tang},
  title     = {Experimental Study on Multiple LDA Classifier Combination
               for High Dimensional Data Classification.},
  booktitle = {Multiple Classifier Systems},
  year      = {2004},
  pages     = {344-353},
  url       = {http://springerlink.metapress.com/openurl.asp?genre=article{&}issn=0302-9743{&}volume=3077{&}spage=344},
}

@inproceedings{Windeatt04,
  author    = {Terry Windeatt},
  title     = {Spectral Measure for Multi-class Problems.},
  booktitle = {Multiple Classifier Systems},
  year      = {2004},
  pages     = {184-193},
  url       = {http://springerlink.metapress.com/openurl.asp?genre=article{&}issn=0302-9743{&}volume=3077{&}spage=184},
}

@inproceedings{WindridgeB04,
  author    = {David Windridge and
               Richard Bowden},
  title     = {Induced Decision Fusion in Automated Sign Language Interpretation:
               Using ICA to Isolate the Underlying Components of Sign.},
  booktitle = {Multiple Classifier Systems},
  year      = {2004},
  pages     = {303-313},
  url       = {http://springerlink.metapress.com/openurl.asp?genre=article{&}issn=0302-9743{&}volume=3077{&}spage=303},
}

@inproceedings{WindridgePK04,
  author    = {David Windridge and
               Robin Patenall and
               Josef Kittler},
  title     = {The Relationship between Classifier Factorisation and Performance
               in Stochastic Vector Quantisation.},
  booktitle = {Multiple Classifier Systems},
  year      = {2004},
  pages     = {194-203},
  url       = {http://springerlink.metapress.com/openurl.asp?genre=article{&}issn=0302-9743{&}volume=3077{&}spage=194},
}

@inproceedings{ZouariHLA04,
  author    = {H{'e}la Zouari and
               Laurent Heutte and
               Yves Lecourtier and
               Adel M. Alimi},
  title     = {Building Diverse Classifier Outputs to Evaluate the Behavior
               of Combination Methods: The Case of Two Classifiers.},
  booktitle = {Multiple Classifier Systems},
  year      = {2004},
  pages     = {273-282},
  url       = {http://springerlink.metapress.com/openurl.asp?genre=article{&}issn=0302-9743{&}volume=3077{&}spage=273}
}

@proceedings{mcs2004,
  editor    = {Fabio Roli and
               Josef Kittler and
               Terry Windeatt},
  title     = {Multiple Classifier Systems, 5th International Workshop,
               MCS 2004, Cagliari, Italy, June 9-11, 2004, Proceedings},
  booktitle = {Multiple Classifier Systems},
  publisher = {Springer},
  series    = {Lecture Notes in Computer Science},
  volume    = {3077},
  year      = {2004},
  isbn      = {3-540-22144-1}
}

@ARTICLE{brown04survey,
  author =   {G. Brown and J.L. Wyatt and R. Harris and X. Yao},
  title =    {Diversity Creation Methods: A Survey and
                  Categorisation},
  journal =  {Journal of Information Fusion},
  volume =   6,
  number =   1,
  month =    {March},
  pages =    {5--20},
  year =     2005
}

@INPROCEEDINGS{brownwyatt05extremes,
  author =   {G. Brown and J.L. Wyatt and P. Sun},
  title =    {Between Two Extremes: Examining Decompositions of
                  the Ensemble Objective Function},
  booktitle =    {Proc. Int. Workshop on Multiple Classifier Systems
                  (LNCS 3541)},
  publisher =    {Springer},
  month =    {June},
  year =     2005,
  address =  {Monterey, California}
}

@ARTICLE{brownwyatt05jmlr,
  title =    {Managing Diversity in Regression Ensembles},
  author =   {G. Brown and J.L. Wyatt and P. Tino},
  journal =  {Journal of Machine Learning Research},
  volume =   6,
  pages =        {1621--1650},
  year =     2005
}

@article{oza2005oba,
  title={{Online bagging and boosting}},
  author={Oza, NC},
  journal={Systems, Man and Cybernetics, 2005 IEEE International Conference on},
  volume={3},
  year={2005}
}




@MISC{roli02notes,
  author =   {Fabio Roli},
  title =    {Lecture Notes: Linear Combiners for Fusion of
                  Pattern Classifiers},
  url =          "http://www.disi.unige.it/person/MasulliF/ricerca/school2002/contributions/vietri02-lect-roli.pdf"
}
