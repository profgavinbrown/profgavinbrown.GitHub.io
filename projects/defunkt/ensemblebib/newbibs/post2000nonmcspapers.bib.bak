#POST2000NONMCSPAPERS


@inproceedings{Dzeroski02Combining,
  author = {Sa{\v{s}}o D{\v{z}}eroski and Bernard {\v{Z}}enko},
  title = {Is Combining Classifiers Better than Selecting the Best One},
  booktitle = {ICML '02: Proceedings of the Nineteenth International Conference on Machine Learning},
  year = {2002},
  isbn = {1-55860-873-7},
  pages = {123--130},
  publisher = {Morgan Kaufmann},
  address = {San Francisco, CA, USA},
  url = {http://www-ai.ijs.si/SasoDzeroski/files/2002_DZ_CombiningClassifiersBetter.pdf},
}

@inproceedings{Zenko02Stacking,
  author = {Bernard {\v{Z}}enko and Sa{\v{s}}o D{\v{z}}eroski},
  title = {{Stacking with an Extended Set of Meta-level Attributes and MLR}},
  booktitle = {ECML '02: Proceedings of the 13th European Conference on Machine Learning},
  year = {2002},
  isbn = {3-540-44036-4},
  pages = {493--504},
  publisher = {Springer},
  address = {Berlin, Germany},
  url = {http://www.springerlink.com/content/qnr9tdtae9n27g3v/},
}

@article{Dzeroski04Combining,
  author = {Sa{\v{s}}o D{\v{z}}eroski and Bernard {\v{Z}}enko},
  title = {{Is Combining Classifiers with Stacking Better than Selecting the Best One?}},
  journal = {Machine Learning},
  volume = {54},
  number = {3},
  year = {2004},
  issn = {0885-6125},
  pages = {255--273},
  doi = {http://dx.doi.org/10.1023/B:MACH.0000015881.36452.6e},
  publisher = {Kluwer},
  address = {Hingham, MA, USA},
}



@article{rodriguez2006rfn,
  title={{Rotation forest: A new classifier ensemble method}},
  author={RODRIGUEZ, J.J. and KUNCHEVA, L.I. and ALONSO, C.J.},
  journal={IEEE transactions on pattern analysis and machine intelligence},
  volume={28},
  number={10},
  pages={1619--1630},
  year={2006},
  publisher={Institute of Electrical and Electronics Engineers}
}

@article{windeatt2006ada,
  title={{Accuracy/Diversity and Ensemble MLP Classifier Design}},
  author={Windeatt, T.},
  journal={Neural Networks, IEEE Transactions on},
  volume={17},
  number={5},
  pages={1194--1211},
  year={2006}
}

@article{ko2006nof,
  title={{A New Objective Function for Ensemble Selection in Random Subspaces}},
  author={Ko, A.H.R. and Sabourin, R. and de Souza Britto Jr, A.},
  journal={Proceedings of the 18th International Conference on Pattern Recognition (ICPR'06)-Volume 04},
  pages={185--188},
  year={2006},
  publisher={IEEE Computer Society Washington, DC, USA}
}

@article{tang2006adm,
  title={{An analysis of diversity measures}},
  author={Tang, E.K. and Suganthan, P.N. and Yao, X.},
  journal={Machine Learning},
  volume={65},
  number={1},
  pages={247--271},
  year={2006},
  publisher={Springer}
}

@article{canuto2006uwd,
  title={{Using weighted dynamic classifier selection methods in ensembles with different levels of diversity}},
  author={Canuto, A.M.P. and Fagundes, D. and Abreu, M.C.C. and Junior, J.C.X.},
  journal={International Journal of Hybrid Intelligent Systems},
  volume={3},
  number={3},
  pages={147--158},
  year={2006},
  publisher={IOS Press}
}

@article{kotsiantis2006lah,
  title={{Local averaging of heterogeneous regression models}},
  author={Kotsiantis, SB},
  journal={International Journal of Hybrid Intelligent Systems},
  volume={3},
  number={2},
  pages={99--107},
  year={2006},
  publisher={IOS Press}
}

@article{zhang2006epv,
  title={{Ensemble Pruning Via Semi-definite Programming}},
  author={Zhang, Y. and Burer, S. and Street, W.N.},
  journal={Journal of Machine Learning Research},
  volume={7},
  pages={1315--1338},
  year={2006}
}

@article{prior2006ptu,
  title={{Parameter Tuning using the Out-of-Bootstrap Generalisation Error Estimate for Stochastic Discrimination and Random Forests}},
  author={Prior, M. and Windeatt, T.},
  journal={Proceedings of the 18th International Conference on Pattern Recognition (ICPR'06)-Volume 02},
  pages={498--501},
  year={2006},
  publisher={IEEE Computer Society Washington, DC, USA}
}

@article{lefaucheur2006rme,
  title={{Robust Multiclass Ensemble Classifiers via Symmetric Functions}},
  author={Lefaucheur, P. and Nock, R.},
  journal={Proceedings of the 18th International Conference on Pattern Recognition (ICPR'06)-Volume 04},
  pages={136--139},
  year={2006},
  publisher={IEEE Computer Society Washington, DC, USA}
}

@article{ko2006eec,
  title={{Evolving ensemble of classifiers in random subspace}},
  author={Ko, A.H.R. and Sabourin, R. and de Souza Britto Jr, A.},
  journal={Proceedings of the 8th annual conference on Genetic and evolutionary computation},
  pages={1473--1480},
  year={2006},
  publisher={ACM Press New York, NY, USA}
}

@article{kim2006oec,
  title={{Optimal ensemble construction via meta-evolutionary ensembles}},
  author={Kim, Y.S. and Street, W.N. and Menczer, F.},
  journal={Expert Systems With Applications},
  volume={30},
  number={4},
  pages={705--714},
  year={2006},
  publisher={Elsevier}
}

@article{karmaker2006bar,
  title={{A boosting approach to remove class label noise}},
  author={Karmaker, A. and Kwek, S.},
  journal={International Journal of Hybrid Intelligent Systems},
  volume={3},
  number={3},
  pages={169--177},
  year={2006},
  publisher={IOS Press}
}

@article{chandra2006elu,
  title={{Ensemble Learning Using Multi-Objective Evolutionary Algorithms}},
  author={Chandra, A. and Yao, X.},
  journal={Journal of Mathematical Modelling and Algorithms},
  volume={5},
  number={4},
  pages={417--445},
  year={2006},
  publisher={Springer}
}

@article{hadjitodorov2006mdb,
  title={{Moderate diversity for better cluster ensembles}},
  author={Hadjitodorov, S.T. and Kuncheva, L.I. and Todorova, L.P.},
  journal={Information Fusion},
  volume={7},
  number={3},
  pages={264--275},
  year={2006},
  publisher={Elsevier}
}

@ARTICLE{atikorale03hong,
  author       = {A. S. Atukorale, T. Downs and P. N. Suganthan},
  title        = {Boosting HONG Networks},
  journal      = {Neurocomputing},
  year         = 2003,
  volume       = 51,
  pages        = {75-86},
  month        = {April},
}

@INPROCEEDINGS{ahn01speciated,
  author       = {Joon-Hyun Ahn and Sung-Bae Cho},
  title        = {Speciated Neural Networks Evolved with Fitness
                  Sharing Technique},
  booktitle    = {Proceedings of the Congress on Evolutionary
                  Computation},
  address      = {Seoul, Korea},
  month        = {May 27-30},
  pages        = {390--396},
  year         = 2001,
}

@ARTICLE{anthony04lists,
  author       = {Martin Anthony},
  title        = {Generalisation Error Bounds for Threshold Decision
                  Lists},
  journal      = {Journal of {M}achine {L}earning {R}esearch},
  year         = 2004,
  volume       = 5,
  pages        = {189--217},
  abstract     = {In this paper we consider the generalization
                  accuracy of classification methods based on the
                  iterative use of linear classifiers. The resulting
                  classifiers, which we call threshold decision lists
                  act as follows. Some points of the data set to be
                  classified are given a particular classification
                  according to a linear threshold function (or
                  hyperplane). These are then removed from
                  consideration, and the procedure is iterated until
                  all points are classified. Geometrically, we can
                  imagine that at each stage, points of the same
                  classification are successively chopped off from the
                  data set by a hyperplane. We analyse theoretically
                  the generalization properties of data classification
                  techniques that are based on the use of threshold
                  decision lists and on the special subclass of
                  multilevel threshold functions. We present bounds on
                  the generalization error in a standard probabilistic
                  learning framework. The primary focus in this paper
                  is on obtaining generalization error bounds that
                  depend on the levels of separation---or
                  margins---achieved by the successive linear
                  classifiers. We also improve and extend previously
                  published theoretical bounds on the generalization
                  ability of perceptron decision trees.},
}

@INPROCEEDINGS{bahler00,
  author       = {Dennis Bahler and Laura Navarro},
  title        = {Methods for Combining Heterogeneous Sets of
                  Classifiers},
  booktitle    = {17th Natl. Conf. on Artificial Intelligence (AAAI),
                  Workshop on New Research Problems for Machine
                  Learning},
  year         = 2000,
}

@article{banfield05thinning,
  author    = {Robert E. Banfield and
               Lawrence O. Hall and
               Kevin W. Bowyer and
               W. Philip Kegelmeyer},
  title     = {Ensemble diversity measures and their application to thinning.},
  journal   = {Information Fusion},
  volume    = {6},
  number    = {1},
  year      = {2005},
  pages     = {49-62}
}

@ARTICLE{baldridge06active,
  title        = {{Active Learning and Logarithmic Opinion Pools for HPSG Parse Selection}},
  author       = {Baldridge, J. and Osborne, M.},
  journal      = {Natural Language Engineering (in press)},
}

@ARTICLE{baram04choice,
  author       = {Y. Baram and R. El-Yaniv and K. Luz},
  title        = {Online Choice of Active Learning Algorithms},
  journal      = {Journal of Machine Learning Research},
  year         = 2004,
  volume       = 5,
  pages        = {255--291},
  month        = {March},
}

@ARTICLE{brameier01evolving,
  author       = {Markus Brameier and Wolfgang Banzhaf},
  title        = {Evolving Teams of Predictors with Linear Genetic
                  Programming},
  journal      = {Genetic Programming and Evolvable Machines},
  volume       = 2,
  number       = 4,
  pages        = {381--407},
  year         = 2001,
}

@TECHREPORT{breiman00infinite,
  author       = {L.~Breiman},
  title        = {Some Infinite Theory for Predictor Ensembles},
  institution  = {Statistics Department, UC Berkeley},
  year         = 2000,
  number       = 577,
  month        = {August},
  pdf          = {http://www.boosting.org/papers/some_theory2001.pdf},
}

@PHDTHESIS{brown04thesis,
  author       = {G. Brown},
  title        = {Diversity in Neural Network Ensembles},
  school       = {School of Computer Science, University of
                  Birmingham},
  year         = 2004,
}

@INPROCEEDINGS{brownwyatt03ambiguity,
  author       = {G. Brown and J.L. Wyatt},
  title        = {The Use of the Ambiguity Decomposition in Neural
                  Network Ensemble Learning Methods},
  booktitle    = {20th {I}nternational {C}onference on {M}achine
                  {L}earning (ICML'03)},
  year         = 2003,
  editor       = {Tom Fawcett and Nina Mishra},
  month        = {August},
  address      = {Washington DC, USA},
}

@ARTICLE{brown04survey,
  author       = {G. Brown and J.L. Wyatt and R. Harris and X. Yao},
  title        = {Diversity Creation Methods: A Survey and
                  Categorisation},
  journal      = {Journal of Information Fusion},
  volume       = 6,
  number       = 1,
  month        = {March},
  pages        = {5--20},
  year         = 2005,
}

@ARTICLE{brownwyatt05jmlr,
  title        = {Managing Diversity in Regression Ensembles},
  author       = {G. Brown and J.L. Wyatt and P. Tino},
  journal      = {Journal of Machine Learning Research},
  volume       = 6,
  pages        = {1621--1650},
  year         = 2005,
}

@INPROCEEDINGS{brownyao01:ukci,
  author       = {Gavin Brown and Xin Yao},
  title        = {On The {E}ffectiveness of {N}egative {C}orrelation
                  {L}earning},
  booktitle    = {Proceedings of First UK Workshop on Computational
                  Intelligence},
  note         = {Edinburgh, Scotland},
  pages        = {57-62},
  year         = 2001,
}

@INPROCEEDINGS{brownyao02exploiting,
  author       = {Gavin Brown and Xin Yao and Jeremy Wyatt and Heiko
                  Wersing and Bernhard Sendhoff},
  title        = {Exploiting Ensemble Diversity for Automatic Feature
                  Extraction},
  booktitle    = {Proc. of the 9th International Conference on Neural
                  Information Processing (ICONIP'02)},
  pages        = {1786-1790},
  year         = 2002,
  month        = {November},
}

@TECHREPORT{buhlmann00explaining,
  author       = {Peter Buhlmann and Bin Yu},
  title        = {Explaining Bagging},
  institution  = {ETH Zurich, Seminar Fur Statistik},
  month        = {May},
  year         = 2000,
  number       = 92,
  url          = {ftp://ftp.stat.math.ethz.ch/Research-Reports/92.html},
}

@INPROCEEDINGS{caruana06icml,
  author       = {Rich Caruana and Alexandru Niculescu-Mizil},
  title        = {An Empirical Comparison of Supervised Learning Algorithms},
  booktitle    = {International Conference on Machine Learning},
  year         = 2006,
  organization = {Department of Computer Science, Cornell University},
  annote       = {Large empirical comparison of ML methods. Boosting and Bagging come top.},
}

@INPROCEEDINGS{chawla01small,
  author       = {Nitesh Chawla and Thomas Moore and Kevin Bowyer and
                  Lawrence Hall and Clayton Springer and Philip
                  Kegelmeyer},
  title        = {Bagging is a Small-Data-Set Phenomenon},
  booktitle    = {International Conference on Computer Vision and
                  Pattern Recognition (CVPR)},
  year         = 2001,
}

@INPROCEEDINGS{cheung03radial,
  author       = {Y.M. Cheung and R.B. Huang},
  title        = {An Advance on Divide-and-Conquer Based Radial Basis
                  Function Networks},
  booktitle    = {Proceedings of Fourth International Conference on
                  Intelligent Data Engineering and Automated Learning},
  address      = {Hong Kong},
  month        = {March},
  year         = 2003,
}

@ARTICLE{cohenIntrator01,
  author       = {S. Cohen and N. Intrator},
  year         = 2002,
  title        = {A hybrid projection based and radial basis function
                  architecture: Initial values and global
                  optimization},
  journal      = {Pattern Anal. Appl. (Special issue on Fusion of
                  Multiple Classifiers)},
  volume       = 5,
  number       = 2,
  pages        = {113--120},
}

@ARTICLE{cohenIntratorFusion01,
  author       = {S. Cohen and N. Intrator},
  title        = {Automatic model selection in a hybrid
                  perceptron/radial network},
  journal      = {Information Fusion},
  volume       = 3,
  number       = 4,
  pages        = {259--266},
  year         = 2002,
}

@INPROCEEDINGS{cunningham00diversity,
  author       = {Padraig Cunningham and John Carney},
  title        = {Diversity versus Quality in Classification Ensembles
                  Based on Feature Selection},
  booktitle    = {LNCS - European Conference on Machine Learning},
  volume       = 1810,
  publisher    = {Springer, Berlin},
  pages        = {109--116},
  year         = 2000,
}

@INPROCEEDINGS{davidson04stable,
  author       = {Ian Davidson},
  title        = {An Ensemble Technique for Stable Learners with
                  Performance Bounds},
  booktitle    = {Proceedings of the Nineteenth National Conference on
                  Artificial Intelligence},
  year         = 2004,
  publisher    = {AAAI Press},
  pages        = {300-335},
}

@ARTICLE{dietterich00experimental,
  author       = {Thomas G. Dietterich},
  title        = {An Experimental Comparison of Three Methods for
                  Constructing Ensembles of Decision Trees: Bagging,
                  Boosting, and Randomization},
  journal      = {Machine Learning},
  volume       = 40,
  number       = 2,
  pages        = {139-157},
  year         = 2000,
}

@INPROCEEDINGS{domingos00unified,
  author       = {Pedro Domingos},
  title        = {A Unified Bias-Variance Decomposition and its
                  Applications},
  booktitle    = {Proc. 17th International Conf. on Machine Learning},
  publisher    = {Morgan Kaufmann, San Francisco, CA},
  pages        = {231--238},
  year         = 2000,
  url          = {http://citeseer.nj.nec.com/domingos00unified.html},
}

@INPROCEEDINGS{domingos00unified-AAAI,
  author       = {Pedro Domingos},
  title        = {A Unified Bias-Variance Decomposition for Zero-One
                  and Squared Loss},
  booktitle    = {{AAAI}/{IAAI}},
  pages        = {564-569},
  year         = 2000,
}

@TECHREPORT{drug06formal,
  author       = {Jan Drugowitsch and Alwyn M Barry},
  title        = {A Formal Framework and Extensions for Function Approximation in Learning Classifier Systems},
  institution  = {University of Bath},
  year         = 2006,
  number       = {CSBU2006-01},
}

@INPROCEEDINGS{duchitert03undemocratic,
  author       = {Wlodzislaw Duch and Lukasz Itert},
  title        = {Committees of Undemocratic Competent Models},
  booktitle    = {International Conference on Artificial Neural
                  Networks (ICANN) and International Conference on
                  Neural Information Processing (ICONIP)},
  year         = 2003,
  address      = {Istanbul, Turkey},
  month        = June,
  pages        = {33--36},
  url          = {http://www.phys.uni.torun.pl/publications/kmk/03-C-Ensambles-s.html},
}

@BOOK{duda01book,
  author       = {Richard Duda and Peter Hart and David Stork},
  title        = {Pattern Classification},
  publisher    = {John Wiley and Sons},
  year         = 2001,
  note         = {0-471-05669-3},
}

@ARTICLE{fern03onlinebranch,
  author       = {Alan Fern and
                  Robert Givan},
  title        = {Online Ensemble Learning: An Empirical Study.},
  journal      = {Machine Learning},
  volume       = 53,
  number       = {1-2},
  year         = 2003,
  pages        = {71-109},
  abstract     = {Applied online bagging and boosting to branch prediction},
}

@ARTICLE{fern2003rph,
  title        = {{Random projection for high dimensional data clustering: A cluster ensemble approach}},
  author       = {X. Fern and C. Brodley},
  journal      = {Proceedings of the Twentieth International Conference on Machine Learning},
  pages        = {186--193},
  year         = 2003,
}

@ARTICLE{fleuret04cmi,
  title        = {{Fast Binary Feature Selection with Conditional Mutual Information}},
  author       = {Fleuret, F.},
  journal      = {The Journal of Machine Learning Research},
  volume       = 5,
  pages        = {1531-1555},
  year         = 2004,
  publisher    = {MIT Press Cambridge, MA, USA},
}

@INPROCEEDINGS{freund01averaging,
  title        = {Why Averaging Classifiers can Protect Against
                  Overfitting},
  author       = {Yoav Freund and Yishay Mansour and Robert Schapire},
  booktitle    = {Eighth International Workshop on Artificial
                  Intelligence and Statistics},
  year         = 2001,
}

@TECHREPORT{friedman03isle,
  author       = {J.H. Friedman and B. Popescu},
  title        = {Importance Sampling Learning Ensembles},
  institution  = {Stanford University},
  year         = 2003,
  month        = {September},
  url          = {http://www-stat.stanford.edu/\~{}jhf/ftp/isle.pdf},
}

@ARTICLE{gencay01pricing,
  author       = {R. Gencay and Min Qi},
  title        = {Pricing and hedging derivative securities with
                  neural networks: Bayesian regularization, early
                  stopping, and bagging},
  journal      = {IEEE Transactions on Neural Networks},
  year         = 2001,
  volume       = 12,
  issue        = 4,
  month        = {July},
  pages        = {726--734},
}

@INPROCEEDINGS{grandvalet01bagging,
  author       = {Y.~Grandvalet},
  title        = {Bagging can stabilize without reducing variance},
  booktitle    = {ICANN'01},
  series       = {Lecture Notes in Computer Science},
  publisher    = {Springer},
  year         = 2001,
}

@INPROCEEDINGS{navone2000,
  author       = {H.D.Navone and P.F.Verdes and P.M.Granitto and
                  H.A.Ceccatto},
  title        = {A New Algorithm for Selecting Diverse Members of a
                  Neural Network Ensemble},
  booktitle    = {6th International Congress on Information
                  Engineering},
  year         = 2000,
  note         = {Buenos Aires, Argentina},
}

@PHDTHESIS{hansen00thesis,
  author       = {J.V. Hansen},
  title        = {Combining Predictors: Meta Machine Learning Methods
                  and Bias/Variance and Ambiguity Decompositions},
  school       = {Aarhus Universitet, Datalogisk Institut},
  year         = 2000,
}

@ARTICLE{hayashi02medical,
  author       = {Y. Hayashi and R. Setiono},
  title        = {Combining neural network predictions for medical
                  diagnosis},
  journal      = {Computers in Biology and Medicine},
  year         = 2002,
  volume       = 32,
  number       = 4,
  pages        = {237--246},
}

@ARTICLE{islam03constructive,
  author       = {Md. M. Islam and X. Yao and K. Murase},
  number       = 4,
  year         = 2003 ,
  journal      = {IEEE Transactions on Neural Networks},
  title        = {A constructive algorithm for training cooperative
                  neural network ensembles},
  month        = {July},
  pages        = {820--834},
  volume       = 14,
}

@ARTICLE{itqon01,
  author       = {Itqon and Shun'ichi Kaneko and Satoru Igarashi},
  title        = {Combining Multiple k-Nearest Neighbor Classifiers
                  Using Feature Combinations},
  journal      = {Journal of IECI (Indonesian Society on Electrical
                  Electronics, Communication and Information)},
  year         = 2000,
  volume       = 2,
  number       = 3,
  pages        = {23--31},
}

@INPROCEEDINGS{wuzhouchen02:ensembling,
  author       = {J.X.Wu and Z.H.Zhou and Z.Q.Chen},
  title        = "Ensemble of {GA} based selective neural network,
                  ensembles" ,
  booktitle    = {8th International Conference on Neural Information
                  Processing (ICONIP)},
  volume       = 3,
  pages        = {1477-1482},
  year         = 2002,
}

@INBOOK{jaakkola00variational,
  author       = {Jaakkola, T.},
  editor       = {D. Saad and M. Opper},
  title        = {Advanced Mean Field methods - Theory and Practice},
  chapter      = {Tutorial on Variational Approximation Methods},
  publisher    = {MIT Press},
  year         = 2000,
}

@ARTICLE{james03bv,
  author       = {Gareth James},
  year         = 2003,
  title        = {Variance and Bias for General Loss Functions},
  journal      = {Machine Learning},
  volume       = 51,
  pages        = {115--135},
}

@INPROCEEDINGS{jin03regularizer,
  author       = {Rong Jin and Yan Liu and Luo Si and Jaime Carbonell
                  and Alexander Hauptmann},
  title        = {A New Boosting Algorithm using Input-Dependent
                  Regularizer},
  booktitle    = {20th International Conference on Machine Learning},
  year         = 2003,
}

@INPROCEEDINGS{joshi01evaluating,
  author       = {Mahesh V. Joshi and Vipin Kumar and Ramesh
                  C. Agarwal},
  title        = {Evaluating Boosting Algorithms to Classify Rare
                  Classes: Comparison and Improvements},
  booktitle    = {{ICDM}},
  pages        = {257-264},
  year         = 2001,
  url          = {http://citeseer.nj.nec.com/joshi01evaluating.html},
}

@INPROCEEDINGS{kanamori02mixture,
  author       = {Takafumi Kanamori},
  title        = {A New Sequential Algorithm for Regression Problems
                  by using Mixture Distribution},
  booktitle    = {Int. Conf. Artif. Neur. Netw. ICANN},
  pages        = {535--540},
  year         = 2002,
  publisher    = {Springer},
  url          = {http://citeseer.nj.nec.com/573689.html},
}

@INPROCEEDINGS{khareyao02,
  author       = {Vineet Khare and Xin Yao},
  title        = {Artificial Speciation of Neural Network Ensembles},
  booktitle    = {Proc. of the 2002 UK Workshop on Computational
                  Intelligence (UKCI'02)},
  pages        = {96--103},
  year         = 2002,
  editor       = {J.A.Bullinaria},
  month        = {September},
  organization = {University of Birmingham, UK},
}

@ARTICLE{kleinberg00algorithmic,
  author       = {Eugene M. Kleinberg},
  title        = {On the Algorithmic Implementation of Stochastic
                  Discrimination},
  journal      = {IEEE Transactions on Pattern Analysis and Machine
                  Intelligence},
  volume       = 22,
  number       = 5,
  pages        = {473-490},
  year         = 2000,
}

@INPROCEEDINGS{kuncheva00independence,
  author       = {L. Kuncheva and C. Whitaker and C. Shipp and
                  R. Duin},
  title        = {Is independence good for combining classifiers},
  booktitle    = {Proceedings of the 15th International Conference on
                  Pattern Recognition, Barcelona, Spain},
  year         = 2000,
  pages        = {168-171},
}

@ARTICLE{kuncheva03limits,
  author       = {L. Kuncheva and C. Whitaker and C. Shipp and
                  R. Duin},
  title        = {Limits on the majority vote accuracy in classifier
                  fusion},
  journal      = {Pattern {A}nalysis and {A}pplications},
  year         = 2003,
  month        = {April},
  volume       = 6,
  number       = 1,
  pages        = {22--31},
}

@ARTICLE{kuncheva02sixstrategies,
  author       = {Kuncheva, L.I.},
  title        = {A Theoretical Study on Six Classifier Fusion
                  Strategies},
  journal      = {PAMI},
  volume       = 24,
  year         = 2002,
  number       = 2,
  month        = {February},
  pages        = {281-286},
}

@ARTICLE{kuncheva02switching,
  author       = {L.I. Kuncheva},
  title        = {Switching between selection and fusion in combining
                  classifiers: An experiment},
  journal      = {IEEE Transactions On Systems Man And Cybernetics},
  volume       = 32,
  number       = 2,
  pages        = {146--156},
  year         = 2002,
}

@INPROCEEDINGS{kuncheva03elusive,
  author       = {Kuncheva, L.I.},
  title        = {That {E}lusive {D}iversity in {C}lassifier
                  {E}nsembles},
  booktitle    = {First Iberian Conference on Pattern Recognition and
                  Image Analysis (IbPRIA), available as LNCS volume
                  2652},
  pages        = {1126--1138},
  year         = 2003,
}

@ARTICLE{kuncheva02generating,
  author       = {L.I. Kuncheva and R.K. Kountchev},
  title        = {Generating Classifier Outputs of Fixed Accuracy and
                  Diversity},
  journal      = {Pattern Recognition Letters},
  year         = 2002,
  number       = 23,
  pages        = {593-600},
}

@ARTICLE{kunchevawhitaker03,
  author       = {L.I. Kuncheva and C. Whitaker},
  title        = {Measures of Diversity in Classifier Ensembles},
  journal      = {Machine Learning},
  year         = 2003,
  number       = 51,
  pages        = {181--207},
}

@ARTICLE{kuncheva2004udc,
  title        = {{Using diversity in cluster ensembles}},
  author       = {Kuncheva, LI and Hadjitodorov, ST},
  journal      = {Systems, Man and Cybernetics, 2004 IEEE International Conference on},
  volume       = 2,
  year         = 2004,
}

@ARTICLE{hadjitodorov2005mdb,
  title        = {{Moderate Diversity for Better Cluster Ensembles}},
  author       = {Hadjitodorov, S.T. and Kuncheva, L.I. and Todorova, L.P.},
  journal      = {Information Fusion},
  volume       = 7,
  number       = 3,
  year         = 2006,
}

@INCOLLECTION{kuncheva01:tenmeasures,
  author       = {L.I. Kuncheva and C.J. Whitaker},
  title        = {Ten Measures of Diversity in Classifier Ensembles:
                  Limits for Two Classifiers},
  booktitle    = {IEE Workshop on Intelligent Sensor Processing},
  publisher    = {IEE},
  year         = 2001,
  month        = {February},
}

@BOOK{kunchevabook,
  author       = {Ludmila Kuncheva},
  title        = {Combining Pattern Classifiers: Methods and
                  Algorithms},
  publisher    = {Wiley Press},
  year         = 2004,
  note         = {ISBN 0-471-21078-1},
}

@TECHREPORT{kutin01stability,
  author       = {Samuel Kutin and Partha Niyogi},
  title        = {The Interaction of Stability and Weakness in
                  AdaBoost},
  institution  = {The University of Chicago},
  year         = 2001,
  number       = {TR-2001-30},
  url          = {http://www.cs.uchicago.edu/research/publications/techreports/TR-2001-30},
}

@TECHREPORT{langdon:2001:edf,
  author       = {W. B. Langdon},
  title        = {Evolutionary Data Fusion},
  institution  = {University College, London},
  year         = 2001,
  number       = {RN/01/19},
  address      = {UK},
  month        = {3 April},
  keywords     = {genetic algorithms, genetic programming, ROC},
  url          = {http://www.cs.ucl.ac.uk/staff/W.Langdon/datafusion.html},
  url          = {http://www.cs.ucl.ac.uk/staff/W.Langdon/roc},
  size         = {12 pages},
  notes        = {Distributed at 25 April 2001 Faraday meeting
                  http://www.npl.co.uk/intersect/ },
}

@INPROCEEDINGS{langdon:2001:wsc6,
  author       = {W. B. Langdon and S. J. Barrett and B. F. Buxton},
  title        = {Genetic Programming for Combining Neural Networks
                  for Drug Discovery},
  booktitle    = {Soft Computing and Industry Recent Applications},
  year         = 2001,
  editor       = {Rajkumar Roy and Mario K\"oppen and Seppo Ovaska and
                  Takeshi Furuhashi and Frank Hoffmann},
  pages        = {597--608},
  month        = {10--24 September},
  publisher    = {Springer-Verlag},
  note         = {Published 2002},
  keywords     = {genetic algorithms, genetic programming, data
                  fusion, data mining, knowledge discovery, Receiver
                  Operating Characteristics, ensemble of classifiers,
                  size fair crossover},
  isbn         = {1-85233-539-4},
  url          = {http://www.springer.de/cgi/svcat/search_book.pl?isbn=1-85233-539-4},
}

@INPROCEEDINGS{langdon03gp
  title        = {Comparison of AdaBoost and Genetic Programming for
                  combining Neural Networks for Drug Discovery},
  author       = {W. B. Langdon and S. J. Barrett and B. F. Buxton},
  booktitle    = {Applications of Evolutionary Computing,
                  EvoWorkshops2003: Evo{BIO}, Evo{COP}, Evo{IASP},
                  Evo{MUSART}, Evo{ROB}, Evo{STIM}},
  editor       = {G\"unther R.~Raidl and Stefano Cagnoni and Juan
                  Jes\'us Romero Cardalda and David W.~Corne and Jens
                  Gottlieb and Agn\`es Guillot and Emma Hart and Colin
                  G.~Johnson and Elena Marchiori and Jean-Arcady Meyer
                  and Martin Middendorf},
  volume       = 2611,
  series       = {LNCS},
  pages        = {87--98},
  address      = {University of Essex, UK},
  publisher    = {Springer-Verlag},
  publisher_address ={Berlin},
  month        = {14-16 April},
  organisation = {EvoNet},
  year         = 2003,
  keywords     = {genetic algorithms, genetic programming, adaboost,
                  drug design, Receiver Operating Characteristics
                  (ROC), ensemble of classifiers, data fusion,
                  artificial neural networks, clementine, high through
                  put screening (HTS)},
  size         = {12 pages},
  notes        = {EvoWorkshops2003},
}

@INPROCEEDINGS{langdon:2001:gROC,
  title        = {Genetic Programming for Combining Classifiers},
  author       = {W. B. Langdon and B. F. Buxton},
  pages        = {66--73},
  year         = 2001,
  publisher    = {Morgan Kaufmann},
  booktitle    = {Proceedings of the Genetic and Evolutionary
                  Computation Conference (GECCO-2001)},
  editor       = {Lee Spector and Erik D. Goodman and Annie Wu and
                  W.B. Langdon and Hans-Michael Voigt and Mitsuo Gen
                  and Sandip Sen and Marco Dorigo and Shahram Pezeshk
                  and Max H. Garzon and Edmund Burke},
  address      = {San Francisco, California, USA},
  publisher_address ={San Francisco, CA 94104, USA},
  month        = {7-11 July},
  keywords     = {genetic algorithms, genetic programming, data
                  fusion, data mining, knowledge discovery, Receiver
                  Operating Characteristics, ensemble of classifiers,
                  size fair crossover},
  isbn         = {1-55860-774-9},
  url          = {ftp://cs.ucl.ac.uk/genetic/papers/WBL_gecco2001_roc.ps.gz},
  url          = {ftp://cs.ucl.ac.uk/genetic/papers/WBL_gecco2001_roc.pdf},
  size         = {8 pages},
  abstract     = {Genetic programming (GP) can automatically fuse
                  given classifiers to produce a combined classifier
                  whose Receiver Operating Characteristics (ROC) are
                  better than scott:1998:BMVC ``Maximum Realisable
                  Receiver Operating Characteristics''
                  (MRROC). I.e. better than their convex hull. This is
                  demonstrated on artificial, medical and satellite
                  image processing bench marks.},
  notes        = {GECCO-2001 A joint meeting of the tenth
                  International Conference on Genetic Algorithms
                  (ICGA-2001) and the sixth Annual Genetic Programming
                  Conference (GP-2001) Part of spector:2001:GECCO},
}

@MISC{langdon:2002:kdmdd,
  author       = {W. B. Langdon and B. F. Buxton and S. J. Barrett},
  title        = {Combining Machine Learning techniques to Predict
                  Compounds' Cytochrome P450 High Throughput Screening
                  Inhibition},
  howpublished = {Knowledge Discovery meets Drug Discovery},
  year         = 2002,
  month        = {23 October},
  note         = {poster},
  keywords     = {genetic algorithms, genetic programming},
  url          = {ftp://cs.ucl.ac.uk/genetic/papers/wbl_kdmdd2002.pdf},
  notes,
                  {http://www.kdnet.org/workshop_overview1_bioinfoLeuven02.htm},
}

@INPROCEEDINGS{langdon02gp,
  year         = 2002,
  title        = {A Hybrid Genetic Programming Neural Network
                  Classifier for Use in Drug Discovery},
  institution  = {Department of Computer Science -- University College
                  London -- UK},
  booktitle    = {Soft Computing Systems - Design, Management and
                  Applications},
  author       = {William B. Langdon},
  abstract     = {We have shown genetic programming (GP) can
                  automatically fuse given classifiers of diverse
                  types to produce a hybrid classifier. Combinations
                  of neural networks, decision trees and Bayes
                  classifier shave been formed. On a range of
                  benchmarks the evolved multiple classifier system is
                  better than all of its components. Indeed its
                  Receiver Operating Characteristics (ROC) are better
                  than [Scott et al., 1998]s "Maximum Realisable
                  Receiver Operating Characteristics" MRROC (convex
                  hull) An important component in the drug discovery
                  is testing potential drugs for activity with P450
                  cell membrane molecules. Our technique has been used
                  in a blind trial where artificial neural networks
                  are trained by Clementine on P450 pharmaceutical
                  data. Using just the trained networks, GP
                  automatically evolves a composite classifier. Recent
                  experiments with boosting the networks will be
                  compared with genetic programming.},
}

@INPROCEEDINGS{langdon:2002:EuroGP,
  title        = {Combining Decision Trees and Neural Networks for
                  Drug Discovery},
  author       = {William B. Langdon and S. J. Barrett and
                  B. F. Buxton},
  booktitle    = {Genetic Programming, Proceedings of the 5th European
                  Conference, EuroGP 2002},
  pages        = {60--70},
  address      = {Kinsale, Ireland},
  publisher_address ={Berlin},
  month        = {3-5 April},
  year         = 2002,
  keywords     = {genetic algorithms, genetic programming, drug
                  design, Receiver Operating Characteristics (ROC),
                  ensemble of classifiers, data fusion, artificial
                  neural networks, clementine, decision trees C4.5,
                  high through put screening (HTS)},
  isbn         = {3-540-43378-3},
  size         = {10 pages},
  abstract     = {Genetic programming (GP) offers a generic method of
                  automatically fusing together classifiers using
                  their receiver operating characteristics (ROC) to
                  yield superior ensembles. We combine decision trees
                  (C4.5) and artificial neural networks (ANN) on a
                  difficult pharmaceutical data mining (KDD) drug
                  discovery application. Specifically predicting
                  inhibition of a P450 enzyme. Training data came from
                  high throughput screening (HTS) runs. The evolved
                  model may be used to predict behaviour of virtual
                  (i.e. yet to be manufactured) chemicals. Measures to
                  reduce over fitting are also described. },
  notes        = {EuroGP'2002, part of lutton:2002:GP},
}

@INPROCEEDINGS{langdon:2001:eROC,
  author       = {William B. Langdon and Bernard F. Buxton},
  title        = {Evolving Receiver Operating Characteristics for Data
                  Fusion},
  booktitle    = {Genetic Programming, Proceedings of EuroGP'2001},
  year         = 2001,
  volume       = 2038,
  pages        = {87--96},
  address      = {Lake Como, Italy},
  publisher_address ={Berlin},
  month        = {18-20 April},
  organisation = {EvoNET},
  publisher    = {Springer-Verlag},
  keywords     = {genetic algorithms, genetic programming, Data
                  Fusion, Data Mining, Knowledge Discovery, Receiver
                  Operating Characteristics, ROC, Combining
                  Classifiers},
  isbn         = {3-540-41899-7},
  url          = {http://evonet.dcs.napier.ac.uk/eurogp2001/},
  url          = {ftp://cs.ucl.ac.uk/genetic/papers/wbl_egp2001.ps.gz},
  size         = {10 pages},
  abstract     = {It has been suggested that the ``Maximum Realisable
                  Receiver Operating Characteristics'' for a
                  combination of classifiers is the convex hull of
                  their individual ROCs [Scott et al., 1998]. As
                  expected in at least some cases better ROCs can be
                  produced. We show genetic programming (GP) can
                  automatically produce a combination of classifiers
                  whose ROC is better than the convex hull of the
                  supplied classifier's ROCs.},
  notes        = {EuroGP'2001, part of miller:2001:gp},
}

@MISC{lappalainen00ensemble,
  author       = {H. Lappalainen and J. Miskin},
  title        = {Ensemble Learning},
  text         = {H. Lappalainen and J. Miskin, Ensemble Learning, in
                  M. Girolami (Ed.), Advances in Independent Component
                  Analysis, Springer, Berlin, 2000 (in press).},
  year         = 2000,
}

@ARTICLE{lee2004lob,
  title        = {{Lossless Online Bayesian Bagging}},
  author       = {Lee, H.K.H. and Clyde, M.A.},
  journal      = {Journal of Machine Learning Research},
  volume       = 5,
  pages        = {143-151},
  year         = 2004,
  publisher    = {MIT Press Cambridge, MA, USA},
}

@ARTICLE{liu00evolutionary,
  author       = {Y. Liu and X. Yao and T. Higuchi},
  title        = {Evolutionary Ensembles with Negative Correlation
                  Learning},
  journal      = {IEEE Transactions on Evolutionary Computation},
  volume       = 4,
  number       = 4,
  month        = {November},
  year         = 2000,
  url          = {http://citeseer.nj.nec.com/article/liu00evolutionary.html},
}

@INPROCEEDINGS{liuyao02decision,
  author       = {Y. Liu and X. Yao and Q. Zhao and T. Higuchi},
  title        = {An experimental comparison of neural network
                  ensemble learning methods on decision boundaries},
  booktitle    = {Proceedings of the 2002 International Joint
                  Conference on Neural Networks (IJCNN'02)},
  pages        = {221-226},
  month        = {May},
  year         = 2002,
  publisher    = {IEEE Press, Piscataway, NJ, USA},
}

@INPROCEEDINGS{liu01mutual,
  author       = {Yong Liu and Xin Yao and Qiangfu Zhao and Tetsuya
                  Higuchi},
  title        = {Evolving a Cooperative Population of Neural Networks
                  by Minimizing Mutual Information},
  booktitle    = {Proceedings of the 2001 Congress on Evolutionary
                  Computation},
  pages        = {384-389},
  year         = 2001,
  month        = {May},
  publisher    = {IEEE Press},
}

@INPROCEEDINGS{luochen02b,
  author       = {Dingsheng Luo and Ke Chen},
  title        = {On the use of statistical ensemble methods for
                  telephone-line speaker identification},
  booktitle    = {Proceedings of International Joint Conference on
                  Communications, Circuits and Systems (ICCCAS'2002)},
  year         = 2002,
  publisher    = {IEEE Press},
  address      = {Chengdu, China},
  pages        = {II904-II908},
  month        = {July},
}

@INPROCEEDINGS{luochen02a,
  author       = {Dingsheng Luo and Ke Chen},
  title        = {A comparative study of statistical ensemble methods
                  on mismatch conditions},
  booktitle    = {Proceedings of World Congress on Computational
                  Intelligence: International Joint Conference on
                  Neural Networks (WCCI 2002 and IJCNN 2002)},
  year         = 2002,
  address      = {Honolulu USA},
  publisher    = {IEEE Press},
  pages        = {59--64},
  month        = {November},
}

@ARTICLE{malzahn03approximate,
  author       = {Dorthe Malzahn and Manfred Opper},
  title        = {An Approximate Analytical Approach to Resampling
                  Averages},
  journal      = {Journal of Machine Learning Research},
  year         = 2003,
  volume       = 4,
  pages        = {1151--1173},
  month        = {December},
  url          = {http://www.jmlr.org},
}

@INBOOK{mason00functional,
  author       = {L. Mason and J. Baxter and P. L. Bartlett and
                  M. Frean},
  editor       = {A.J. Smola, P. L. Bartlett, B. Scholkopf, and
                  D. Schuurmans},
  title        = {Advances in Large Margin Classifiers : Functional
                  gradient techniques for combining hypotheses},
  publisher    = {MIT Press},
  year         = 2000,
  address      = {Cambridge, MA},
  pages        = {221--246},
}

@INPROCEEDINGS{mckay00sharing,
  author       = {Bob McKay},
  title        = {Fitness Sharing in Genetic Programming},
  pages        = {435--442},
  year         = 2000,
  publisher    = {Morgan Kaufmann},
  booktitle    = {Proceedings of the Genetic and Evolutionary
                  Computation Conference (GECCO-2000)},
  editor       = {Darrell Whitley and David Goldberg and Erick
                  Cantu-Paz and Lee Spector and Ian Parmee and
                  Hans-Georg Beyer},
  address      = {Las Vegas, Nevada, USA},
  publisher_address ="San Francisco, CA 94104, USA",
  month        = "10-12 " # jul,
  keywords     = {genetic algorithms, genetic programming},
  isbn         = {1-55860-708-0},
  notes        = {A joint meeting of the ninth International
                  Conference on Genetic Algorithms (ICGA-2000) and the
                  fifth Annual Genetic Programming Conference
                  (GP-2000) Part of whitley:2000:GECCO},
}

@INPROCEEDINGS{mckayabbass01analyzing,
  author       = {R. McKay and H. Abbass},
  title        = {Analyzing Anticorrelation in Ensemble Learning},
  booktitle    = {Proceedings of 2001 Conference on Artificial Neural
                  Networks and Expert Systems},
  year         = 2001,
  pages        = {22--27},
  address      = {Otago, New Zealand},
}

@INPROCEEDINGS{mckayabbass01rtqrt,
  author       = {Robert McKay and Hussein Abbass},
  title        = {Anticorrelation Measures in Genetic Programming},
  booktitle    = {Australasia-Japan Workshop on Intelligent and
                  Evolutionary Systems},
  pages        = {45--51},
  year         = 2001,
}

@INPROCEEDINGS{meir00localized,
  author       = {Ron Meir and Ran El-{Y}aniv and Shai Ben-{D}avid},
  title        = {Localized Boosting},
  booktitle    = {Proc. 13th Annu. Conference on Comput. Learning
                  Theory},
  publisher    = {Morgan Kaufmann, San Francisco},
  pages        = {190--199},
  year         = 2000,
  url          = {http://citeseer.nj.nec.com/511402.html},
}

@article{melville05artificial,
  author    = {Prem Melville and
               Raymond J. Mooney},
  title     = {Creating diversity in ensembles using artificial data.},
  journal   = {Information Fusion},
  volume    = {6},
  number    = {1},
  year      = {2005},
  pages     = {99-111}
}

@phdthesis{melville05thesis,
  author      = {Prem Melville},
  title       = {Creating Ensemble Diversity to Reduce Supervision},
  institution = {University of Texas at Austin}
  year        = {2005}
}

@INPROCEEDINGS{melville03decorate,
  author       = {Prem Melville and Ray Mooney},
  title        = {Constructing Diverse Classifier Ensembles Using
                  Artificial Training Examples},
  booktitle    = {Proceedings of the Eighteenth International Joint
                  Conference on Artificial Intelligence},
  pages        = {505--510},
  year         = 2003,
  address      = {Mexico},
  month        = {August},
}

@article{galor05assessing,
  author    = {Mordechai Gal-Or and
               Jerrold H. May and
               William E. Spangler},
  title     = {Assessing the predictive accuracy of diversity measures
               with domain-dependent, asymmetric misclassification costs.},
  journal   = {Information Fusion},
  volume    = {6},
  number    = {1},
  year      = {2005},
  pages     = {37-48}
}

@ARTICLE{rooney2006pes,
  title        = {{Pruning extensions to stacking}},
  author       = {Rooney, N. and Patterson, D. and Nugent, C.},
  journal      = {Intelligent Data Analysis},
  volume       = 10,
  number       = 1,
  pages        = {47-66},
  year         = 2006,
  publisher    = {IOS Press},
  abstract     = {In this paper we investigate an algorithmic extension to the technique of Stacking for regression
                  that prunes the ensemble set before application based on a consideration of the training accuracy and diversity
                  of the ensemble members. We evaluate two variants of this approach in comparison to the standard Stacking
                  algorithm, one of which is a static approach that prunes back the ensemble to the same constant size; the
                  other of which is a variable approach prunes the ensemble to an appropriate level based on measures of
                  accuracy and diversity of the ensemble members. We show that on average both techniques are robust in
                  performance to their non-pruned counterpart, while having the advantage of producing smaller and less
                  complex ensembles. In the latter respect, the static approach proved more effective, but we show that
                  the variable approach lends itself better for further optimization.},
}

@ARTICLE{dettling2003btc,
  title        = {{Boosting for tumor classification with gene expression data}},
  author       = {Dettling, M. and B{\"u}hlmann, P.},
  journal      = {Bioinformatics},
  volume       = 19,
  number       = 9,
  pages        = {1061-1069},
  year         = 2003,
}

@ARTICLE{eibl06multi,
  title        = {{Multiclass Boosting for Weak Classifiers}},
  author       = {Eibl, G. and Pfeiffer, K.P.},
  journal      = {The Journal of Machine Learning Research},
  volume       = 6,
  pages        = {189-210},
  year         = 2005,
  publisher    = {MIT Press Cambridge, MA, USA},
}

@ARTICLE{friedman2000sip,
  title        = {{Special Invited Paper. Additive Logistic Regression: A Statistical View of Boosting}},
  author       = {Friedman, J. and Hastie, T. and Tibshirani, R.},
  journal      = {The Annals of Statistics},
  volume       = 28,
  number       = 2,
  pages        = {337-374},
  year         = 2000,
  publisher    = {JSTOR},
}

@ARTICLE{torralba2004sfe,
  title        = {{Sharing features: efficient boosting procedures for multiclass object detection}},
  author       = {Torralba, A. and Murphy, KP and Freeman, WT},
  journal      = {Computer Vision and Pattern Recognition, 2004},
  volume       = 2,
}

@ARTICLE{folino2006icg,
  title        = {{Improving cooperative GP ensemble with clustering and pruning for pattern classification}},
  author       = {Folino, G. and Pizzuti, C. and Spezzano, G.},
  journal      = {Proceedings of the 8th annual conference on Genetic and evolutionary computation},
  pages        = {791-798},
  year         = 2006,
  publisher    = {ACM Press New York, NY, USA},
}

@ARTICLE{bi2006eae,
  title        = {{An evidential approach in ensembles}},
  author       = {Bi, Y. and Dubitzky, W.},
  journal      = {Proceedings of the 2006 ACM symposium on Applied computing},
  pages        = {1-6},
  year         = 2006,
  publisher    = {ACM Press New York, NY, USA},
  abstract     = {In this paper, we describe an approach to modeling the general process of combining
                  decisions involved in ensembles of classifiers as an evidential reasoning process. This work proposes
                  a novel structure, theoretical properties and manipulation mechanisms for representing classifier
                  decisions as pieces of evidence. The advantage of the representation formalism is that it not only
                  facilitates the distinguishing of trivial focal elements from important ones, resulting in the
                  improvement of the ensemble performance, but it also effectively reduces the computation-time from
                  exponential (as required in the conventional process of combining multiple pieces of evidence) to linear.
                  We have conducted a comparative analysis on the effectiveness of the proposed evidence representation
                  formalism in the text categorization domain. By comparing this method with majority voting and the previous
                  results, we also demonstrate the advantage of this novel approach in combining classifiers.},
}

@ARTICLE{mashao2006ccd,
  title        = {{Combining Classifier Decisions for Robust Speaker Identification}},
  author       = {MASHAO, D.J. and SKOSAN, M.},
  journal      = {Pattern recognition},
  volume       = 39,
  number       = 1,
  pages        = {147-155},
  year         = 2006,
  publisher    = {Elsevier Science},
}

@ARTICLE{murua02upperbound,
  author       = {Murua, A.},
  title        = {Upper Bounds for Error Rates of Linear Combinations
                  of Classifiers},
  journal      = {PAMI},
  volume       = 24,
  year         = 2002,
  number       = 5,
  month        = {May},
  pages        = {591-602},
}

@ARTICLE{nanni2006ekl,
  title        = {{An ensemble of K-local hyperplanes for predicting protein-protein interactions}},
  author       = {Nanni, L. and Lumini, A.},
  journal      = {Bioinformatics},
  volume       = 22,
  number       = 10,
  pages        = 1207,
  year         = 2006,
}

@ARTICLE{nair:jmlr02,
  title        = {Some greedy learning algorithms for sparse
                  regression and classification with mercer kernels},
  author       = {P. B. Nair and A. Choudhury and A. J. Keane},
  journal      = {Journal of Machine Learning Research},
  volume       = 3,
  pages        = {781-801},
  year         = 2002,
}

@PHDTHESIS{oza2001thesis,
  title        = {{Online Ensemble Learning}},
  author       = {Oza, N.C.},
  year         = 2001,
  school       = {UNIVERSITY of CALIFORNIA},
}

@ARTICLE{oza2001eco,
  title        = {{Experimental comparisons of online and batch versions of bagging and boosting}},
  author       = {Oza, N.C. and Russell, S.},
  journal      = {Proceedings of the seventh ACM SIGKDD international conference on Knowledge discovery and data mining},
  pages        = {359-364},
  year         = 2001,
  publisher    = {ACM Press New York, NY, USA},
}

@ARTICLE{oza2005oba,
  title        = {{Online bagging and boosting}},
  author       = {Oza, NC},
  journal      = {Systems, Man and Cybernetics, 2005 IEEE International Conference on},
  volume       = 3,
  year         = 2005,
}

@INPROCEEDINGS{pennock00normative,
  author       = {David M. Pennock and Pedrito {Maynard-Reid {II}} and
                  C. Lee Giles and Eric Horvitz},
  title        = {A Normative Examination of Ensemble Learning
                  Algorithms},
  booktitle    = {Proc. 17th International Conf. on Machine Learning},
  publisher    = {Morgan Kaufmann, San Francisco, CA},
  pages        = {735--742},
  year         = 2000,
}

@TECHREPORT{poggio02bagging,
  author       = {Tomaso Poggio and Ryan Rifkin and Sayan Mukherjee
                  and Alex Rakhlin},
  title        = {Bagging Regularizes},
  institution  = {MIT AI Lab},
  year         = 2002,
  number       = {AI Memo 2002-003, CBCL Memo 214},
}

@MISC{roli02notes,
  author       = {Fabio Roli},
  title        = {Lecture Notes: Linear Combiners for Fusion of
                  Pattern Classifiers},
  url          = {http://www.disi.unige.it/person/MasulliF/ricerca/school2002/contributions/vietri02-lect-roli.pdf},
}

@article{ruta05selection,
  author    = {Dymitr Ruta and
               Bogdan Gabrys},
  title     = {Classifier selection for majority voting.},
  journal   = {Information Fusion},
  volume    = {6},
  number    = {1},
  year      = {2005},
  pages     = {63-81}
}

@ARTICLE{ruta03sets,
  author       = {Dymitr Ruta and Bogdan Gabrys},
  title        = {Set Analysis of Coincident Errors and Its
                  Applications for Combining Classifiers},
  journal      = {Combinatorial Optimisation},
  volume       = 13,
  publisher    = {Kluwer Academic},
  year         = 2003,
}

@ARTICLE{schapiresinger00boostexter,
  author       = {R.E.~Schapire and Y.~Singer},
  title        = {BoosTexter: A boosting-based system for text
                  categorization},
  journal      = {Machine Learning},
  year         = 2000,
  volume       = 39,
  number       = {2/3},
  pages        = {135-168},
}

@INPROCEEDINGS{shan05cmi,
  author       = {Shan, C. and Gong, S. and McOwan, P.W.},
  title        = {{Conditional Mutual Information Based Boosting for Facial Expression Recognition}},
  booktitle    = {BMVC},
  year         = 2005
}

@INPROCEEDINGS{shih03bundling,
  author       = {Lawrence Shih and Jason Rennie and Yu-Han Chang and
                  David Karger},
  title        = {Text Bundling : Statistics Based Data Reduction},
  booktitle    = {20th {I}nternational {C}onference on {M}achine
                  {L}earning (ICML'03)},
  year         = 2003,
  editor       = {Tom Fawcett and Nina Mishra},
  month        = {August},
  address      = {Washington DC, USA},
}

@ARTICLE{shippkuncheva02,
  author       = {C.A. Shipp and L.I. Kuncheva},
  title        = {Relationships between combination methods and
                  measures of diversity in combining classifiers},
  journal      = {Information Fusion},
  year         = 2002,
  number       = 3,
  pages        = {135-148},
}

@INPROCEEDINGS{skurichina00role,
  author       = {M. Skurichina and R.P.W. Duin},
  title        = {The Role of Combining Rules in Bagging and Boosting},
  booktitle    = {Advances in Pattern Recognition, Proc. Joint IAPR
                  International Workshops SSPR2000 and SPR2000 Lecture
                  Notes in Computer Science, vol. 1876},
  publisher    = {Springer},
  month        = {June},
  pages        = {236-245},
  editors      = {F.J. Ferri, J.M. Inesta, A. Amin, P. Pudil},
  year         = 2000,
  address      = {Alicante, Spain},
}

@PHDTHESIS{schubert2005qca,
  title        = {{Quantifying correlation and its effects on system performance in classifier fusion}},
  author       = {Schubert, C.M.},
  year         = 2005,
  school       = {AIR FORCE INSTITUTE OF TECHNOLOGY},
  url          = {http://gradworks.umi.com/31/91/3191355.html}
}

@ARTICLE{skurichina2002bba,
  title        = {{Bagging, Boosting and the Random Subspace Method for Linear Classifiers}},
  author       = {Skurichina, M.W. and Duin, R.P.W.W.},
  journal      = {Pattern Analysis \& Applications},
  volume       = 5,
  number       = 2,
  pages        = {121-135},
  year         = 2002,
  publisher    = {Springer},
}

@INPROCEEDINGS{stainvas00,
  title        = {Blurred Face Recognition via a Hybrid Network
                  Architecture},
  author       = {I. Stainvas and N. Intrator},
  booktitle    = {ICPR},
  year         = 2000,
  volume       = 2,
  pages        = {809--812},
}

@INPROCEEDINGS{stephenson03compiler,
  author       = {Mark Stephenson and Una-May O'Reilly and Martin
                  C. Martin and Saman P. Amarasinghe},
  title        = {Genetic Programming Applied to Compiler Heuristic
                  Optimization.},
  booktitle    = {EuroGP},
  year         = 2003,
  pages        = {238-253},
}

@ARTICLE{strehl2003cek,
  title        = {{Cluster ensembles- A knowledge reuse framework for combining multiple partitions.}},
  author       = {Strehl, A. and Ghosh, J.},
  journal      = {Journal of Machine Learning Research},
  volume       = 3,
  number       = 3,
  pages        = {583-617},
  year         = 2003,
  publisher    = {MIT Press},
}

@ARTICLE{suganthan01hierarchical,
  author       = {P. N. Suganthan},
  title        = {Pattern classification using multiple hierarchical
                  overlapped self-organising maps},
  journal      = {Pattern Recognition},
  year         = 2001,
  volume       = 34,
  number       = 11,
  pages        = {2173-2179},
  month        = {November},
}

@ARTICLE{tetko02associative,
  author       = {Tetko, I. V.},
  title        = {Associative neural network},
  journal      = {Neural Processing Letters},
  volume       = 16,
  number       = 2,
  pages        = {187-199},
  abstract     = {An associative neural network (ASNN) is a
                  combination of an ensemble of the feed-forward
                  neural networks and the K-nearest neighbor
                  technique. The introduced network uses correlation
                  between ensemble responses as a measure of distance
                  among the analyzed cases for the nearest neighbor
                  technique and provides an improved prediction by the
                  bias correction of the neural network ensemble both
                  for function approximation and
                  classification. Actually, the proposed method
                  corrects a bias of a global model for a considered
                  data case by analyzing the biases of its nearest
                  neighbors determined in the space of calculated
                  models. An associative neural network has a memory
                  that can coincide with the training set. If new data
                  become available the network can provide a
                  reasonable approximation of such data without a need
                  to retrain the neural network ensemble. Applications
                  of ASNN for prediction of lipophilicity of chemical
                  compounds and classification of UCI letter and
                  satellite data set are presented. The developed
                  algorithm is available on-line at
                  http://www.virtuallaboratory.org/lab/asnn.},
  year         = 2002,
  url          = {http://cogprints.ecs.soton.ac.uk/archive/00001441/},
}

@ARTICLE{tetko02associative2,
  author       = {Tetko, I. V.},
  title        = {Neural network studies. 4. Introduction to
                  associative neural networks},
  journal      = {J Chem Inf Comput Sci},
  volume       = 42,
  number       = 3,
  pages        = {717-28.},
  keywords     = {*Neural Networks (Computer) Support,
                  Non-U.S. Gov\'t},
  abstract     = {Associative neural network (ASNN) represents a
                  combination of an ensemble of feed-forward neural
                  networks and the k-nearest neighbor technique. This
                  method uses the correlation between ensemble
                  response,, as a measure of distance amid the
                  analyzed cases for the nearest neighbor
                  technique. This provides an improved prediction by
                  the bias correction of the neural network
                  ensemble. An associative neural network has a memory
                  that can coincide with the training set. If new data
                  becomes available, the network further improves its
                  predictive ability and provides a reasonable
                  approximation of the unknown function without a need
                  to retrain the neural network ensemble, This feature
                  of the method dramatically improves its predictive
                  ability over traditional neural networks and
                  k-nearest neighbor techniques, as demonstrated using
                  several artificial data sets and a program to
                  predict lipophilicity of chemical compounds. Another
                  important feature of ASNN is the possibility to
                  interpret neural network results by analysis of
                  correlations between data cases in the space of
                  models, It is shown that analysis of such
                  correlations makes it possible to provide
                  \"property-targeted\" clustering of data. The
                  possible applications and importance of ASNN in drug
                  design and medicinal and combinatorial chemistry are
                  discussed. The method is available on-line at
                  http://www.vcclab.org/lab/asnn.},
  year         = 2002,
}

@ARTICLE{tetko01volume,
  author       = {Tetko, I. V. and Kovalishyn, V. V. and Livingstone,
                  D. J.},
  title        = {Volume learning algorithm artificial neural networks
                  for 3D QSAR studies},
  journal      = {J Med Chem},
  volume       = 44,
  number       = 15,
  pages        = {2411-20.},
  abstract     = {The current study introduces a new method, the
                  volume learning algorithm (VLA), for the
                  investigation of three-dimensional quantitative
                  structure-activity relationships (QSAR) of chemical
                  compounds. This method incorporates the advantages
                  of comparative molecular field analysis (CoMFA) and
                  artificial neural network approaches. VLA is a
                  combination of supervised and unsupervised neural
                  networks applied to solve the same problem. The
                  supervised algorithm is a feed-forward neural
                  network trained with a back-propagation algorithm
                  while the unsupervised network is a self-organizing
                  map of Kohonen. The use of both of these algorithms
                  makes it possible to cluster the input CoMFA field
                  variables and to use only a small number of the most
                  relevant parameters to correlate spatial properties
                  of the molecules with their activity. The
                  statistical coefficients calculated by the proposed
                  algorithm for cannabimimetic aminoalkyl indoles were
                  comparable to, or improved, in comparison to the
                  original study using the partial least squares
                  algorithm. The results of the algorithm can be
                  visualized and easily interpreted. Thus, VLA is a
                  new convenient tool for three-dimensional QSAR
                  studies.},
  year         = 2001,
}

@ARTICLE{tetko02asnnapp,
  author       = {Tetko, I. V. and Tanchuk, V. Y.},
  title        = {Application of associative neural networks for
                  prediction of lipophilicity in ALOGPS 2.1 program},
  journal      = {J Chem Inf Comput Sci},
  volume       = 42,
  number       = 5,
  pages        = {1136-45.},
  abstract     = {This article provides a systematic study of several
                  important parameters of the Associative Neural
                  Network (ASNN), such as the number of networks in
                  the ensemble, distance measures, neighbor functions,
                  selection of smoothing parameters, and strategies
                  for the user-training feature of the algorithm. The
                  performance of the different methods is assessed
                  with several training/test sets used to predict
                  lipophilicity of chemical compounds. The Spearman
                  rank-order correlation coefficient and Parzen-window
                  regression methods provide the best performance of
                  the algorithm. If additional user data is available,
                  an improved prediction of lipophilicity of chemicals
                  up to 2-5 times can be calculated when the
                  appropriate smoothing parameters for the neural
                  network are selected. The detected best combinations
                  of parameters and strategies are implemented in the
                  ALOGPS 2.1 program that is publicly available at
                  http://www.vcclab.org/lab/alogps.},
  year         = 2002,
}

@ARTICLE{tetko01estimation,
  author       = {Tetko, I. V. and Tanchuk, V. Y. and Kasheva,
                  T. N. and Villa, A. E.},
  title        = {Estimation of aqueous solubility of chemical
                  compounds using E-state indices},
  journal      = {J Chem Inf Comput Sci},
  volume       = 41,
  number       = 6,
  pages        = {1488-93.},
  abstract     = {The molecular weight and electrotopological E-state
                  indices were used to estimate by Artificial Neural
                  Networks aqueous solubility for a diverse set of
                  1291 organic compounds. The neural network with
                  33-4-1 neurons provided highly predictive results
                  with r(2) = 0.91 and RMS = 0.62. The used parameters
                  included several combinations of E-state indices
                  with similar properties. The calculated results were
                  similar to those published for these data by
                  Huuskonen (2000). However, in the current study only
                  E-state indices were used without need of additional
                  indices (the molecular connectivity, shape,
                  flexibility and indicator indices) also considered
                  in the previous study. In addition, the present
                  neural network contained three times less hidden
                  neurons. Smaller neural networks and use of one
                  homogeneous set of parameters provides a more robust
                  model for prediction of aqueous solubility of
                  chemical compounds. Limitations of the developed
                  method for prediction of large compounds are
                  discussed, The developed approach is available
                  online at http://www.vcclab.org/lab/alogps},
  year         = 2001,
}

@ARTICLE{tetko01prediction,
  author       = {Tetko, I. V. and Tanchuk, V. Y. and Villa, A. E.},
  title        = {Prediction of n-octanol/water partition coefficients
                  from PHYSPROP database using artificial neural
                  networks and E-state indices},
  journal      = {J Chem Inf Comput Sci},
  volume       = 41,
  number       = 5,
  pages        = {1407-21.},
  abstract     = {The molecular weight and electrotopological E-state
                  indices were used to estimate by Artificial Neural
                  Networks aqueous solubility for a diverse set of
                  1291 organic compounds. The neural network with
                  33-4-1 neurons provided highly predictive results
                  with r(2) = 0.91 and RMS = 0.62. The used parameters
                  included several combinations of E-state indices
                  with similar properties. The calculated results were
                  similar to those published for these data by
                  Huuskonen (2000). However, in the current study only
                  E-state indices were used without need of additional
                  indices (the molecular connectivity, shape,
                  flexibility and indicator indices) also considered
                  in the previous study. In addition, the present
                  neural network contained three times less hidden
                  neurons. Smaller neural networks and use of one
                  homogeneous set of parameters provides a more robust
                  model for prediction of aqueous solubility of
                  chemical compounds. Limitations of the developed
                  method for prediction of large compounds are
                  discussed. The developed approach is available
                  online at http://www.vcclab.org/lab/alogps.},
  year         = 2001,
}

@article{tsymbal05search,
  author    = {Alexey Tsymbal and
               Mykola Pechenizkiy and
               Padraig Cunningham},
  title     = {Diversity in search strategies for ensemble feature selection.},
  journal   = {Information Fusion},
  volume    = {6},
  number    = {1},
  year      = {2005},
  pages     = {83-98}
}

@ARTICLE{tino04nonlinear,
  author       = {P. Tino and I. Nabney and B.S. Williams and J. Losel
                  and Y. Sun},
  title        = {Non-linear Prediction of Quantitative
                  Structure-Activity Relationships},
  journal      = {Journal of Chemical Information and Computer
                  Sciences},
  year         = 2004,
  volume       = 44,
  number       = 5,
  pages        = {1647--1653},
}

@ARTICLE{topchy2004acp,
  title        = {{Analysis of consensus partition in cluster ensemble}},
  author       = {Topchy, AP and Law, MHC and Jain, AK and Fred, AL},
  journal      = {Data Mining, 2004. ICDM 2004. Proceedings. Fourth IEEE International Conference on},
  pages        = {225-232},
  year         = 2004,
}

@INPROCEEDINGS{tresp00generalizedbayescommittee,
  author       = {Volker Tresp},
  title        = {The Generalized Bayesian Committee Machine},
  booktitle    = {Proceedings of the Sixth ACM SIGKDD International
                  Conference on Knowledge Discovery and Data Mining,
                  KDD-2000},
  year         = 2000,
  pages        = {130-139},
  abstract     = { In this paper we introduce the Generalized Bayesian
                  Committee Machine (GBCM) for applications with large
                  data sets. In particular, the GBCM can be used in
                  the context of kernel based systems such as
                  smoothing splines, kriging, regularization networks
                  and Gaussian process regression which ---for
                  computational reasons--- are otherwise limited to
                  rather small data sets. The GBCM provides a novel
                  and principled way of combining estimators trained
                  for regression, classification, the prediction of
                  counts, the prediction of lifetimes and other
                  applications which can be derived from the
                  exponential family of distributions. We describe an
                  online version of the GBCM which only requires one
                  pass through the data set and only requires the
                  storage of a matrix of the dimension of the number
                  of query or test points. After training, the
                  prediction at additional test points only requires
                  resources dependent on the number of query points
                  but is independent of the number of training
                  data. We confirm the good scaling behavior using
                  real and experimental data sets. },
  url          = {http://www.boosting.org/papers/upload_7240_kddpaper2.ps},
}

@ARTICLE{tresp00bayescommittee,
  author       = {Volker Tresp},
  title        = {A Bayesian Committee Machine},
  journal      = {Neural Computation},
  year         = 2000,
  pages        = {2719-2741},
  volume       = 12,
  number       = 11,
  abstract     = { The Bayesian committee machine (BCM) is a novel
                  approach to combining estimators which were trained
                  on different data sets. Although the BCM can be
                  applied to the combination of any kind of estimators
                  the main foci are Gaussian process regression and
                  related systems such as regularization networks and
                  smoothing splines for which the degrees of freedom
                  increase with the number of training data. Somewhat
                  surprisingly, we find that the performance of the
                  BCM improves if several test points are queried at
                  the same time and is optimal if the number of test
                  points is at least as large as the degrees of
                  freedom of the estimator. The BCM also provides a
                  new solution for online learning with potential
                  applications to data mining. We apply the BCM to
                  systems with fixed basis functions and discuss its
                  relationship to Gaussian process
                  regression. Finally, we also show how the ideas
                  behind the BCM can be applied in a non-Bayesian
                  setting to extend the input dependent combination of
                  estimators.},
  url          = {http://www.boosting.org/papers/upload_7235_bcm5.ps},
}

@INBOOK{tresp01committeemachines,
  author       = {Volker Tresp},
  editor       = {Yu Hen Hu and Jenq-Nen Hwang},
  title        = {Handbook for Neural Network Signal Processing},
  chapter      = {Committee machines},
  publisher    = {CRC Press},
  year         = 2001,
}

@ARTICLE{tumerghosh00robust,
  author       = {K Tumer and J Ghosh},
  title        = {Robust Combining of Disparate Classifiers through
                  Order Statistics},
  journal      = {To appear in Pattern Analysis and Applications,
                  Special issue on Fusion of Multiple Classifiers},
  year         = 2002,
  number       = 2,
  volume       = 5,
  url          = {http://citeseer.nj.nec.com/592387.html},
}

@ARTICLE{ueda00optimal,
  author       = {N Ueda},
  title        = {Optimal linear combination of neural networks for
                  improving classification Performance },
  journal      = {IEEE Transactions on Pattern Analysis and Machine
                  Intelligence},
  year         = 2000,
  volume       = 22,
  number       = 2,
  pages        = {207--215},
}

@INPROCEEDINGS{valentini03baggedsvm,
  author       = {Giorgio Valentini and Thomas G. Dietterich},
  title        = {Low Bias Bagged Support Vector Machines},
  booktitle    = {20th {I}nternational {C}onference on {M}achine
                  {L}earning (ICML'03)},
  year         = 2003,
  editor       = {Tom Fawcett and Nina Mishra},
  month        = {August},
  address      = {Washington DC, USA},
}

@ARTICLE{valev01multi,
  author       = {V. Valev and A. Asaithambi},
  title        = {Multidimensional pattern recognition problems and
                  combining classifiers},
  journal      = {Pattern Recognition Letters},
  volume       = 22,
  year         = 2001,
  number       = 12,
  month        = {October},
  pages        = {1291-1297},
}

@ARTICLE{viola01boosting,
  title        = {{Rapid object detection using a boosted cascade of simple features}},
  author       = {Viola, P. and Jones, M.},
  journal      = {Proc. CVPR},
  volume       = 1,
  pages        = {511-518},
  year         = 2001,
}

@article{wang2003mcd,
  title        = {{Mining concept-drifting data streams using ensemble classifiers}},
  author       = {Wang, H. and Fan, W. and Yu, P.S. and Han, J.},
  journal      = {Proceedings of the ninth ACM SIGKDD international conference on 
                  Knowledge discovery and data mining},
  pages        = {226-235},
  year         = {2003},
  publisher    = {ACM Press New York, NY, USA}
}

@INPROCEEDINGS{wang01diversity,
  author       = {Wenjia Wang and Derek Partridge and John
                  Etherington},
  title        = {Hybrid Ensembles and Coincident-Failure Diversity},
  booktitle    = {Proceedings of the International Joint Conference on
                  Neural Networks},
  publisher    = {IEEE Press},
  month        = {July},
  pages        = {2376--2381},
  volume       = 4,
  year         = 2001,
  address      = {Washington, USA},
}

@ARTICLE{webb05nsn,
  title        = {{Not So Naive Bayes: Aggregating One-Dependence Estimators}},
  author       = {Webb, G.I. and Boughton, J.R. and Wang, Z.},
  journal      = {Machine Learning},
  volume       = 58,
  number       = 1,
  pages        = {5-24},
  year         = 2005,
  publisher    = {Springer},
  annote       = {Combining probabilistic models - similar to naive bayes.},
}

@ARTICLE{webb00multiboosting,
  author       = {Geoffrey I. Webb},
  title        = {Multi{B}oosting: {A} Technique for Combining
                  {B}oosting and {W}agging},
  journal      = {Machine Learning},
  volume       = 40,
  number       = 2,
  publisher    = {Kluwer Academic Publishers, Boston},
  pages        = {159--196},
  year         = 2000,
  url          = {http://citeseer.nj.nec.com/webb98multiboosting.html},
}

@INPROCEEDINGS{wersing02a,
  author       = {H. Wersing and E. K{\"o}rner},
  title        = {Unsupervised Learning of Combination Features for
                  Hierarchical Recognition Models},
  booktitle    = {Int. Conf. Artif. Neur. Netw. ICANN},
  year         = 2002,
  note         = {accepted},
}

@ARTICLE{wersingkorner02,
  author       = {Heiko Wersing and Edgar Korner},
  title        = {Learning Optimized Features for Hierarchical Models
                  of Invariant Object Recognition},
  journal      = {Neural Computation (to appear)},
  year         = 2002,
}

@INPROCEEDINGS{wezel00nonconformist,
  author       = {M.C. van Wezel and M.D. Out and W.A. Kosters},
  title        = {Ensembles of nonconformist neural networks},
  booktitle    = {Proceedings of the Twelfth Belgium-Netherlands
                  Artificial Intelligence Conference},
  editors      = {H. Weigand and A. van den Bosch},
  pages        = {165-172},
  year         = 2000,
}

@TECHREPORT{whitaker03examining,
  author       = {Christopher Whitaker and Ludmila Kuncheva},
  title        = {Examining the relationship between majority vote
                  accuracy and diversity in bagging and boosting},
  institution  = {School of Informatics, University of Wales, Bangor},
  year         = 2003,
  type         = {Technical Report},
  url          = {http://www.informatics.bangor.ac.uk/~kuncheva/papers/lkcw_tr.pdf},
}

@article{windeatt05measures,
  author    = {Terry Windeatt},
  title     = {Diversity measures for multiple classifier system analysis
               and design.},
  journal   = {Information Fusion},
  volume    = {6},
  number    = {1},
  year      = {2005},
  pages     = {21-36}
}

@INPROCEEDINGS{yaobrown01telecoms,
  author       = {X. Yao and M. Fischer and G. Brown},
  title        = {Neural Network Ensembles and their Application to
                  Traffic Flow Prediction in Telecommunications
                  Networks},
  booktitle    = {Proceedings of International Joint Conference on
                  Neural Networks},
  note         = {Washington DC},
  pages        = {693-698},
  publisher    = {IEEE Press},
  year         = 2001,
}

@INPROCEEDINGS{yang06select,
  author       = {Ying Yang and Geoff Webb and Jesus Cerquides and
                  Kevin Korb and Janice Boughton and Kai Ming Ting},
  title        = {To Select or To Weigh: A Comparative Study of Model
                  Selection and Model Weighing for SPODE Ensembles},
  booktitle    = {17th European Conference on Machine Learning (ECML)},
  year         = 2006,
}

@INPROCEEDINGS{yang05ensemble,
  author       = {Ying Yang and Kevin Korb and Kai Ming Ting and Geoff Webb},
  title        = {Ensemble Selection for SuperParent-One-Dependence Estimators},
  booktitle    = {18th Australian Joint Conference on Artificial Intelligence},
  year         = 2005,
}

@ARTICLE{zhouwutang02ensembling,
  author       = {Z.-H. Zhou, J. Wu, and W. Tang},
  title        = {Ensembling neural networks: Many could be better
                  than all},
  journal      = {Artificial Intelligence},
  volume       = 137,
  number       = {1-2},
  pages        = {239-263},
  year         = 2002,
}

@ARTICLE{zhou02extracting,
  author       = {Z.-H. Zhou, Y. Jiang, and S.-F. Chen},
  title        = {Extracting Symbolic Rules from Trained Neural
                  Network Ensembles},
  journal      = {AI Communications, 2003, 16(1): 3-15},
  volume       = 16,
  number       = 1,
  pages        = {3-15},
  year         = 2003,
  url          = {http://citeseer.nj.nec.com/zhou03extracting.html},
}

@ARTICLE{zenobi01using,
  author       = {Gabriele Zenobi and P{\'a}draig Cunningham},
  title        = {Using Diversity in Preparing Ensembles of
                  Classifiers Based on Different Feature Subsets to
                  Minimize Generalization Error},
  journal      = {Lecture Notes in Computer Science},
  volume       = 2167,
  pages        = {576--587},
  year         = 2001,
}

@ARTICLE{zhang2004onb,
  title        = {{The optimality of naive Bayes}},
  author       = {Zhang, H.},
  journal      = {17th International FLAIRS conference, Miami Beach, May},
  pages        = {17-19},
  year         = 2004,
}

@INPROCEEDINGS{zhou02ensembling,
  author       = {Z.H. Zhou and J.Wu and W.Tang and Z.Q. Chen},
  title        = "Selectively ensembling neural classifiers" ,
  booktitle    = {International Joint Conference on Neural Networks},
  volume       = 2,
  page         = {1411-1415},
  year         = 2002,
}

@ARTICLE{zhu2006ecn,
  title        = {{Effective classification of noisy data streams with attribute-oriented dynamic classifier selection}},
  author       = {Zhu, X. and Wu, X. and Yang, Y.},
  journal      = {Knowledge and Information Systems},
  volume       = 9,
  number       = 3,
  pages        = {339-363},
  year         = 2006,
  publisher    = {Springer},
}
