


#PRE2000ENSEMBLEPAPERS

@TECHREPORT{ali95comparison,
  author       = {K. Ali},
  year         = 1995,
  title        = {A comparison of methods for learning and combining
                  evidence from multiple models},
  institution  = {University of California, Irvine, Dept. of
                  Information and Computer Sciences},
  number       = {UCI TR \#95-47},
  abstract     = {Most previous work on multiple models has been done
                  on a few domains. We present a comparsion of three
                  ways of learning multiple models on 29 data sets
                  from the UCI repository. The methods are bagging,
                  $k$-fold partition learning and stochastic
                  search. By using 29 data sets of various kinds -
                  artificial data sets, artificial data sets with
                  noise, molecular-biology and real-world noisy data
                  sets - we are able to draw robust experimental
                  conclusions about the kinds of data sets for which
                  each learning method works best. We also compare
                  four evidence combination methods (Uniform Voting,
                  Bayesian Combination, Distribution Summation and
                  Likelihood Combination) and characterize the kinds
                  of data sets for which each method works best.},
}

@ARTICLE{avnimelech99boostedregression,
  author       = {R.~Avnimelech and N.~Intrator},
  title        = {Boosting Regression Estimators},
  journal      = {Neural Computation},
  year         = 1999,
  volume       = 11,
  pages        = {491--513},
}

@ARTICLE{avnimelechintrator99boosted,
  pages        = {475-490},
  author       = {Ran Avnimelech and Nathan Intrator},
  year         = 1999,
  journal      = {Neural Computation},
  volume       = 11,
  title        = {Boosted Mixtures of Experts: An Ensemble Learning
                  Scheme},
}

@ARTICLE{parmanto96,
  author       = {B.Parmanto and P.W.Munro and H.R.Doyle},
  title        = {Improving Committee Diagnosis with Resampling
                  Techniques},
  editor       = {D.S.Touretzky and M.C.Mozer and M.E.Hesselmo},
  volume       = 8,
  journal      = {Advances in Neural Information Processing Systems},
  year         = 1996,
  pages        = {882--888},
  publisher    = {The {MIT} Press},
}

@ARTICLE{back93evolution,
  author       = {T. B{\"{a}}ck and H.-P. Schwefel},
  title        = {An overview of evolutionary algorithms for parameter
                  optimization},
  journal      = {Evolutionary Computation},
  year         = 1993,
  volume       = 1,
  number       = 1,
  pages        = {1-23},
}

@ARTICLE{batesgranger69,
  author       = {J. M. Bates and C. W. J. Granger},
  title        = {The combination of forecasts},
  journal      = {Operations Research Quarterly},
  year         = 1969,
  number       = 20,
  pages        = {451-468},
  annote       = {The use of combination techniques in financial forecasting.}
}

@ARTICLE{battiti1994democracy,
  author       = {Roberto Battiti and Anna Maria Colla},
  title        = {Democracy in Neural Nets: Voting Schemes for
                  Classification},
  journal      = {Neural Networks},
  year         = 1994,
  volume       = 7,
  number       = 4,
  pages        = {691--707},
  abstract     = {Discusses some possible ways to combine the outputs
                  of a set of neural network classifiers to reach a
                  combined decision with a higher performance in terms
                  of lower rejection rates and/or better accuracy
                  rates. The methods considered range from the
                  requirement of a complete agreement among the
                  individual classifications to election schemes based
                  on the distribution of votes collected by the
                  different classes. In addition, the rejection rules
                  based on the different output classes can be
                  complemented by rules that also consider the
                  information in the individual output vectors, with
                  the possibility of using threshold requirements and
                  that of averaging the different vectors. Although
                  the Bayesian framework and some probabilistic
                  assumptions provide useful indications about the
                  potential advantage of different combination
                  schemes, the combined performance ultimately depends
                  on the joint probability distribution of the
                  outputs, and it can be estimated by joining the
                  results of different nets on the same test set. The
                  combination methods are very flexible, they permit a
                  straightforward cooperation of neural and
                  traditional recognizers, and they are appropriate in
                  a development environment where experiments are
                  performed with different kinds of nets and features
                  for a selected application. From the authors'
                  experiments in the field of handwritten digit
                  recognition (up to a total of more than 50000
                  characters), they found that the use of a small
                  number of nets (two to three) with a sufficiently
                  large uncorrelation in their mistakes reaches a
                  combined performance that is significantly higher
                  than the best obtainable from the individual nets,
                  with a negligible effort after starting from a pool
                  of networks produced in the development phase of an
                  application. In particular, for a real-world OCR
                  application, the best accuracy increase is about
                  half the increase in the rejection rate, so that
                  accuracies of the order of 99.5% can be reached by
                  rejecting less than 5% of the patterns. This
                  performance is significant for real applications.},
}

@ARTICLE{bauer99empirical,
  author       = {E. Bauer and R. Kohavi},
  title        = {An Empirical Comparison of Voting Classification
                  Algorithms: Bagging, Boosting, and Variants},
  journal      = {Machine Learning},
  year         = 1999,
  volume       = 36,
  number       = {1,2},
}

@ARTICLE{bishop95training,
  author       = {Chris M. Bishop},
  title        = {Training with Noise is Equivalent to {T}ikhonov
                  Regularization},
  journal      = {Neural Computation},
  volume       = 7,
  number       = 1,
  pages        = {108--116},
  year         = 1995,
  url          = {http://citeseer.nj.nec.com/bishop94training.html},
}

@BOOK{bishop95book,
  author       = {Christopher M. Bishop},
  title        = {Neural Networks for Pattern Recogntion},
  publisher    = {Oxford University Press},
  year         = 1995,
  isbn         = {0-19-853864-2},
}

@MISC{uci,
  author       = {C.L. Blake and C.J. Merz},
  year         = 1998,
  title        = {{UCI} Repository of machine learning databases},
  url          = {http://www.ics.uci.edu/$\sim$mlearn/MLRepository.html},
  institution  = {University of California, Irvine, Dept. of
                  Information and Computer Sciences},
}

@INPROCEEDINGS{bottou91framework,
  author       = {L{\'e}on Bottou and Patrick Gallinari},
  title        = {A Framework for the Cooperation of Learning
                  Algorithms},
  booktitle    = {Advances in Neural Information Processing Systems},
  volume       = 3,
  publisher    = {Morgan Kaufmann Publishers, Inc.},
  editor       = {Richard P. Lippmann and John E. Moody and David
                  S. Touretzky},
  pages        = {781--788},
  year         = 1991,
}

@TECHREPORT{breiman96arcing,
  author       = {Leo Breiman},
  title        = {Bias, Variance, and Arcing Classifiers},
  institution  = {Statistics Department, Berkeley},
  year         = 1996,
  number       = 460,
}

@ARTICLE{breiman96bagging,
  author       = {Leo Breiman},
  title        = {Bagging Predictors},
  journal      = {Machine Learning},
  volume       = 24,
  number       = 2,
  pages        = {123-140},
  year         = 1996,
}

@TECHREPORT{breiman98randomizing,
  author       = {Leo Breiman},
  title        = {Randomizing Outputs to increase prediction accuracy},
  institution  = {Statistics Department, University of California},
  month        = {May},
  year         = 1998,
  type         = {Technical Report},
  number       = 518,
  url          = {http://www.boosting.org/papers/Bre98.pdf},
}

@TECHREPORT{breiman99random,
  author       = {Leo Breiman},
  year         = 1999,
  title        = {Random Forests Random Features},
  institution  = {University of California, Berkley (Dept of
                  Statistics)},
  number       = 567,
}

@INPROCEEDINGS{brodleylane96,
  author       = {C. Brodley and T. Lane},
  title        = {Creating and exploiting coverage and diversity},
  booktitle    = {AAAI-96 Workshop Integrating Multiple Learned
                  Models},
  year         = 1996,
}

@TECHREPORT{carneytuning99,
  author       = {John Carney and Padraig Cunningham},
  title        = {Tuning diversity in bagged neural network ensembles},
  institution  = {Trinity College Dublin},
  year         = 1999,
  number       = {TCD-CS-1999-44},
}

@INPROCEEDINGS{chan95arbiter,
  author       = {Philip K.~Chan and Salvatore J.~Stolfo},
  title        = {Learning Arbiter and Combiner Trees from Partitioned
                  Data for Scaling Machine Learning},
  booktitle    = {The first international conference on knowledge
                  discovery and data mining, KDD '95},
  year         = 1995,
  pages        = {39-45},
}

@INPROCEEDINGS{chan96local,
  author       = {Philip K.~Chan and Salvatore J.~Stolfo},
  title        = {Sharing Learned Models among Remote Database
                  Partitions by Local Meta-learning},
  booktitle    = {Proc. Second Intl. Conf. on Knowledge Discovery \&
                  Data Mining},
  year         = 1996,
  pages        = {2--7},
}

@ARTICLE{chan98accuracy,
  author       = {Philip K.~Chan and Salvatore J.~Stolfo},
  title        = {On the Accuracy of Meta-learning for Scalable Data
                  Mining,},
  journal      = {Journal of Intelligent Information Systems},
  year         = 1997,
  volume       = 8,
  pages        = {5--28},
}

@ARTICLE{clemen89forecasts,
  author       = {R. Clemen},
  year         = 1989,
  title        = {Combining forecast: A review and annotated
                  bibliography},
  journal      = {International Journal on Forecasting},
  volume       = 5,
  pages        = {559--583},
}

@INPROCEEDINGS{costa95bayesian,
  author       = {M. Costa and E. Filippi and E. Pasero},
  title        = {Artificial neural network ensembles: a Bayesian
                  standpoint},
  booktitle    = {Proceedings of the 7th Italian Workshop on Neural
                  Nets},
  pages        = {39-57},
  publisher    = {World Scientific},
  year         = 1995,
  editor       = {M. Marinaro and R. Tagliaferri},
  abstract     = {Pooled estimates naturally arise from the Bayesian
                  framework as the elected approach to both regression
                  and classification problems whenever optimality in
                  the average sense is concerned. In contrast,
                  selection of a single 'best' member appears to be a
                  somewhat crude approximation where some important
                  features of the underlying model are
                  discarded. However, the elegant and powerful full
                  fledged formalism carries a very high computational
                  complexity. Some practical implementations relevant
                  to artificial neural network ensembles are therefore
                  reviewed that make things more tractable while
                  keeping the same generality.},
}

@ARTICLE{hoeting99bma,
  author       = {D. Hoeting, J. A.and Madigan and C.T. Raftery,
                  A.E.and Volinsky},
  title        = {Bayesian model averaging: A tutorial},
  journal      = {Statistical Science},
  year         = 1999,
  volume       = 44,
  number       = 4,
  pages        = {382--417},
  url          = {http://www.stat.washington.edu/www/research/online/hoeting1999.pdf},
  url          = {http://citeseer.nj.nec.com/context/1052498/0},
}

@ARTICLE{darwenyao97,
  author       = {Paul J. Darwen and Xin Yao},
  title        = {Speciation as Automatic Categorical Modularization},
  journal      = {{IEEE} {T}rans. on {E}volutionary {C}omputation},
  volume       = 1,
  number       = 2,
  pages        = {100--108},
  year         = 1997,
}

@TECHREPORT{darwenyao95,
  year         = 1995,
  author       = {Paul Darwen and Xin Yao},
  institution  = {University of New South Wales},
  number       = {CS 8/95},
  title        = {How Good is Fitness Sharing with a Scaling Function},
  month        = {April},
}

@INCOLLECTION{darwenyao96,
  publisher    = {Springer-Verlag},
  author       = {Paul Darwen and Xin Yao},
  title        = {Every niching method has its niche: fitness sharing
                  and implicit sharing compared},
  year         = 1996 ,
  booktitle    = {Proc. of Parallel Problem Solving from Nature (PPSN)
                  IV - Lecture Notes in Computer Science 1141},
}

@ARTICLE{deb99multiobjective,
  author       = {Kalyanmoy Deb},
  title        = {Multi-objective Genetic Algorithms: Problem
                  Difficulties and Construction of Test Problems},
  journal      = {Evolutionary Computation},
  volume       = 7,
  number       = 3,
  pages        = {205-230},
  year         = 1999,
}

@ARTICLE{devroye79distribution,
  author       = {L. Devroye and T. Wagner},
  title        = {Distribution-free Performance Bounds for Potential
                  Function Rules},
  journal      = {IEEE Transactions on Information Theory},
  year         = 1979,
  volume       = 25,
  number       = 5,
  pages        = {601--604},
}

@INPROCEEDINGS{dietterichbakiri91:errorcorrecting,
  author       = {T. G. Dietterich and G. Bakiri},
  title        = {Error-correcting output codes: a general method for
                  improving multiclass inductive learning programs},
  booktitle    = {Proceedings of the Ninth {AAAI} National Conference
                  on Artificial Intelligence},
  publisher    = {AAAI Press},
  address      = {Menlo Park, CA},
  editor       = {Dean, T. L. and McKeown, K.},
  pages        = {572--577},
  year         = 1991,
}

@ARTICLE{dietterich98approximate,
  author       = {Thomas G. Dietterich},
  title        = {Approximate Statistical Test For Comparing
                  Supervised Classification Learning Algorithms},
  journal      = {Neural Computation},
  volume       = 10,
  number       = 7,
  pages        = {1895-1923},
  year         = 1998,
  url          = {citeseer.nj.nec.com/dietterich98approximate.html},
  abstract     = {This paper reviews five approximate statistical
                  tests for determining whether one learning algorithm
                  out-performs another on a particular learning
                  task. These tests are compared experimentally to
                  determine their probability of incorrectly detecting
                  a difference when no difference exists (type I
                  error). Two widely-used statistical tests are shown
                  to have high probability of Type I error in certain
                  situations and should never be used. These tests are
                  (a) a test for the difference of two proportions and
                  (b) a paired-differences t test based on taking
                  several random train/test splits. A third test, a
                  paired-differences t test based on 10-fold
                  cross-validation, exhibits somewhat elevated
                  probability of Type I error. A fourth test,
                  McNemar's test, is shown to have low Type I
                  error. The fifth test is a new test, 5x2cv, based on
                  5 iterations of 2-fold cross-validation. Experiments
                  show that this test also has acceptable Type I
                  error.},
}

@INPROCEEDINGS{domingos97,
  title        = {Why Does Bagging Work? {A} Bayesian Account and its
                  Implications},
  author       = {P.~Domingos},
  pages        = 155,
  booktitle    = {Proceedings of the Third International Conference on
                  Knowledge Discovery and Data Mining ({KDD}-97)},
  year         = 1997,
  editor       = {David Heckerman and Heikki Mannila and Daryl
                  Pregibon and Ramasamy Uthurusamy},
  publisher    = {AAAI Press},
  url          = {http://www.boosting.org/papers/Dom97.ps.gz},
  ps           = {http://www.boosting.org/papers/Dom97.ps},
  psgz         = {http://www.boosting.org/papers/Dom97.ps.gz},
  pdf          = {http://www.boosting.org/papers/Dom97.pdf},
}

@INPROCEEDINGS{domingos98occams,
  author       = {Pedro Domingos},
  title        = {Occam's Two Razors: The Sharp and the Blunt},
  booktitle    = {Proceedings of the Fourth International Conference
                  on Knowledge Discovery and Data Mining},
  publisher    = {AAAI Press},
  year         = 1998,
}

@ARTICLE{drucker94boosting,
  author       = {Harris Drucker and Corinna Cortes and L. D. Jackel
                  and Yann LeCun and Vladimir Vapnik},
  title        = {Boosting and Other Ensemble Methods},
  type         = {Letter},
  journal      = {Neural Computation},
  volume       = 6,
  number       = 6,
  pages        = {1289--1301},
  year         = 1994,
  abstract     = {Compares the performance of three types of neural
                  network-based ensemble techniques to that of a
                  single neural network. The ensemble algorithms are
                  two versions of boosting and committees of neural
                  networks trained independently. For each of the four
                  algorithms, we experimentally determine the test and
                  training error curves in an optical character
                  recognition (OCR) problem as both a function of
                  training set size and computational cost, using
                  three architectures. We show that a single machine
                  is best for small training set sizes, while for
                  large training set sizes, some version of boosting
                  is best. However, for a given computational cost,
                  boosting is always best. Furthermore, we show a
                  surprising result for the original boosting
                  algorithm, namely that as the training set size
                  increases, the training error decreases until it
                  asymptotes to the test error rate. This has
                  potential implications in the search for better
                  training algorithms. },
}

@INCOLLECTION{edelmanintrator97,
  author       = {S. Edelman and N. Intrator},
  title        = {Learning as Extraction of Low-Dimensional
                  Representations},
  booktitle    = {Mechanisms of Perceptual Learning},
  publisher    = {Academic Press},
  year         = 1997,
  editor       = {D. Medlin, R. Goldstone, P. Schyns},
}

@BOOK{efron93bootstrap,
  author       = {B. Efron and R. Tibshirani},
  title        = {An Introduction to the Bootstrap},
  publisher    = {Chapman and Hall},
  year         = 1993,
}

@ARTICLE{elkan1997ban,
  title        = {{Boosting and Naive Bayesian Learning}},
  author       = {Elkan, C.},
  journal      = {proceeding of KDD-97, New Port beach, CA},
  year         = 1997
}

@INPROCEEDINGS{fan99adacost,
  author       = {Wei Fan and Salvatore J. Stolfo and Junxin Zhang and
                  Philip K. Chan},
  title        = {Ada{C}ost: misclassification cost-sensitive
                  boosting},
  booktitle    = {Proc. 16th International Conf. on Machine Learning},
  publisher    = {Morgan Kaufmann, San Francisco, CA},
  pages        = {97--105},
  year         = 1999,
  url          = {http://citeseer.nj.nec.com/fan99adacost.html},
}

@ARTICLE{feder94entropy,
  author       = {Meir Feder and Neri Merhav},
  title        = {Relations between entropy and error probability},
  journal      = {IEEE Transactions on Information Theory},
  volume       = 40,
  number       = 1,
  year         = 1994,
  pages        = 259,
}

@INPROCEEDINGS{feraud98ensemblemodular,
  author       = {Rapha{\"e}l Feraud and Olivier Bernier},
  title        = {Ensemble and Modular Approaches for Face Detection:
                  {A} Comparison},
  booktitle    = {Advances in Neural Information Processing Systems},
  volume       = 10,
  year         = 1998,
  publisher    = {The {MIT} Press},
  editor       = {Michael I. Jordan and Michael J. Kearns and Sara
                  A. Solla},
}

@ARTICLE{freund99short,
  author       = {Y. Freund and R. Schapire},
  title        = {A short introduction to boosting},
  journal      = {Journal of Japanese Society for Artificial
                  Intelligence},
  year         = 1999,
  pages        = {771--780},
  volume       = 14,
  number       = 5,
  url          = {http://citeseer.nj.nec.com/freund99short.html},
}

@INPROCEEDINGS{freundschapire96experiments,
  author       = {Yoav Freund and Robert E. Schapire},
  title        = {Experiments with a new boosting algorithm},
  booktitle    = {Proceedings of the 13th International Conference on
                  Machine Learning},
  publisher    = {Morgan Kaufmann},
  year         = 1996,
  pages        = {148--156},
  abstract     = {In an earlier paper, we introduced a new 'boosting'
                  algorithm called AdaBoost which, theoretically, can
                  be used to significantly reduce the error of any
                  learning algorithm that consistently generates
                  classifiers whose performance is a little better
                  than random guessing. We also introduced the related
                  notion of a 'pseudo-loss' which is a method for
                  forcing a learning algorithm of multi-label concepts
                  to concentrate on the labels that are hardest to
                  discriminate. In this paper, we describe experiments
                  we carried out to assess how well AdaBoost with and
                  without pseudo-loss, performs on real learning
                  problems. We performed two sets of experiments. The
                  first set compared boosting to Breiman's 'bagging'
                  method when used to aggregate various classifiers
                  (including decision trees and single attribute-value
                  tests). We compared the performance of the two
                  methods on a collection of machine-learning
                  benchmarks. In the second set of experiments, we
                  studied in more detail the performance of boosting
                  using a nearest-neighbor classifier on an OCR
                  problem.},
}

@TECHREPORT{friedman99bagging,
  author       = {J. Friedman and P. Hall},
  title        = {On Bagging and Nonlinear Estimation - available
                  online at
                  http://citeseer.nj.nec.com/friedman99bagging.html},
  institution  = {Stanford University},
  year         = 1999,
  url          = {http://citeseer.nj.nec.com/friedman99bagging.html},
}

@ARTICLE{friedman91mars,
  author       = {J.H. Friedman},
  title        = {Multivariate Adaptive Regression Splines},
  journal      = {Annals of Statistics},
  year         = 1991,
  volume       = 19,
  pages        = {1--141},
}

@TECHREPORT{friedman96,
  author       = {J.H. Friedman},
  title        = {Bias, Variance, 0-1 Loss and the Curse of
                  Dimensionality},
  institution  = {Stanford University},
  year         = 1996,
}

@ARTICLE{geman92,
  author       = {S. Geman and E. Bienenstock and R. Doursat},
  title        = {Neural Networks and the Bias/Variance Dilemma},
  type         = {Letter},
  journal      = {Neural Computation},
  volume       = 4,
  number       = 1,
  pages        = {1--58},
  year         = 1992,
  abstract     = {Feedforward neural networks trained by error
                  backpropagation are examples of nonparametric
                  regression estimators. We present a tutorial on
                  nonparametric inference and its relation to neural
                  networks, and we use the statistical viewpoint to
                  highlight strengths and weaknesses of neural
                  models. We illustrate the main points with some
                  recognition. experiments involving artificial data
                  as well as handwritten numerals. In way of
                  conclusion, we suggest that current-generation
                  feedforward neural networks are largely inadequate
                  for difficult problems in machine perception and
                  machine learning, regardless of
                  parallel-versus-serial hardware or other
                  implementation issues. Furthermore, we suggest that
                  the fundamental challenges in neural modeling are
                  about representation rather than learning per
                  se. This last point is supported by additional
                  experiments with handwritten numerals.},
}

@ARTICLE{girosi95regularization,
  author       = {Federico Girosi and Michael Jones and Tomaso Poggio},
  title        = {Regularization Theory and Neural Networks
                  Architectures},
  journal      = {Neural Computation},
  volume       = 7,
  number       = 2,
  pages        = {219--269},
  year         = 1995,
  url          = {http://citeseer.nj.nec.com/girosi95regularization.html},
}

@ARTICLE{granger89combining,
  pages        = {167--174},
  author       = {C.W.J. Granger},
  year         = 1989,
  journal      = {Journal of Forecasting},
  volume       = 8,
  title        = {Combining Forecasts -- Twenty Years Later},
}

@INPROCEEDINGS{guerrasalcedo99genetic,
  author       = {Cesar Guerra-Salcedo and Darrell Whitley},
  title        = {Genetic Approach to Feature Selection for Ensemble
                  Creation},
  booktitle    = {Proceedings of the Genetic and Evolutionary
                  Computation Conference},
  volume       = 1,
  month        = {13-17},
  publisher    = {Morgan Kaufmann},
  address      = {Orlando, Florida, USA},
  editor       = {Wolfgang Banzhaf and Jason Daida and Agoston
                  E. Eiben and Max H. Garzon and Vasant Honavar and
                  Mark Jakiela and Robert E. Smith},
  isbn         = {1-55860-611-4},
  pages        = {236--243},
  year         = 1999,
  url          = {citeseer.ist.psu.edu/531553.html},
}

@ARTICLE{hansen99combining,
  author       = {J.V. Hansen},
  title        = {Combining Predictors: Comparison of Five Meta
                  Machine Learning Methods},
  journal      = {Information Sciences},
  volume       = 119,
  number       = {1-2},
  pages        = {91-105},
  year         = 1999,
}

@ARTICLE{hansensalamon90,
  volume       = 12,
  number       = 10,
  title        = {Neural Network Ensembles},
  author       = {Lars Kai Hansen and Peter Salamon},
  journal      = {IEEE Transactions on Pattern Analysis and Machine
                  Intelligence},
  year         = 1990,
  pages        = {993-1001},
}

@MISC{haselsteiner99dynamic,
  author       = {E. Haselsteiner},
  title        = {Dynamic targets - adapting supervised learning to
                  time series classification},
  text         = {In Proceedings of the International Joint Conference
                  on Neural Networks IJCNN'99, Washington D.C., IEEE
                  Press.},
  year         = 1999,
}

@PHDTHESIS{hashem93thesis,
  author       = {Sherif Hashem},
  title        = {{O}ptimal {L}inear {C}ombinations of {N}eural
                  {N}etworks},
  school       = {School of Industrial Engineering, University of
                  Purdue},
  year         = 1993,
}

@ARTICLE{hashem97optimal,
  author       = {Sherif Hashem},
  title        = {{O}ptimal {L}inear {C}ombinations of {N}eural
                  {N}etworks},
  journal      = {Neural Networks},
  volume       = 10,
  number       = 4,
  month        = {August},
  pages        = {599-614},
  year         = 1997,
}

@ARTICLE{hashem95optimal,
  author       = {Sherif Hashem and Bruce Schmeiser},
  title        = {Improving Model Accuracy Using Optimal Linear
                  Combinations of Trained Neural Networks},
  journal      = {IEEE Transactions on Neural Networks},
  type         = {Letter},
  year         = 1995,
  volume       = 6,
  number       = 3,
  pages        = {792--794},
  month        = may,
  abstract     = {Neural network (NN) based modeling often requires
                  trying multiple networks with different
                  architectures and training parameters in order to
                  achieve an acceptable model accuracy. Typically,
                  only one of the trained networks is selected as
                  'best' and the rest are discarded. The authors
                  propose using optimal linear combinations (OLC's) of
                  the corresponding outputs on a set of NN's as an
                  alternative to using a single network. Modeling
                  accuracy is measured by mean squared error (MSE)
                  with respect to the distribution of random
                  inputs. Optimality is defined by minimizing the MSE,
                  with the resultant combination referred to as
                  MSE-OLC. The authors formulate the MSE-OLC problem
                  for trained NN's and derive two closed-form
                  expressions for the optimal combination-weights. An
                  example that illustrates significant improvement in
                  model accuracy as a result of using MSE-OLC's of the
                  trained networks is included.},
}

@ARTICLE{ho98subspaces,
  author       = {T.K. Ho},
  title        = {The Random Subspace Method for Constructing Decision
                  Forests},
  journal      = {IEEE Trans. on Pattern Analysis and Machine
                  Intelligence},
  year         = 1998,
  volume       = 20,
  number       = 8,
  pages        = {832--844},
  month        = {August},
}

@ARTICLE{ho94mcs,
  author       = {Tin Kam Ho and Jonathan J.~Hull and Sargur
                  N.~Srihari},
  title        = {Decision Combination in Multiple Classifier Systems},
  journal      = {Pattern Analysis and Machine Intelligence},
  year         = 1994,
  volume       = 16,
  number       = 1,
  pages        = {66--75},
  month        = {January},
}

@INPROCEEDINGS{huang94nn,
  author       = {Y S Huang and C Y Suen},
  title        = {A Method of Combining Multiple Classifiers - A
                  Neural Network Approach},
  booktitle    = {Proceedings of the 12th International Conference on
                  Pattern Recognition and Computer Vision},
  year         = 1994,
  pages        = {473-475},
  address      = {Jerusalem, Israel},
}

@ARTICLE{huang95numerals,
  author       = {Y.S.~Huang and C.Y.~Suen},
  title        = {A Method of Combining Multiple Experts for the
                  Recognition of Unconstrained Handwritten Numerals},
  journal      = {Pattern Analysis and Machine Intelligence},
  year         = 1995,
  volume       = 17,
  number       = 1,
  pages        = {90--94},
  month        = {January},
}

@ARTICLE{husmeier98overfitting,
  author       = {Husmeier D., Althoefer K.},
  title        = {Modelling conditional probabilities with network
                  committees: how overfitting can be useful},
  journal      = {Neural Network World},
  year         = 1998,
  volume       = 8,
  numver       = 4,
  pages        = {417--439},
}

@ARTICLE{intratoredelman96,
  author       = {N. Intrator and S. Edelman},
  title        = {Making a Low-Dimensional Representation Suitable for
                  Diverse Tasks},
  journal      = {Connection Science : Special Issue on Reuse of
                  Neural Networks Through Transfer},
  year         = 1996,
  volume       = 8,
  number       = 2,
  pages        = {205-224},
}

@ARTICLE{jacobsjordan91,
  author       = {R. A. Jacobs and M. I. Jordan and S. J. Nowlan and
                  G. E. Hinton},
  title        = {Adaptive Mixtures of Local Experts},
  type         = {Letter},
  journal      = {Neural Computation},
  volume       = 3,
  number       = 1,
  pages        = {79--87},
  year         = 1991,
  abstract     = {We present a new supervised learning procedure for
                  systems composed of many separate networks, each of
                  which learns to handle a subset of the complete set
                  of training cases. The new procedure can be viewed
                  either as a modular version of a multilayer
                  supervised network, or as an associative version of
                  competitive learning. It therefore provides a new
                  link beetween these two apparently different
                  approaches. We demonstrate that the learning
                  procedure divides up a voewel discrimination task
                  into appropriate subtasks, each of which can be
                  solved by a very simple expert network.},
}

@INBOOK{jacobs99mixturesofx,
  author       = {R.A. Jacobs and M.A. Tanner},
  chapter      = {Mixtures of {X}},
  editor       = {A.J. Sharkey},
  title        = {Combining Articial Neural Nets},
  publisher    = {Springer-Verlag, London},
  year         = 1999,
}

@ARTICLE{jacobs97analyses,
  pages        = {369--383},
  author       = {Robert Jacobs},
  year         = 1997,
  journal      = {Neural Computation},
  volume       = 9,
  title        = {Bias-Variance Analyses of Mixture-of-Experts
                  Architectures},
}

@ARTICLE{jacobsjordanbarto91,
  pages        = {219-250},
  title        = {Task decomposition through competition in a modular
                  connectionist architecture - the What and Where
                  vision tasks},
  volume       = 15,
  year         = 1991,
  author       = {Robert A. Jacobs and Michael I. Jordan and Andrew
                  G. Barto},
  journal      = {Cognitive Science},
}

@INPROCEEDINGS{jelonek96replicated,
  author       = {J. Jelonek},
  title        = {Generalization Capability of Homogeneous Voting
                  Classifier Based on Partially Replicated Data},
  series       = {Integrating Multiple Learned Models for Improving
                  and Scaling Machine Learning Algorithms Workshop},
  booktitle    = {AAAI'96},
  year         = 1996,
  note         = {Portland, OR},
}

@ARTICLE{jima97weak,
  author       = {C. Ji and S. Ma},
  title        = {Combinations of Weak Classifiers},
  journal      = {IEEE Trans. on Neural Networks, Special Issue on
                  Neural Networks and Pattern Recognition},
  volume       = 8,
  year         = 1997,
  month        = {January},
  pages        = {32--42},
}

@INPROCEEDINGS{jimenez98dynamically,
  author       = {D. Jimenez and N. Walsh},
  title        = {Dynamically weighted ensemble neural networks for
                  classification},
  booktitle    = {Proceedings of the International Joint Conference on
                  Neural Networks},
  year         = 1998,
  url          = {http://citeseer.nj.nec.com/jimenez98dynamically.html},
}

@ARTICLE{jordan99introduction,
  author       = {Michael I. Jordan and Zoubin Ghahramani and Tommi
                  Jaakkola and Lawrence K. Saul},
  title        = {An Introduction to Variational Methods for Graphical
                  Models},
  journal      = {Machine Learning},
  volume       = 37,
  number       = 2,
  pages        = {183-233},
  year         = 1999,
}

@ARTICLE{jordan94hierarchical,
  author       = {Michael I. Jordan and Robert A. Jacobs},
  title        = {Hierarchical Mixtures of Experts and the {EM}
                  Algorithm},
  journal      = {Neural Computation},
  year         = 1994,
  volume       = 6,
  pages        = {181--214},
  class        = {nn, learning},
  abstract     = {We present a tree-structured architecture for
                  supervised learning. The statistical model
                  underlying the architecture is a hierarchical
                  mixture model in which both the mixture coefficients
                  and the mixture components are generalized linear
                  models (GLIM). Learning is treated as a maximum
                  likelihood problem; in particular, we present an
                  Expectation-Maximization (EM) algorithm for
                  adjusting the parameters of the architecture. We
                  develop an online learning algorithm in which the
                  parameters are updated incrementally. Comparative
                  simulation results are presented in the robot
                  dynamics domain.},
}

@INPROCEEDINGS{kang95learning,
  author       = {Kukjin Kang and Jong-Hoon Oh},
  title        = {Learning by a Population of Perceptrons},
  booktitle    = {Computational Learning Theory},
  pages        = {297-300},
  year         = 1995,
  url          = {http://citeseer.nj.nec.com/kang97learning.html},
}

@INPROCEEDINGS{kang97statistical,
  author       = {Kukjin Kang and Jong-Hoon Oh},
  title        = {Statistical Mechanics of the Mixture of Experts},
  booktitle    = {Advances in Neural Information Processing Systems},
  volume       = 9,
  publisher    = {The {MIT} Press},
  editor       = {Michael C. Mozer and Michael I. Jordan and Thomas
                  Petsche},
  pages        = 183,
  year         = 1997,
  url          = {http://citeseer.nj.nec.com/kang97statistical.html},
}

@ARTICLE{kleinberg90stochastic,
  author       = {E. M. Kleinberg},
  title        = {Stochastic Discrimination},
  journal      = {Annals of Mathematics and Artificial Intelligence},
  volume       = 1,
  year         = 1990,
}

@INPROCEEDINGS{kleinberg93pattern,
  author       = {E. M. Kleinberg and T. K. Ho},
  title        = {Pattern Recognition by Stochastic Modeling},
  booktitle    = {Proc. of the 3rd Workshop on Frontiers in
                  Handwriting Recognition},
  month        = {May},
  address      = {Buffalo, New York},
  pages        = {175--183},
  year         = 1993,
}

@INPROCEEDINGS{kohavi96bias,
  author       = {Ron Kohavi and David H. Wolpert},
  title        = {Bias Plus Variance Decomposition for Zero-One Loss
                  Functions},
  booktitle    = {Machine Learning: Proceedings of the Thirteenth
                  International Conference},
  publisher    = {Morgan Kaufmann},
  editor       = {Lorenza Saitta},
  pages        = {275--283},
  year         = 1996,
  abstract     = {We present a bias-variance decomposition of expected
                  misclassification rate, the most commonly used loss
                  function in supervised classification learning. The
                  bias-variance decomposition for quadratic loss
                  functions is well known and serves as an important
                  tool for analyzing learning algorithms, yet no
                  decomposition was offered for the more commonly used
                  zero-one (misclassification) loss functions until
                  the work of Kong and Dietterich (1995) and Breiman
                  (1996). Their decomposition suffers from some major
                  shortcomings though (e.g., potentially negative
                  variance), which our decomposition avoids. We show
                  that, in practice, the naive frequency-based
                  estimation of the decomposition terms is by itself
                  biased and show how to correct for this bias. We
                  illustrate the decomposition on various algorithms
                  and datasets from the UCI repository.},
}

@INPROCEEDINGS{kongdietterich95correcting,
  author       = {E. B. Kong and T. G. Dietterich},
  title        = {Error-Correcting Output Coding Corrects Bias and
                  Variance},
  booktitle    = {Proceedings of the 12th International Conference on
                  Machine Learning},
  publisher    = {Morgan Kaufmann},
  year         = 1995,
  pages        = {313--321},
  comment      = {Tahoe Cite, CA},
  abstract     = {Previous research has shown that a technique called
                  error-correcting output coding (ECOC) can
                  dramatically improve the classification accuracy of
                  supervised learning algorithms that learn to
                  classify data points into one of k>>2 classes. This
                  paper presents an investigation of why the ECOC
                  technique works, particularly when employed with
                  decision-tree learning algorithms. It shows that the
                  ECOC method, like any form of voting or committee,
                  can reduce the variance of the learning
                  algorithm. Furthermore, unlike methods that simply
                  combine multiple runs of the same learning
                  algorithm, ECOC can correct errors caused by the
                  bias of the learning algorithm. Experiments show
                  that this bias correction ability relies on the
                  non-local behavior of C4.5.},
}

@ARTICLE{kovalishyn98cascade,
  author       = {Kovalishyn, V. V. and Tetko, I. V. and Luik,
                  A. I. and Kholodovych, V. V. and Villa, A. E. P. and
                  Livingstone, D. J.},
  title        = {Neural network studies. 3. Variable selection in the
                  cascade-correlation learning architecture},
  journal      = {Journal of Chemical Information & Computer Sciences},
  volume       = 38,
  number       = 4,
  pages        = {651-659},
  abstract     = {Pruning methods for feed-forward artificial neural
                  networks trained by the cascade-correlation learning
                  algorithm are proposed. The cascade-correlation
                  algorithm starts with a small network and
                  dynamically adds new nodes until the analyzed
                  problem has been solved. This feature of the
                  algorithm removes the requirement to predefine the
                  architecture of the neural network prior to network
                  training. The developed pruning methods are used to
                  estimate the importance of large sets of initial
                  variables for quantitative structure-activity
                  relationship studies and simulated data sets. The
                  calculated results are compared with the performance
                  of fixed-size back-propagation neural networks and
                  multiple regression analysis and are carefully
                  validated using different training/test set
                  protocols, such as leave-one-out and full
                  cross-validation procedures. The results suggest
                  that the pruning methods can be successfully used to
                  optimize the set of variables for the
                  cascade-correlation learning algorithm neural
                  networks. The use of variables selected by the
                  elaborated methods provides an improvement of neural
                  network prediction ability compared to that
                  calculated using the unpruned sets of variables.},
  year         = 1998,
}

@BOOK{koza92book,
  author       = {John R. Koza},
  title        = {Genetic Programming: On the Programming of Computers
                  by Means of Natural Selection},
  publisher    = {MIT Press},
  year         = 1992,
  isbn         = {0-262-11170-5},
}

@ARTICLE{kroghsollich97:statistical_mechanics,
  title        = {Statistical mechanics of ensemble learning},
  author       = {A. Krogh and P. Sollich},
  journal      = {Physical Review E},
  year         = 1997,
  volume       = 55,
  number       = {1PtB},
  pages        = {811--825},
  abstract     = {Within the context of learning a rule from examples,
                  we study the general characteristics of learning,
                  with ensembles. The generalization performance
                  achieved by a simple model ensemble of linear
                  students is calculated exactly in the thermodynamic
                  limit of a large number of input components and
                  shows a surprisingly rich behavior. Our main
                  findings are the following. For learning in large
                  ensembles, it is advantageous to use
                  underregularized students, which actually overfit
                  the training data. Globally optimal generalization
                  performance can be obtained by choosing the training
                  set sizes of the students optimally. For smaller
                  ensembles, optimization of the ensemble weights can
                  yield significant improvements in ensemble
                  generalization performance, in particular if the
                  individual students are subject to noise in the
                  training process. Choosing students with a wide
                  range of regularization parameters makes this
                  improvement robust against changes in the unknown
                  level of corruption of the training data.},
}

@ARTICLE{kroghvedelsby95,
  author       = {A. Krogh and J. Vedelsby},
  title        = {Neural Network Ensembles, Cross Validation, and
                  Active Learning},
  journal      = {NIPS},
  year         = 1995,
  volume       = 7,
  pages        = {231--238},
  abstract     = {The learning of continuous-valued functions using
                  neural network ensembles (committees) can give
                  improved accuracy, a reliable estimation of the
                  generalization error, and active learning. The
                  ambiguity is defined as the variation of the output
                  of ensemble members averaged over unlabeled data, so
                  it quantifies the disagreement among the
                  networks. We discuss how to use the ambiguity in
                  combination with cross-validation to give a reliable
                  estimate of the ensemble generalization error, and
                  how this type of ensemble cross-validation can
                  sometimes improve performance. We show how to
                  estimate the optimal weights of the ensemble members
                  using unlabeled data. By a generalization of
                  query-by-committee, we show how the ambiguity can be
                  used to select new training data to be labeled in an
                  active learning scheme.},
}

@BOOK{laplace1818,
  author       = {P. S. de Laplace},
  title        = {Deuxieme supplement a la theorie analytique des
                  probabilites},
  publisher    = {Paris, Gauthier-Villars},
  year         = 1818,
  note         = {Reprinted (1847) in Oeuvres Completes de Laplace,
                  vol. 7},
  annote       = {First known observation that combining probabilistic classifiers can help.},
}

@ARTICLE{leblanc96combining,
  author       = {Michael LeBlanc and Robert Tibshirani},
  title        = {Combining Estimates in Regression and
                  Classification},
  journal      = {Journal of the American Statistical Association},
  volume       = 91,
  number       = 436,
  pages        = 1641,
  year         = 1996,
  url          = {http://citeseer.nj.nec.com/leblanc93combining.html},
}

@ARTICLE{lehto95,
  volume       = 7,
  author       = {Mikko Lehtokangas and Jukka Saarinen and Kimmo Kaski
                  and Pentti Huuhtanen},
  year         = 1995,
  pages        = {983-999},
  number       = 5,
  title        = {Initializing Weights of a Multilayer Perceptron
                  Network by Using the Orthogonal Least Squares
                  Algorithm},
  journal      = {Neural Computation},
}

@ARTICLE{liaomoody99,
  author       = {Yuansong Liao and John Moody},
  title        = {Constructing Heterogeneous Committees Using Input
                  Feature Grouping},
  journal      = {Advances in Neural Information Processing Systems},
  year         = 1999,
  volume       = 12,
}

@PHDTHESIS{liu98thesis,
  author       = {Y. Liu},
  title        = {Negative Correlation Learning and Evolutionary
                  Neural Network Ensembles},
  school       = {University College, The University of New South
                  Wales, Australian Defence Force Academy, Canberra,
                  Australia},
  year         = 1998,
}

@ARTICLE{liuyao97negatively,
  title        = {Negatively Correlated Neural Networks can Produce
                  Best Ensembles},
  author       = {Y. Liu and X. Yao},
  number       = {3/4},
  year         = 1997,
  journal      = {Australian Journal of Intelligent Information
                  Processing Systems},
  pages        = {176--185},
  volume       = 4,
}

@INPROCEEDINGS{liuyao98towards,
  month        = {February},
  author       = {Yong Liu and Xin Yao},
  year         = 1998 ,
  booktitle    = {Proceedings of International Symposium on Artificial
                  Life and Robotics (AROB)},
  pages        = {265-268},
  title        = {Towards Designing Neural Network Ensembles by
                  Evolution},
}

@ARTICLE{liuyao99:ensemblelearningvia,
  author       = {Yong Liu and Xin Yao},
  title        = {Ensemble learning via negative correlation},
  journal      = {Neural Networks},
  volume       = 12,
  number       = 10,
  pages        = {1399--1404},
  year         = 1999,
}

@INPROCEEDINGS{maclinshavlik95weights,
  author       = {R. Maclin and J. W. Shavlik},
  title        = {Combining the Predictions of Multiple Classifiers:
                  Using Competitive Learning to Initialize Neural
                  Networks},
  booktitle    = {Proceedings of the 14th International Joint
                  Conference on Artificial Intelligence, Montreal,
                  Canada},
  year         = 1995,
  pages        = {524-530},
  abstract     = {The primary goal of inductive learning is to
                  generalize well-that is, induce a function that
                  accurately produces the correct output for future
                  inputs. Hansen and Salamon (1990) showed that, under
                  certain assumptions, combining the predictions of
                  several separately trained neural networks will
                  improve generalization. One of their key assumptions
                  is that the individual networks should be
                  independent in the errors they produce. In the
                  standard way of performing backpropagation this
                  assumption may be violated, because the standard
                  procedure is to initialize network weights in the
                  region of weight space near the origin. This means
                  that backpropagation's gradient-descent search may
                  only reach a small subset of the possible local
                  minima. In this paper we present an approach to
                  initializing neural networks that uses competitive
                  learning to intelligently create networks that are
                  originally located far from the origin of weight
                  space, thereby potentially increasing the set of
                  reachable local minima. We report experiments on two
                  real-world datasets where combinations of networks
                  initialized with our method generalize better than
                  combinations of networks initialized the traditional
                  way.},
}

@INPROCEEDINGS{maclin97empirical,
  author       = {Richard Maclin and David Opitz},
  title        = {An Empirical Evaluation of Bagging and Boosting},
  booktitle    = {{AAAI}/{IAAI}},
  pages        = {546-551},
  year         = 1997,
  url          = {http://citeseer.nj.nec.com/maclin97empirical.html},
}

@MISC{mak97combining,
  author       = {B. Mak},
  title        = {Combining ANNs to improve phoneme recognition},
  text         = {B. Mak. Combining ANNs to improve phoneme
                  recognition. ICASSP, 4:3253--3256, 1997.},
  year         = 1997,
  url          = {http://citeseer.nj.nec.com/mak97combining.html},
}

@INBOOK{mandler88:combining,
  author       = {E. Mandler and J. Schuermann},
  chapter      = {Combining the Classification Results of independent
                  classifiers based on the Dempster/Shafer theory of
                  evidence},
  title        = {Pattern Recognition and Artificial Intelligence},
  editor       = {E.S.Gelsema and L.N.Kanal},
  publisher    = {North Holland, Amsterdam},
  pages        = {381--393},
  year         = 1988,
}

@ARTICLE{mani91portfolio,
  author       = {G. Mani},
  title        = {Lowering variance of decisions by using artificial
                  network portfolios},
  type         = {Letter},
  journal      = {Neural Computation},
  volume       = 3,
  number       = 4,
  pages        = {484--486},
  year         = 1991,
}

@INPROCEEDINGS{margineantu97pruning,
  author       = {Dragos D. Margineantu and Thomas G. Dietterich},
  title        = {Pruning adaptive boosting},
  booktitle    = {Proc. 14th International Conference on Machine
                  Learning},
  publisher    = {Morgan Kaufmann},
  pages        = {211--218},
  year         = 1997,
  url          = {http://citeseer.nj.nec.com/margineantu97pruning.html},
}

@ARTICLE{markowitz52,
  title        = {Portfolio Selection},
  author       = {H. Markowitz},
  journal      = {Journal of Finance},
  volume       = 7,
  issue        = 1,
  month        = {March},
  year         = 1952,
  annote       = {Shows that a linear combination (ensemble)
                  of individual regressors obey the
                  bias-variance-covariance decomposition.
                  For this paper, Markowitz received the
                  1990 Nobel Prize for Economics.},
}

@BOOK{mcclellandrumelhart86,
  author       = {J. McClelland and D. Rumelhart},
  title        = {Parallel Distributed Processing},
  publisher    = {MIT Press},
  year         = 1986,
}

@ARTICLE{meir95bias,
  author       = {Ronny Meir},
  title        = {Bias, Variance and the Combination of Least Squares
                  Estimators},
  pages        = {295--302},
  editor       = {G. Tesauro and D. Touretzky and T. Leen},
  volume       = 7,
  booktitle    = {Advances in Neural Information Processing Systems},
  year         = 1995,
  publisher    = {The {MIT} Press},
  abstract     = {We consider the effect of combining several least
                  squares estimators on the expected performance of a
                  regression problem. Computing the exact bias and
                  variance curves as a function of the sample size we
                  are able to quantitatively compare the effect of the
                  combination on the bias and variance separately, and
                  thus on the expected error which is the sum of the
                  two. Our exact calculations, demonstrate that the
                  combination of estimators is particularly useful in
                  the case where the data set is small and noisy and
                  the function to be learned is unrealizable. For
                  large data sets the single estimator produces
                  superior results. Finally, we show that by splitting
                  the data set into several independent parts and
                  training each estimator on a different subset, the
                  performance can in some cases be significantly
                  improved.},
}

@PHDTHESIS{merz98thesis,
  author       = {C.J. Merz},
  title        = {Classification and Regression by Combining Models},
  school       = {University of California, Irvine},
  year         = 1998,
  abstract     = {Two novel methods for combining predictors are
                  introduced in this thesis; one for the task of
                  regression, and the other for the task of
                  classification. The goal of combining the
                  predictions of a set of models is to form an
                  improved predictor. This dissertation demonstrates
                  how a combining scheme can rely on the stability of
                  the consensus opinion and, at the same time,
                  capitalize on the unique contributions of each
                  model. An empirical evaluation reveals that the new
                  methods consistently perform as well or better than
                  existing combining schemes for a variety of
                  prediction problems. The success of these algorithms
                  is explained empirically and analytically by
                  demonstrating how they adhere to a set of
                  theoretical and heuristic guidelines. A byproduct of
                  the empirical investigation is the evidence that
                  existing combining methods fail to satisfy one or
                  more of the guidelines defined. The new combining
                  approaches satisfy these criteria by relying upon
                  Singular Value Decomposition as a tool for filtering
                  out the redundancy and noise in the predictions of
                  the learn models, and for characterizing the areas
                  of the example space where each model is
                  superior. The SVD-based representation used in the
                  new combining methods aids in avoiding sensitivity
                  to correlated predictions without discarding any
                  learned models. Therefore, the unique contributions
                  of each model can still be discovered and
                  exploited. An added advantage of the combining
                  algorithms derived in this dissertation is that they
                  are not limited to models generated by a single
                  algorithm; they may be applied to model sets
                  generated by a diverse collection of machine
                  learning and statistical modeling methods.},
}

@ARTICLE{merz99using,
  author       = {Christopher J. Merz},
  title        = {Using Correspondence Analysis to Combine
                  Classifiers},
  journal      = {Machine Learning},
  volume       = 36,
  number       = {1-2},
  pages        = {33-58},
  year         = 1999,
}

@INPROCEEDINGS{merz97combining,
  author       = {Christopher J. Merz and Michael J. Pazzani},
  title        = {Combining Neural Network Regression Estimates with
                  Regularized Linear Weights},
  booktitle    = {Advances in Neural Information Processing Systems},
  volume       = 9,
  publisher    = {The {MIT} Press},
  editor       = {Michael C. Mozer and Michael I. Jordan and Thomas
                  Petsche},
  pages        = 564,
  year         = 1997,
}

@INBOOK{minsky90multiple,
  author       = {Marvin Minsky},
  editor       = {Patrick H. Winston},
  title        = {Artificial Intelligence at MIT : Expanding
                  Frontiers},
  chapter      = {Logical vs. Analogical or Symbolic vs. Connectionist
                  or Neat vs Scruffy},
  publisher    = {MIT Press},
  year         = 1990,
  volume       = 1,
  annote       = {Minsky comments on the benefits of combining multiple representations.}
}

@TECHREPORT{moerland99dynaboost,
  author       = {P.~Moerland and E.~Mayoraz},
  title        = {DynaBoost: Combining Boosted Hypotheses in a Dynamic
                  Way},
  institution  = {IDIAP},
  year         = 1999,
  number       = {RR 99-09},
  address      = {Switzerland},
  month        = {May},
  url          = {http://www.boosting.org/papers/MoeMay99.pdf},
}

@ARTICLE{montgomeryfriedman93,
  author       = {D.C. Montgomery and D.J. Friedman},
  title        = {Prediction Using Regression Models with
                  Multicollinear Predictor Variables},
  journal      = {IIE Transactions},
  year         = 1993,
  volume       = 25,
  number       = 3,
  pages        = {73--85},
  abstract     = {Linear regression models are widely used for
                  forecasting and prediction of new observations from
                  the underlying modeled process. The article explores
                  the use of regression models in this context when
                  the regressor or predictor variables exhibit
                  multicollinearity, or near-linear dependence. It is
                  shown that multicollinearity can severely impact the
                  predictive performance of a regression model and
                  that biased estimation methods can be an effective
                  countermeasure when multicollinearity is
                  present. Several biased estimation methods are
                  described and evaluated, including a new method for
                  selecting the biasing parameter in ordinary ridge
                  regression. A simulation study is performed to
                  provide some guidelines for the choice of an
                  estimation method. },
}

@INPROCEEDINGS{murata98bias,
  author       = {N. Murata},
  title        = {Bias of estimators and regularization terms},
  booktitle    = {Proceedings of Workshop on Information-Based
                  Induction Sciences},
  pages        = {87--94},
  year         = 1998,
  month        = {July},
  url          = {http://citeseer.nj.nec.com/murata98bias.html},
}

@INBOOK{murphy97exploring,
  author       = {Patrick M. Murphy and Michael J. Pazzani},
  title        = {Exploring the decision forest: an empirical
                  investigation of Occam's razor in decision tree
                  induction},
  booktitle    = {Computational Learning Theory and Natural Learning
                  Systems},
  volume       = {IV: Making Learning Systems Practical},
  publisher    = {MIT Press},
  pages        = {171--187},
  year         = 1997,
  url          = {http://citeseer.nj.nec.com/murphy94exploring.html},
}

@BOOK{nilsson65,
  author       = {N.J.Nilsson},
  title        = {Learning Machines: Foundations of Trainable
                  Pattern-Classifying Systems},
  publisher    = {McGraw-Hill},
  year         = 1965,
}

@ARTICLE{naftaly97optimal,
  author       = {Ury Naftaly and Nathan Intrator and David Horn},
  year         = 1997,
  volume       = 8,
  title        = {Optimal Ensemble Averaging of Neural Networks},
  month        = {May},
  number       = 3 ,
  pages        = {283--296},
  journal      = {Network},
}

@INBOOK{ohkang97,
  author       = {Jong-Hoon Oh and Kookjin Kang},
  editor       = {K.Y.M.Wong and D.Yan},
  title        = {Theoretical Aspects of Neural Computation: A
                  Multidisciplinary Perspective},
  chapter      = {Experts or Ensemble? A statistical Mechanics of
                  Multiple Neural Network Approaches},
  publisher    = {Springer, Heidelberg},
  pages        = {81--92},
  year         = 1997,
}

@MISC{opitz99genetic,
  author       = {D. Opitz and J. Shavlik},
  title        = {A genetic algorithm approach for creating neural
                  network ensembles},
  text         = {In A.J.C. Sharkey, editor, Combining Articial Neural
                  Nets, pages 79-99. Springer-Verlag, London, 1999.},
  year         = 1999,
}

@INPROCEEDINGS{opitz99feature,
  author       = {David Opitz},
  title        = {Feature Selection for Ensembles},
  booktitle    = {Proceedings of 16th National Conference on
                  Artificial Intelligence (AAAI)},
  pages        = {379-384},
  year         = 1999,
}

@ARTICLE{opitzmaclin99:popular,
  title        = {Popular Ensemble Methods: An Empirical Study},
  journal      = {Journal of Artificial Intelligence Research},
  volume       = 11 ,
  year         = 1999,
  pages        = {169-198},
  author       = {David Opitz and Richard Maclin},
}

@ARTICLE{opitz96:generating,
  author       = {David W. Opitz and Jude W. Shavlik},
  title        = {Generating Accurate and Diverse Members of a
                  Neural-Network Ensemble},
  pages        = {535--541},
  journal      = {NIPS},
  editor       = {David S. Touretzky and Michael C. Mozer and Michael
                  E. Hasselmo},
  volume       = 8,
  year         = 1996,
  publisher    = {The {MIT} Press},
  abstract     = {Neural-network ensembles have been shown to be very
                  accurate classification techniques. Previous work
                  has shown that an effective ensemble should consist
                  of networks that are not only highly correct, but
                  ones that make their errors on different parts of
                  the input space as well. Most existing techniques,
                  however, only indirectly address the problem of
                  creating such a set of networks. In this paper we
                  present a technique called ADDEMUP that uses genetic
                  algorithms to directly search for an accurate and
                  diverse set of trained networks. ADDEMUP works by
                  first creating an initial population, then uses
                  genetic operators to continually create new
                  networks, keeping the set of networks that are as
                  accurate as possible while disagreeing with each
                  other as much as possible. Experiments on three DNA
                  problems show that ADDEMUP is able to generate a set
                  of trained networks that is more accurate than
                  several existing approaches. Experiments also show
                  that ADDEMUP is able to effectively incorporate
                  prior knowledge, if available, to improve the
                  quality of its ensemble.},
}

@INPROCEEDINGS{ormoneit96improved,
  author       = {Dirk Ormoneit and Volker Tresp},
  title        = {Improved Gaussian Mixture Density Estimates Using
                  Bayesian Penalty Terms and Network Averaging},
  booktitle    = {Advances in Neural Information Processing Systems},
  volume       = 8,
  publisher    = {The {MIT} Press},
  editor       = {David S. Touretzky and Michael C. Mozer and Michael
                  E. Hasselmo},
  pages        = {542--548},
  year         = 1996,
}

@MISC{orr95regularisation,
  author       = {M. Orr},
  title        = {Regularisation in the Selection of Radial Basis
                  Function Centres},
  text         = {M. J. L. Orr, Regularisation in the Selection of
                  Radial Basis Function Centres, Neural Computation,
                  Vol. 7, pp. 606-623, 1995.},
  year         = 1995,
  url          = {citeseer.nj.nec.com/orr95regularisation.html},
}

@TECHREPORT{tumer99decimated,
  author       = {N. Oza and K. Tumer},
  title        = {Dimensionality Reduction through Classifier
                  Ensembles},
  institution  = {NASA Ames Labs},
  year         = 1999,
  number       = {NASA-ARC-IC-1999-126},
}

@ARTICLE{partridge96network,
  author       = {D. Partridge},
  title        = {Network Generalization Differences Quantified},
  journal      = {Neural Networks},
  volume       = 9,
  number       = 2,
  pages        = {263--271},
  year         = 1996,
  url          = {http://citeseer.nj.nec.com/partridge94network.html},
}

@ARTICLE{partridge96:engineering,
  author       = {D. Partridge and W. B. Yates},
  title        = {Engineering Multiversion Neural-Net Systems},
  journal      = {Neural Computation},
  volume       = 8,
  number       = 4,
  pages        = {869--893},
  year         = 1996,
}

@INPROCEEDINGS{nips90:Pearlmutter-Rosenfeld,
  author       = {Barak A. Pearlmutter and Ronald Rosenfeld},
  title        = {Chaitin-{K}olmogorov Complexity and Generalization
                  in Neural Networks},
  pages        = {925--931},
  url          = {http://nips.djvuzone.org/djvu/nips03/0925.djvu},
  booktitle    = {Advances in Neural Information Processing Systems 3},
  publisher    = {Morgan Kaufmann},
  year         = 1991,
}

@INCOLLECTION{perronecooper93,
  author       = {M. P. Perrone and L. N. Cooper},
  title        = {When networks disagree: Ensemble methods for hybrid
                  neural networks},
  editor       = {R. J. Mammone},
  booktitle    = {Artificial Neural Networks for Speech and Vision},
  address      = {London},
  pages        = {126--142},
  year         = 1993,
}

@PHDTHESIS{perrone93improving,
  author       = {M.P. Perrone},
  title        = {Improving Regression Estimation: Averaging Methods
                  for Variance Reduction with Extensions to General
                  Convex Measure Optimization},
  year         = 1993,
  school       = {Brown University, Institute for Brain and Neural
                  Systems},
}

@ARTICLE{prechelt98automatic,
  author       = {Lutz Prechelt},
  title        = {Automatic early stopping using cross validation:
                  quantifying the criteria},
  journal      = {Neural Networks},
  volume       = 11,
  number       = 4,
  pages        = {761--767},
  year         = 1998,
  url          = {http://citeseer.nj.nec.com/prechelt98automatic.html},
}

@INPROCEEDINGS{powalka95,
  author       = {R Powalka, N Sherkat, R Whitrow},
  title        = {Multiple recognizer combination topologies},
  booktitle    = {Proceedings of the Seventh Biannual Conference of
                  the International Graphonomics Society},
  pages        = {128--129},
  year         = 1995,
  month        = {August},
  organization = {University of Western Ontario},
  note         = {ISBN 0-921121-14-8},
}

@ARTICLE{rahman98,
  author       = {A.F.R. Rahman and M.C. Fairhurst},
  title        = {An evaluation of multi-expert configurations for the
                  recognition of handwritten numerals},
  journal      = {Pattern Recognition},
  year         = 1998,
  number       = 9,
  pages        = {1255--1273},
}

@ARTICLE{intratorraviv96:noise,
  volume       = 8,
  pages        = {355-372},
  author       = {Yuval Raviv and Nathan Intrator},
  year         = 1996,
  title        = {Bootstrapping with Noise: An Effective
                  Regularisation Technique},
  journal      = {Connection Science},
}

@ARTICLE{rogova94combining,
  author       = {Galina Rogova},
  title        = {Combining the Results of Neural Network Classifiers},
  journal      = {Neural Networks},
  year         = 1994,
  volume       = 7,
  number       = 5,
  pages        = {777--781},
  class        = {nn, learning},
}

@ARTICLE{rosen96:decorrelated,
  volume       = 8,
  number       = {3 and 4},
  title        = {Ensemble Learning using Decorrelated Neural
                  Networks},
  author       = {B.E. Rosen},
  journal      = {Connection Science - Special Issue on Combining
                  Artificial Neural Networks: Ensemble Approaches},
  year         = 1996,
  pages        = {373--384},
  abstract     = {We describe a decorrelation network training method
                  for improving the quality of regression learning in
                  'ensemble' neural networks (NNs) that are composed
                  of linear combinations of individual NNs. In this
                  method, individual networks are trained by
                  backpropagation not only to reproduce a desired
                  output, but also to have their errors linearly
                  decorrelated with the other networks. Outputs from
                  the individual networks are then linearly combined
                  to produce the output of the ensemble network. We
                  demonstrate the performances of decorrelated network
                  training on learning the 'three-parity' logic
                  function, a noisy sine function and a
                  one-dimensional non-linear function, and compare the
                  results with the ensemble networks composed of
                  independently trained individual networks (without
                  decorrelation training). Empirical results show that
                  when individual networks are forced to be
                  decorrelated with one another the resulting ensemble
                  NNs have lower mean squared errors than the ensemble
                  networks having independently trained individual
                  networks. This method is particularly applicable
                  when there is insufficient data to train each
                  individual network on disjoint subsets of training
                  patterns.},
}

@ARTICLE{ruck90bayesperceptron,
  author       = {D. W. Ruck and S. K. Rogers and M. Kabrisky and
                  M. E. Oxley and B. W. Suter},
  title        = {The multilayer perceptron ass an approximation to a
                  Bayes optimal discrimant function},
  journal      = {IEEE Transactions on Neural Networks},
  type         = {Letter},
  year         = 1990,
  volume       = 1,
  number       = 4,
  pages        = {296--298},
}

@INPROCEEDINGS{saborin93digit,
  author       = {Michael Saborin and Amar Mitiche and Danny Thomas
                  and George Nagy},
  title        = {Classifier Combination for Hand-Printed Digit
                  Recognition},
  pages        = {163--166},
  booktitle    = {Proceedings of the Second International Conference
                  on Document Analysis and Recognition},
  year         = 1993,
  organization = {IEEE},
  address      = {Japan},
}

@ARTICLE{schapiresinger99confidence,
  author       = {R.E.~Schapire and Y.~Singer},
  title        = {Improved boosting algorithms using confidence-rated
                  predictions},
  journal      = {Machine Learning},
  year         = 1999,
  volume       = 37,
  number       = 3,
  month        = dec,
  pages        = {297-336},
  abstract     = { We describe several improvements to Freund and
                  Schapire's \adaboost\ boosting algorithm,
                  particularly in a setting in which hypotheses may
                  assign confidences to each of their predictions. We
                  give a simplified analysis of AdaBoost in this
                  setting, and we show how this analysis can be used
                  to find improved parameter settings as well as a
                  refined criterion for training weak hypotheses. We
                  give a specific method for assigning confidences to
                  the predictions of decision trees, a method closely
                  related to one used by Quinlan. This method also
                  suggests a technique for growing decision trees
                  which turns out to be identical to one proposed by
                  Kearns and Mansour. We focus next on how to apply
                  the new boosting algorithms to multiclass
                  classification problems, particularly to the
                  multi-label case in which each example may belong to
                  more than one class. We give two boosting methods
                  for this problem. One of these leads to a new method
                  for handling the single-label case which is simpler
                  but as effective as techniques suggested by Freund
                  and Schapire. Finally, we give some experimental
                  results comparing a few of the algorithms discussed
                  in this paper. },
  url          = {http://www.boosting.org/papers/SchSin99b.pdf},
}

@ARTICLE{schapire90strength,
  author       = {Robert E. Schapire},
  title        = {The Strength of Weak Learnability},
  journal      = {Machine Learning},
  volume       = 5,
  pages        = {197-227},
  year         = 1990,
  url          = {http://citeseer.nj.nec.com/schapire90strength.html},
}

@INPROCEEDINGS{schapire99theoretical,
  author       = {Robert E. Schapire},
  title        = {Theoretical Views of Boosting and Applications},
  booktitle    = {Algorithmic Learning Theory, 10th International
                  Conference, {ALT} '99, Tokyo, Japan, December 1999,
                  Proceedings},
  volume       = 1720,
  publisher    = {Springer},
  pages        = {13--25},
  year         = 1999,
  url          = {http://citeseer.nj.nec.com/article/schapire99theoretical.html},
}

@INPROCEEDINGS{schapire98improved,
  author       = {Robert E. Schapire and Yoram Singer},
  title        = {Improved Boosting Algorithms using Confidence-Rated
                  Predictions},
  booktitle    = {Computational Learning Theory},
  pages        = {80-91},
  year         = 1998,
  url          = {http://citeseer.nj.nec.com/schapire99improved.html},
}

@ARTICLE{schapire99improved,
  author       = {Robert E.~Schapire and Yoram Singer},
  title        = {Improved Boosting Using Confidence-rated Predictions},
  journal      = {Machine Learning},
  volume       = 37,
  number       = 3,
  year         = 1999,
  pages        = {297--336},
}

@TECHREPORT{scott98parcel,
  author       = {M. Scott and M. Niranjan and R. Prager},
  title        = {Parcel: Feature subset selection in variable cost
                  domains},
  number       = {CUED/F-INFENG/TR 323},
  institution  = {Cambridge University},
  year         = 1998,
  url          = {http://citeseer.nj.nec.com/scott98parcel.html},
}

@ARTICLE{sharkey97combining,
  author       = {A. Sharkey and N. Sharkey},
  title        = {Combining diverse neural networks},
  journal      = {The Knowledge Engineering Review},
  year         = 1997,
  volume       = 12,
  pages        = {231-247},
  number       = 3,
}

@INBOOK{sharkey98:book,
  publisher    = {Springer-Verlag},
  pages        = {1--30},
  year         = 1999,
  author       = {Amanda Sharkey},
  chapter      = {Combining Artificial Neural Nets: Ensemble and
                  Modular Multi-Net Systems},
  title        = {Multi-Net Systems},
}

@INPROCEEDINGS{sharkey97:diversity,
  booktitle    = {Neural Networks and their Applications (NEURAP'97)},
  pages        = {205--212},
  year         = 1997 ,
  author       = {Amanda Sharkey and Noel Sharkey},
  title        = {Diversity, Selection, and Ensembles of Artificial
                  Neural Nets},
}

@ARTICLE{sharkeychandroth96,
  title        = {Diverse Neural Net Solutions to a Fault Diagnosis
                  Problem},
  year         = 1996,
  journal      = {Neural Computing and Applications},
  volume       = 4 ,
  author       = {Amanda Sharkey and Noel Sharkey and Gopinath
                  Chandroth},
  pages        = {218--227},
}

@INPROCEEDINGS{sharkey98:adapting,
  author       = {Amanda Sharkey and Noel Sharkey and Simon Cross},
  booktitle    = {ICANN `98},
  publisher    = {Springer-Verlag},
  title        = {Adapting an ensemble approach for the diagnosis of
                  breast cancer},
  year         = 1998,
  pages        = {281-286},
}

@ARTICLE{sharkey97:arm,
  author       = {Noel Sharkey},
  title        = {Artificial Neural Networks for Coordination and
                  Control: The Portability of Experimental
                  Representations},
  journal      = {Robotics and Autonomous Systems},
  year         = 1997,
  volume       = 22,
  pages        = {345-360},
}

@ARTICLE{sharkey95weight,
  author       = {Sharkey,N.E. and Neary,J. and Sharkey,A.J.C.},
  title        = {{S}earching {W}eight {S}pace for {B}ackpropagation
                  {S}olution {T}ypes},
  journal      = {{C}urrent {T}rends in {C}onnectionism: {P}roceedings
                  of the 1995 {S}wedish {C}onference on
                  {C}onnectionism},
  editor       = {Niklasson,L.F. and Boden,M.B.},
  pages        = {103--120},
  year         = 1995,
}

@ARTICLE{sierra98global,
  author       = {A. Sierra and C. Cruz},
  title        = {Global and Local Neural Network Ensembles},
  journal      = {Pattern Recognition Letters},
  volume       = 19,
  number       = 8,
  pages        = {651--655},
  year         = 1998,
  pdf          = {http://dx.doi.org/10.1016/S0167-8655(98)00042-7},
}

@ARTICLE{smith92cooperative,
  number       = 2,
  pages        = {127--149},
  title        = {Searching for Diverse Cooperative Populations with
                  Genetic Algorithms},
  volume       = 1,
  year         = 1992 ,
  journal      = {Evolutionary Computation},
  author       = {Robert E. Smith and Stephanie Forrest and Alan
                  S. Perelson},
}

@ARTICLE{sollichkrogh96:overfitting,
  author       = {P. Sollich and A. Krogh},
  title        = {Learning with ensembles: How overfitting can be
                  useful},
  pages        = {190--196},
  booktitle    = {Advances in Neural Information Processing Systems},
  editor       = {David S. Touretzky and Michael C. Mozer and Michael
                  E. Hasselmo},
  volume       = 8,
  year         = 1996,
  publisher    = {The {MIT} Press},
  abstract     = {We study the characteristics of learning with
                  ensembles. Solving exactly the simple model of an
                  ensemble of linear students, we find surprisingly
                  rich behaviour. For learning in large ensembles, it
                  is advantageous to use under-regularized students,
                  which actually over-fit the training data. Globally
                  optimal performance can be obtained by choosing the
                  training set sizes of the students
                  appropriately. For smaller ensembles, optimization
                  of the ensemble weights can yield significant
                  improvements in ensemble generalization performance,
                  in particular if the individual students are subject
                  to noise in the training process. Choosing students
                  with a wide range of regularization parameters makes
                  this improvement robust against changes in the
                  unknown level of noise in the training data.},
}

@ARTICLE{swann98:fastcommittee,
  volume       = 34,
  number       = 14,
  title        = {Fast Committee Learning: Preliminary Results},
  month        = {July},
  journal      = {Electronics Letters},
  pages        = {1408-1410},
  year         = 1998,
  author       = {A. Swann and N. Allinson},
}

@ARTICLE{taniguchi97averaging,
  author       = {Michiaki Taniguchi and Volker Tresp},
  title        = {Averaging Regularized Estimators},
  journal      = {Neural Computation},
  volume       = 9,
  number       = 5,
  pages        = {1163-1178},
  year         = 1997,
}

@ARTICLE{tetko95overfitting,
  author       = {Tetko, I. V. and Livingstone, D. J. and Luik, A. I.},
  title        = {Neural network studies. 1. Comparison of overfitting
                  and overtraining},
  journal      = {Journal of Chemical Information & Computer Sciences},
  volume       = 35,
  number       = 5,
  pages        = {826-833},
  abstract     = {The application of feed forward back propagation
                  artificial neural networks with one hidden layer
                  (ANN) to perform the equivalent of multiple linear
                  regression (MLR) has been examined using artificial
                  structured data sets and real literature data. The
                  predictive ability of the networks has been
                  estimated using a training/test set protocol. The
                  results have shown advantages of ANN over MLR
                  analysis. The ANNs do not require high order terms
                  or indicator variables to establish complex
                  structure-activity relationships. Overfitting does
                  not have any influence on network prediction ability
                  when overtraining is avoided by
                  cross-validation. Application of ANN ensembles has
                  allowed the avoidance of chance correlations and
                  satisfactory predictions of new data have been
                  obtained for a wide range of numbers of neurons in
                  the hidden layer. [References: 30]},
  keywords     = {Qsar},
  year         = 1995,
}

@ARTICLE{tetko93structure,
  author       = {Tetko, I. V. and Luik, A. I. and Poda, G. I.},
  title        = {Applications of neural networks in
                  structure-activity relationships of a small number
                  of molecules},
  journal      = {Journal of Medicinal Chemistry},
  volume       = 36,
  number       = 7,
  pages        = {811-4},
  abstract     = {We investigated the applications of back propagation
                  artificial neural networks (ANN) for a small dataset
                  analysis in the field of structure-activity
                  relationships. The derivatives of carboquinone were
                  used as an example. It\'s been found that in this
                  case the use of the same neural network results in
                  unambiguous classification of new
                  molecules. Predictions can be improved with
                  statistical analysis of independent prognosis
                  sets. We suggest that the sign criterion be used as
                  a classification rule. We also compared neural
                  networks with FALS and ALS in leave-one-out
                  prediction. ANN applied to the same dataset has
                  shown the same predictive ability as ALS but poorer
                  than FALS.},
  keywords     = {*Carbazilquinone/aa [Analogs & Derivatives]
                  Comparative Study *Mitomycins/pd [Pharmacology]
                  *Neural Networks (Computer) Structure-Activity
                  Relationship},
  year         = 1993,
}

@ARTICLE{tetko97epa,
  author       = {Tetko, I. V. and Villa, A. E.},
  title        = {An efficient partition of training data set improves
                  speed and accuracy of cascade-correlation algorithm},
  journal      = {Neural Processing Letters},
  volume       = 6,
  number       = {1-2},
  pages        = {51-59},
  abstract     = {This study extends an application of efficient
                  partition algorithm (EPA) for artificial neural
                  network ensemble trained according to Cascade
                  Correlation Algorithm. We show that EPA allows to
                  decrease the number of cases in learning and
                  validated data sets. The predictive ability of the
                  ensemble calculated using the whole data set is not
                  affected and in some cases it is even improved. It
                  is shown that a distribution of cases selected by
                  this method is proportional to the second derivative
                  of the analyzed function},
  keywords     = {algorithm, cascade correlation, early stopping,
                  efficient partition of training data set},
  year         = 1997,
}

@ARTICLE{tetko97overfitting,
  author       = {Tetko, I. V. and Villa, A. E.},
  title        = {An enhancement of generalization ability in cascade
                  correlation algorithm by avoidance of
                  overfitting/overtraining problem},
  journal      = {Neural Processing Letters},
  volume       = 6,
  number       = {1-2},
  pages        = {43-50},
  abstract     = {The current study investigates a method for
                  avoidance of an overfitting/overtraining problem in
                  Artificial Neural Network (ANN) based on a
                  combination of two algorithms: Early Stopping and
                  Ensemble averaging (ESE). We show that ESE provides
                  an improvement of the prediction ability of ANN
                  trained according to Cascade Correlation
                  Algorithm. A simple algorithm to estimate the
                  generalization ability of the method according to
                  the Leave-One-Out technique is proposed and
                  discussed. In the accompanying paper the problem of
                  optimal selection of training cases is considered
                  for accelerated learning of the ESE method},
  keywords     = {cascade correlation algorithm, early stopping,
                  overfitting, overtraining},
  year         = 1997,
}

@ARTICLE{tetko97partition,
  author       = {Tetko, I. V. and Villa, A. E.},
  title        = {Efficient Partition of Learning Data Sets for Neural
                  Network Training},
  journal      = {Neural Networks},
  volume       = 10,
  number       = 8,
  pages        = {1361-1374.},
  abstract     = {This study investigates the emerging possibilities
                  of combining unsupervised and supervised learning in
                  neural network ensembles. Such strategy is used to
                  get an efficient partition of a noisy input data set
                  in order to focus the training of neural networks on
                  the most complex and informative domains of the data
                  set and accelerate the learning phase. The proposed
                  algorithm provides a good prediction accuracy using
                  fewer cases from non-informative domains according
                  to a correlative measure of dependency between cases
                  of the training set. This measure takes into account
                  internal relationships amid analyzed data and can be
                  used to cluster neighbor cases in a multidimensional
                  space and to filter out the outliers. The possible
                  relation of the proposed algorithm to brain
                  processing occurring in the thalamo-cortical pathway
                  is discussed.},
  year         = 1997,
}

@ARTICLE{tetko96variable,
  author       = {Tetko, I. V. and Villa, A. E. and Livingstone,
                  D. J.},
  title        = {Neural network studies. 2. Variable selection},
  journal      = {Journal of Chemical Information & Computer Sciences},
  volume       = 36,
  number       = 4,
  pages        = {794-803},
  abstract     = {Quantitative structure-activity relationship (QSAR)
                  studies usually require an estimation of the
                  relevance of a very large set of initial
                  variables. Determination of the most important
                  variables allows theoretically a better
                  generalization by all pattern recognition
                  methods. This study introduces and investigates five
                  pruning algorithms designed to estimate the
                  importance of input variables in feed-forward
                  artificial neural network trained by back
                  propagation algorithm (ANN) applications and to
                  prune nonrelevant ones in a statistically reliable
                  way. The analyzed algorithms performed similar
                  variable estimations for simulated data sets, but
                  differences were detected for real QSAR
                  examples. Improvement of ANN prediction ability was
                  shown after the pruning of redundant input
                  variables. The statistical coefficients computed by
                  ANNs for QSAR examples were better than those of
                  multiple linear regression. Restrictions of the
                  proposed algorithms and the potential use of ANNs
                  are discussed.},
  keywords     = {Databases, Factual Linear Models *Neural Networks
                  (Computer) Nonlinear Dynamics Pharmaceutical
                  Preparations/ch [Chemistry] Structure-Activity
                  Relationship Support, Non-U.S. Gov\'t},
  year         = 1996,
}

@ARTICLE{tetko98pruning,
  author       = {Tetko, I. V. and Villa, A. E. P. and Aksenova,
                  T. I. and Zielinski, W. L. and Brower, J. and
                  Collantes, E. R. and Welsh, W. J.},
  title        = {Application of a pruning algorithm to optimize
                  artificial neural networks for pharmaceutical
                  fingerprinting},
  journal      = {Journal of Chemical Information & Computer Sciences},
  volume       = 38,
  number       = 4,
  pages        = {660-668},
  abstract     = {The present study investigates an application of
                  artificial neural networks (ANNs) for use in
                  pharmaceutical fingerprinting. Several pruning
                  algorithms were applied to decrease the dimension of
                  the input parameter data set. A localized
                  fingerprint region was identified within the
                  original input parameter space from which a subset
                  of input parameters was extracted leading to
                  enhanced ANN performance. The present results
                  confirm that ANNs can provide a fast, accurate, and
                  consistent methodology applicable to pharmaceutical
                  fingerprinting. [References: 26]},
  year         = 1998,
}

@BOOK{tikhonov77book,
  author       = {A. N. Tikhonov and V. Y. Arsenin},
  title        = {Solutions of Ill-posed problems},
  publisher    = {W.H.Winston and Sons, Washington D.C.},
  year         = 1977,
}

@INPROCEEDINGS{ting97stacked,
  author       = {Kai Ming Ting and Ian H. Witten},
  title        = {Stacked Generalization: When Does It Work?},
  booktitle    = {{IJCAI} (2)},
  pages        = {866--873},
  year         = 1997,
  url          = {http://citeseer.nj.nec.com/ting97stacked.html},
}

@INPROCEEDINGS{tresp95combining,
  author       = {Volker Tresp and Michiaki Taniguchi},
  title        = {Combining Estimators Using Non-Constant Weighting
                  Functions},
  pages        = {419--426},
  editor       = {G. Tesauro and D. Touretzky and T. Leen},
  volume       = 7,
  booktitle    = {Advances in Neural Information Processing Systems},
  year         = 1995,
  publisher    = {The {MIT} Press},
  abstract     = {This paper discusses the linearly weighted
                  combination of estimators in which the weighting
                  functions are dependent on the input. We show that
                  the weighting functions can be derived either by
                  evaluating the input dependent variance of each
                  estimator or by estimating how likely it is that a
                  given estimator has seen data in the region of the
                  input space close to the input pattern. The latter
                  solution is closely related to the mixture of
                  experts approach and we show how learning rules for
                  the mixture of experts can be derived from the
                  theory about learning with missing features. The
                  presented approaches are modular since the weighting
                  functions can easily be modified (no retraining) if
                  more estimators are added. Furthermore, it is easy
                  to incorporate estimators which were not derived
                  from data such as expert systems or algorithms.},
}

@ARTICLE{tumerghosh94framework,
  author       = {K. Tumer and J. Ghosh},
  title        = {A framework for estimating performance improvements
                  in hybrid pattern classifiers},
  journal      = {World Congress on Neural Networks},
  volume       = 3,
  pages        = {220--5},
  publisher    = {Lawrence Erlbaum Associates},
  month        = {June},
  year         = 1994,
  abstract     = {Classification methods often perform significantly
                  below Bayesian limits in complex, high-dimensional
                  classification tasks because of model bias,
                  inadequate training data and noise/variability in
                  the data. When several classifiers are used for a
                  given task, selecting one method over all others
                  discards potentially valuable
                  information. Strategies aimed at suitably combining
                  the results of multiple classifiers are expected to
                  perform better than any single method, and reduce
                  overall bias and noise. An underwater passive sonar
                  data set consisting of over 1000 samples processed
                  to produce different 25-dimensional and
                  24-dimensional feature vectors is used in this study
                  to examine an evidence combination framework. An
                  analysis of the conditions that the data sets must
                  satisfy, and the conditions under which improvements
                  can be obtained is provided, and results are
                  presented for hybrid networks using both local and
                  global classifiers. },
}

@INPROCEEDINGS{tumerghosh94limits,
  author       = {K. Tumer and J. Ghosh},
  title        = {Limits to Performance Gains in Combined Neural
                  Classifiers},
  editor       = {Dagli and Akay and Chen and Fernandez and Ghosh},
  booktitle    = {Intelligent engineering systems through artificial
                  neural networks: proceedings of the Artificial
                  Neural Networks in Engineering ({ANNIE} '95)
                  Conference},
  publisher    = {American Society of Mechanical Engineers},
  address      = {United Engineering Center, 345 E. 47th St., New
                  York, NY 10017, USA},
  year         = 1994,
  volume       = 5,
  series       = {ASME Press Series on International Advances in
                  Design Productivity},
  abstract     = {The performance of a single classifier is often
                  inadequate in difficult classification problems. In
                  such cases, several researchers have combined the
                  outputs of multiple classifiers to obtain better
                  performance. However, the amount of improvement
                  possible through such combination techniques is
                  generally not known. We present two approaches to
                  estimating performance limits in hybrid
                  networks. First, we present a framework that
                  estimates Bayes error rates when linear combiners
                  are used. Then we discuss a more general method that
                  provides decision confidences and error bounds based
                  on error types arising from the training data. The
                  methods are illustrated for a difficult four class
                  problem involving underwater acoustic data. For this
                  data, we compute the single classifier and combiner
                  classification performances, as well as the Bayes
                  error rate and an error bound.},
}

@INPROCEEDINGS{tumerghosh95boundary,
  author       = {K. Tumer and J. Ghosh},
  title        = {Boundary variance reduction for improved
                  classification through hybrid networks},
  booktitle    = {Proceedings of the Spie Conf. on Applications and
                  Science of Artificial Neural Networks IV},
  volume       = 2492,
  pages        = {573--585},
  month        = {April},
  year         = 1995,
  address      = {Orlando, FL},
  abstract     = {Several researchers have shown experimentally that
                  substantial improvements can be obtained in
                  difficult pattern recognition problems by combining
                  or integrating the outputs of multiple
                  classifiers. This paper provides an analytical
                  framework that quantifies the improvements in
                  classification results due to linear combination. We
                  show that combining networks in the output space
                  reduces the variance of the actual decision region
                  boundaries around the optimum boundary. In the
                  absence of network bias, the added classification
                  error is directly proportional to the boundary
                  variance. Moreover, if the network errors are
                  independent, then the reduction in variance boundary
                  location is by a factor of N, the number of
                  classifiers that are combined. In the presence of
                  network bias, the reductions are less than or equal
                  to N, depending on the interaction between network
                  biases. We discuss how the individual networks can
                  be selected to achieve significant gains through
                  combination, and we support them with experimental
                  results on 25-dimensional sonar data. The analysis
                  presented facilitates the understanding of the
                  relationships among error rates, classifier boundary
                  distributions and combination in the output space.},
}

@ARTICLE{tumerghosh95orderstats,
  author       = {K. Tumer and J. Ghosh},
  title        = {Order Statistics Combiners for Neural Classifiers},
  journal      = {World Congress on Neural Networks},
  volume       = {Vol. I},
  pages        = {31--34},
  publisher    = {INNS Press},
  address      = {Washington, DC},
  month        = {July},
  year         = 1995,
  abstract     = {Several researchers have shown that linearly
                  combining outputs of multiple neural classifiers
                  results in better performance for many
                  applications. In this paper we introduce a family of
                  order statistics combiners as an alternative to
                  linear combiners. We show analytically that the
                  selection of the median, the maximum and in general,
                  the i-th order statistic improves classification
                  performance. Specifically, we show that order
                  statistics combiners reduce the variance of the
                  actual decision boundaries around the optimum
                  boundary, and that this is directly related to
                  classification error.},
}

@TECHREPORT{tumerghosh95theoretical,
  author       = {Kagan Tumer and Joydeep Ghosh},
  title        = {Theoretical Foundations of Linear and Order
                  Statistics Combiners for Neural Pattern Classifiers},
  institution  = {Computer and Vision Research Center, University of
                  Texas, Austin},
  year         = 1995,
  number       = {TR-95-02-98},
  url          = {http://citeseer.nj.nec.com/tumer96theoretical.html},
}

@ARTICLE{tumerghosh96,
  author       = {Kagan Tumer and Joydeep Ghosh},
  title        = {Error Correlation and Error Reduction in Ensemble
                  Classifiers},
  journal      = {Connection Science},
  volume       = 8,
  number       = {3-4},
  pages        = {385--403},
  year         = 1996,
  abstract     = {Using an ensemble of classifiers, instead of a
                  single classifier, can lead to improved
                  generalization. The gains obtained by combining,
                  however, are often affected more by the selection of
                  what is presented to the combiner than by the actual
                  combining method that is chosen. In this paper, we
                  focus on data selection and classifier training
                  methods, in order to 'prepare' classifiers for
                  combining. We review a combining framework for
                  classification problems that quantifies the need for
                  reducing the correlation among individual
                  classifiers. Then, we discuss several methods that
                  make the classifiers in an ensemble more
                  complementary. Experimental results are provided to
                  illustrate the benefits and pitfalls of reducing the
                  correlation among classifiers, especially when the
                  training data are in limited supply.},
}

@ARTICLE{tumerghosh96analysis,
  author       = {Kagan Tumer and Joydeep Ghosh},
  title        = {Analysis of decision boundaries in linearly combined
                  neural classifiers},
  journal      = {Pattern Recognition},
  year         = 1996,
  volume       = 29,
  number       = 2,
  pages        = {341--348},
  month        = {February},
}

@INPROCEEDINGS{tumerghosh96estimating,
  author       = {Kagan Tumer and Joydeep Ghosh},
  title        = {Estimating the Bayes Error Rate Through Classifier
                  Combining},
  booktitle    = {Proceedings of the 13th International Conference on
                  Pattern Recognition},
  month        = {August},
  year         = 1996,
  abstract     = {The Bayes error provides the lowest achievable error
                  rate for a given pattern classification
                  problem. There are several classical approaches for
                  estimating or finding bounds for the Bayes
                  error. One type of approach focuses on obtaining
                  analytical bounds, which are both difficult to
                  calculate and dependent on distribution parameters
                  that may not be known. Another strategy is to
                  estimate the class densities through non-parametric
                  methods, and use these estimates to obtain bounds on
                  the Bayes error. This article presents a novel
                  approach to estimating the Bayes error based on
                  classifier combining techniques. For an artificial
                  data set where the Bayes error is known, the
                  combiner-based estimate outperforms the classical
                  methods.},
}

@ARTICLE{tumer98rbfmedical,
  author       = {Kagan Tumer and Nirmala Ramanujam and Joydeep Ghosh
                  and Rebecca Richards-Kortum},
  title        = {Ensembles of Radial Basis Function Networks for
                  Spectroscopic Detection of Cervical Pre-Cancer},
  journal      = {IEEE Transactions on Biomedical Engineering},
  year         = 1998,
  volume       = 45,
  number       = 8,
  pages        = {953--961},
  abstract     = {Medical applications usually used Radial Basis
                  Function Networks just as Artificial Neural
                  Networks. However, RBFNs are Knowledge-Based
                  Networks that can be interpreted in several way:
                  Artificial Neural Networks, Regularization Networks,
                  Support Vector Machines, Wavelet Networks, Fuzzy
                  Controllers, Kernel Estimators, Instanced-Based
                  Learners. A survey of their interpretations and of
                  their corresponding learning algorithms is provided
                  as well as a brief survey on dynamic learning
                  algorithms. RBFNs' interpretations can suggest
                  applications that are particularly interesting in
                  medical domains. },
}

@INPROCEEDINGS{uedanakano96,
  booktitle    = {Proceedings of International Conference on Neural
                  Networks},
  title        = {Generalization Error of Ensemble Estimators},
  author       = {N. Ueda and R. Nakano},
  year         = 1996 ,
  pages        = {90--95},
  annote       = {First presentation (in ML literature) of the bias-variance-covariance decomposition.}
}

@INPROCEEDINGS{wahba99biasvariance,
  author       = {G. Wahba and X. Lin and F. Gao and D. Xiang and
                  R. Klein and B. Klein},
  title        = {The bias-variance tradeoff and the randomized
                  {GACV}},
  booktitle    = {Advances in Neural Information Processing Systems},
  number       = 11,
  pages        = {620--626},
  publisher    = {MIT Press},
  editor       = {M. Kearns and S. Solla and D. Cohn},
  year         = 1999,
  url          = {http://citeseer.nj.nec.com/wahba99biasvariance.html},
}

@ARTICLE{wan90bayesneural,
  author       = {E. A. Wan},
  title        = {Neural network classification: A Bayesian
                  interpretation},
  journal      = {IEEE Transactions on Neural Networks},
  type         = {Letter},
  year         = 1990,
  volume       = 1,
  number       = 4,
  pages        = {303--304},
}

@INPROCEEDINGS{wan97:sunspots,
  author       = {Eric A. Wan},
  booktitle    = {International Conference On Neural Networks
                  (ICNN97)},
  year         = 1997 ,
  title        = {Combining Fossil and Sunspot Data: Committee
                  Predictions},
  url          = {http://citeseer.ist.psu.edu/146595.html},
}

@INPROCEEDINGS{wezel98maximum,
  author       = {M.C. van Wezel and W.A. Kosters and J.N. Kok},
  title        = {Maximum Likelihood Weights for a Linear Ensemble of
                  Regression Neural Networks},
  booktitle    = {Proceedings International Conference in Neural
                  Information Processing (ICONIP\'98)},
  year         = 1998,
  publisher    = {IOS Press},
  editor       = {S. Usui and T. Omori},
  pages        = {498--501},
  address      = {Kitakyushu, Japan},
}

@ARTICLE{windeatt97spectral,
  author       = {Windeatt, T. and Tebbs, R.},
  title        = {Spectral technique for hidden layer neural network
                  training},
  journal      = {Pattern Recognition Letters},
  volume       = 18,
  issue        = 8,
  pages        = 723,
  year         = 1997,
  month        = {August},
}

@ARTICLE{wolpert92stacked,
  author       = {D. H. Wolpert},
  title        = {Stacked Generalization},
  journal      = {Neural Networks},
  volume       = 5,
  year         = 1992,
  pages        = {241--259},
  abstract     = {This paper introduces stacked generalization, a
                  scheme for minimizing the generalization error rate
                  of one or more generalizers. Stacked generalization
                  works by deducing the biases of the generalizer(s)
                  with respect to a provided learning set. This
                  deduction proceeds by generalizing in a second space
                  whose inputs are (for example) the guesses of the
                  original generalizers when taught with part of the
                  learning set and trying to guess the rest of it, and
                  whose output is (for example) the correct
                  guess. When used with multiple generalizers, stacked
                  generalization can be seen as a more sophisticated
                  version of cross-validation, exploiting a strategy
                  more sophisticated than cross-validation's crude
                  winner-takes-all for combining the individual
                  generalizers. When used with a single generalizer,
                  stacked generalization is a scheme for estimating
                  (and then correcting for) the error of a generalizer
                  which has been trained on a particular learning set
                  and then asked a particular question. After
                  introducing stacked generalization and justifying
                  its use, this paper presents two numerical
                  experiments. The first demonstrates how stacked
                  generalization improves upon a set of separate
                  generalizers for the NETtalk task or translating
                  text to phonemes. The second demonstrates how
                  stacked generalization improves the performance of a
                  single surface-fitter. With the other experimental
                  evidence in the literature, the usual arguments
                  supporting cross-validation, and the abstract
                  justifications presented in this paper, the
                  conclusion is that for almost any real-world
                  generalization problem one should use some version
                  of stacked generalization to minimize the
                  generalization error rate. This paper ends by
                  discussing some of the variation of stacked
                  generalization, and how it touches on other fields
                  like chaos theory. },
}

@ARTICLE{wolpert97bias,
  author       = {David Wolpert},
  title        = {On Bias Plus Variance},
  journal      = {Neural Computation},
  volume       = 9,
  number       = 6,
  pages        = {1211-1243},
  year         = 1997,
  url          = {http://citeseer.nj.nec.com/article/wolpert96bias.html},
}

@ARTICLE{woods97local,
  author       = {K. Woods and W.P. Kegelmeyer and K. Bowyer},
  title        = {Combination of multiple classifiers using local
                  accuracy estimates},
  journal      = {IEEE Transactions on Pattern Analysis and Machine
                  Intelligence},
  year         = 1997,
  volume       = 19,
  pages        = {405-410},
}

@INCOLLECTION{yao99review,
  publisher    = {IEEE},
  pages        = {1423-1447},
  volume       = 87 ,
  year         = 1999,
  author       = {Xin Yao},
  number       = 9,
  title        = {Evolving Artificial Neural Networks},
  month        = {September},
  booktitle    = {Proceedings of the IEEE},
}

@INCOLLECTION{yaoliu96,
  author       = {Xin Yao and Yong Liu},
  pages        = {229-242},
  booktitle    = {Complex Systems - From Local Interactions to Global
                  Phenomena},
  publisher    = {IOS Press, Amsterdam},
  title        = {How to Make Best Use of Evolutionary Learning},
  year         = 1996,
}

@ARTICLE{yaoliu97epnet,
  author       = {Xin Yao and Yong Liu},
  number       = 3,
  pages        = {694--713},
  journal      = {IEEE Transactions on Neural Networks},
  year         = 1997,
  title        = {A new evolutionary system for evolving artificial
                  neural networks},
  month        = {May},
  volume       = 8,
}

@INCOLLECTION{yaoliu98making_use,
  pages        = {417--425},
  booktitle    = {IEEE Transactions on Systems, Man and Cybernetics,
                  Part B: Cybernetics},
  year         = 1998,
  volume       = 28,
  number       = 3,
  publisher    = {IEEE Press},
  title        = {Making use of Population Information in Evolutionary
                  Artificial Neural Networks},
  month        = {June},
  author       = {Xin Yao and Yong Liu},
}

@INPROCEEDINGS{yaoliu99breastcancer,
  author       = {Xin Yao and Yong Liu},
  title        = {Neural networks for breast cancer diagnosis},
  year         = 1999,
  booktitle    = {Proceedings of the 1999 Congress on Evolutionary
                  Computation},
  pages        = {1760-1767},
  volume       = 3,
  month        = {July},
  publisher    = {IEEE Press},
}

@ARTICLE{yates95use,
  author       = {W. Yates and D. Partridge},
  title        = {Use of methodological diversity to improve neural
                  network generalization},
  journal      = {Neural Computing and Applications},
  year         = 1996,
  volume       = 4,
  pages        = {114--128},
  number       = 2,
  url          = {http://citeseer.nj.nec.com/partridge95use.html},
}

@ARTICLE{condorcet88review,
  author       = {P. Young},
  title        = {Condorcet's Theory of Voting},
  journal      = {American Political Science Review},
  year         = 1988,
  volume       = 82,
  number       = {1231-1244},
}
