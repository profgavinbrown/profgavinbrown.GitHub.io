<head><link rel="stylesheet" href="style.css"></head>

<?php include("./menu.php"); ?>

<?php include("./track.php"); ?>

<div class="bodybox">

<br>
<center>
<table width=90% border=2><tr><td bgcolor=#f19126>
<center><font size=+1 color=blue><br><b>News : DEADLINE EXTENSION TO <u>WED 4th JULY, 
MIDNIGHT GMT</u></b></font><bR><br></center>
</td></tr></table><br>
</center>
<br>

<font size=+1 color=blue><b>Call for Papers : <i>Principles and Practice of 
Multiple Learning Systems</i></b></font><bR><br>
<b><i>Multiple Learning Systems</b></i>, aka multiple classifier systems, ensembles, 
information fusion, have emerged as one of the strongest 
pattern recognition techniques of the last decade, influencing every area of Machine 
Learning and Data Mining. Techniques like Boosting or Bagging, that create multiple 
classifiers then combine them by voting or averaging are becoming a staple part of the ML 
toolbox. All techniques in this family rely on the 
general principle of combining information from compatible sources, 
whether they be classifiers, regressors, clusters, kernels, proximity 
measures, or procedures.  In empirical evaluations, MLS consistently 
give outstanding performance compared to more complex state-of-the-art 
learning methods.
<br>
<br>

On a deeper level, the "combining" principle has recently been extended 
in several new directions. Cluster ensembles, semi-supervised learning, 
changing environments, kernel combining methods 
are all utilizing this principle.  In particular, this workshop aims 
to highlight these advanced uses of the "combining" principle, right across 
the spectrum of machine learning and data mining.<br>
<bR>
This is the first ECML workshop to address this important principle, 
aiming to cover both the deep theoretical questions and practical applications 
of the idea.

<br><br>

<font size=+1 color=blue><b>Workshop Goals</b></font><bR>

The goal of this workshop is to bring together researchers that work with 
the combining principle in a very broad sense, from the theory of 
classifier ensembles through to meta-learning 
such as the integration of complementary learning methodologies.
We aim to provide a platform for exchanging ideas between people
from different sub-fields of machine learning or data mining.
We expect participants that focus on various learning problems such 
as classification, clustering, regression, semi-supervised learning, 
multi-task learning, novelty detection, data organisation or 
information fusion and use either theory or efficient heuristics.
We aim to discuss the state-of-the-art techniques, ideas originating 
from various subfields of computational intelligence and to be representative of
the remaining open problems in the field.<br>
<br>
<center>


<table border=0 cellpadding=5><tr><td valign=top>
<b> Learning schemes</b>
<ul>
    <li> Classifier selection vs classifier combination
    <li> Boosting and Bagging methods
    <li> Rule Ensembles
    <li> Feature subspaces / feature projections
    <li> Learning classifier systems
</ul>
</td><td valign=top>
<b>Paradigm combination</b>
<ul>
    <li> Combining Objective Functions
    <li> Combining Discriminative and Generative models
    <li> Combining Kernels
    <li> Combining Statistical and Structural techniques
    <li> Combining proximity measures
</ul>
</td></tr>
<tr><td valign=top>
<b>Theory</b>
<ul> Methods of classifier combination
    <li> Diversity measures
    <li> Classifier selection vs classifier combination
    <li> Weak and strong models vs overtraining
    <li> Handling noise and outliers
</ul>
</td><td valign=top>
<b>Data flexibility</b>
<ul>   
    <li> MLS in changing/dynamic environments
    <li> Cluster ensembles
    <li> Stream mining
    <li> Learning from distributed data
    <li> Learning from multiple data sets
    <li> Semi-supervised learning
</ul>
</td>
</tr>
<tr><td valign=top colspan=2>
<b> Applications</b>
<ul>
    <li> Image classification/retrieval,
     Data mining,
    Text classification,
     Computer vision,
     Biometric identification.
</ul>
</td>
<td><b></td></tr>
</table>







<!--
<table border=0 cellpadding=5><tr><td valign=top>
<b>Theory</b>
<ul>
<li> Methods of classifier combination
<li> Diversity measures 
<li> Classifier selection vs classifier combination
<li> Weak and strong models vs overtraining
<li> Handling noise and outliers 
</ul>
</td><td valign=top>
<b>Learning schemes</b>
<ul>
<li> Learning classifier systems
<li> Rule ensembles
<li> Classifier selection vs classifier combination
<li> Boosting and bagging methods
<li> Feature subspaces / feature projections
</ul>
</td></tr>

<tr><td valign=top>
<b>Multiple Representations</b>
<ul>
<li> Kernel combining and kernel selection
<li> Learning from multiple data sets
<li> Combining proximity measures
<li> Information fusion and sensor fusion
</ul>
</td><td valign=top>
<b>Data flexibility</b>
<ul>
<li> MLS in changing/dynamic environments
<li> Cluster ensembles
<li> Stream mining
<li> Learning from distributed data
<li> Semi-supervised learning
</ul>
</td></tr>

<tr><td valign=top>
<b>Paradigm combination</b>
<ul>
<li> Hybrid ensemble methods
<li> Integration of statistical and structural techniques
<li> Combination of generative and discriminative models 
</ul>
</td><td valign=top>
<b>Applications</b>
<ul>
<li> Image classification/retrieval
<li> Data mining
<li> Text classification
<li> Computer vision
<li> Biometric identification
</ul>
</td></tr></table>

</center>

-->



<br>

We particularly welcome novel ideas on combination of complementary 
methodologies, understood in a broad sense, such as combination of
generative and discriminative models or probabilistic and information 
theoretic models for learning. We also encourage the researchers to 
share their both positive and negative experience with the combining
paradigm.

<span style="visibility:hidden">
